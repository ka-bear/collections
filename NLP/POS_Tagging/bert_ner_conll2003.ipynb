{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d72a8b8604854d4e9ed5fbec7685956b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fecd1cea6daf40b5aa5abafe3aa9b7fe",
              "IPY_MODEL_99cf2c5d5bdd4d82b84b601e8c12c147",
              "IPY_MODEL_25c7e99b5d7a48cf892e376ff1dc8136"
            ],
            "layout": "IPY_MODEL_9b6942ac194f400388293d033248fd98"
          }
        },
        "fecd1cea6daf40b5aa5abafe3aa9b7fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b8b7ef0673f4061993d3d23a02bbd34",
            "placeholder": "​",
            "style": "IPY_MODEL_96acaa4084154627af554b0b7ba26d2d",
            "value": "Map: 100%"
          }
        },
        "99cf2c5d5bdd4d82b84b601e8c12c147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94e4b87d9af34f49bb6e93e612050511",
            "max": 3250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae8028ce30dc4c3db7cce42c871b6833",
            "value": 3250
          }
        },
        "25c7e99b5d7a48cf892e376ff1dc8136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a129225dee44af5b701bea1e35f435c",
            "placeholder": "​",
            "style": "IPY_MODEL_2ae8f898d37449318acafc1f6b477d95",
            "value": " 3250/3250 [00:00&lt;00:00, 7659.90 examples/s]"
          }
        },
        "9b6942ac194f400388293d033248fd98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b8b7ef0673f4061993d3d23a02bbd34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96acaa4084154627af554b0b7ba26d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94e4b87d9af34f49bb6e93e612050511": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae8028ce30dc4c3db7cce42c871b6833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a129225dee44af5b701bea1e35f435c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ae8f898d37449318acafc1f6b477d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition with BERT\n",
        "\n",
        "In this article, we will demonstrate how to perform Named Entity Recognition (NER) using BERT. We will train a BERT model on the CONLL2003 dataset in 10 steps.\n",
        "\n",
        "Named Entity Recognition (NER) is a common task in Natural Language Processing that extracts relevant information from data. It involves training a system to identify and categorize entities in a text by simply tagging them with pre-defined tags.  In this example our tags are like ‘Person’ or ‘Location’ since they are coming from CONLL2003 dataset.\n",
        "\n",
        "We will use the pre-trained BERT model from the Hugging Face Hub and fine-tune it to make Named Entity Recognition (NER) on a spesific dataset.\n",
        "\n",
        "Compared to traditional NLP pipelines, Transformer architectures offer a more comprehensive, end-to-end approach. Models can be trained from stratch or can be fine-tuned like in our example.\n",
        "\n",
        "The Hugging Face Transformers Library,\n",
        "*  Provides state-of-the-art machine learning models like BERT, GPT-2, and T5. It is used for tasks such as text classification, information extraction, summarization.\n",
        "\n",
        "* Functions as an unified high-level API for AI models. It is an interface designed to be compatible with both PyTorch and TensorFlow, two of the most popular deep learning libraries.\n",
        "\n",
        "HuggingFace Transformers access easily pre-trained models from the libraries can switch between with minimal effort. Models can be pushed to the Hugging Face Hub for sharing and collaboration. (not necessarly) Alternatively, they can also be downloaded and used locally.\n",
        "\n",
        "* CONLL2003 is a dataset for fine-tuning the model.\n",
        "* A Python Notebook In Google Colab (Enabled with GPU)\n",
        "\n",
        "The training process involves the following steps:\n",
        "\n",
        "1. Load the Libraries\n",
        "2. Inspect the Dataset\n",
        "3. Verify the Alignment\n",
        "4. Tokenize the Dataset\n",
        "5. Configure the Data Collator\n",
        "6. Set up the Metric Calculation\n",
        "7. Initialize the Model\n",
        "8. Define the Training Arguments\n",
        "9. Begin Training\n",
        "10. Evaluate the Results\n",
        "\n",
        "Their documentation was also main resource for this article. I find it somehow difficult to find a precise tutorial like this one.\n",
        "\n",
        "I highly recommend to check both transformer architecture, BERT other models and a traditional methods of NLP.\n",
        "\n",
        "* I'm planning to more blog posts on both medium and my blog.\n",
        "* Source code is both accessible on GitHub and Colab.\n",
        "* Model is accessible on Hugging Face Hub.\n",
        "\n",
        "Lets move on!"
      ],
      "metadata": {
        "id": "qylxY6kUTo1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 : Load The Librares"
      ],
      "metadata": {
        "id": "ZCGH4e67IXaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lines here installs several Python libraries using pip, Python’s package installer:"
      ],
      "metadata": {
        "id": "DvtCNNLaj7ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Python libraries using pip\n",
        "!pip install transformers datasets tokenizers seqeval -q\n",
        "!pip install tensorflow_probability -U\n",
        "!pip install seqeval -U"
      ],
      "metadata": {
        "id": "fT2JQ96MM0-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **transformers:** HuggingFace transformers library.\n",
        "* **datasets:** This library provides a simple way to download, process, and load datasets. It is developed by Hugging Face, the same organization that develops the transformers library.\n",
        "* **tokenizers:** This library is used for text tokenization. It is capable of training new tokenizers and comes with several pre-trained ones.\n",
        "* **seqeval:** This library is used for sequence labeling evaluation. It is commonly used in Named Entity Recognition (NER) tasks.\n",
        "\n",
        "* The -q flag is used to run the installation in quiet mode, which means it won’t print all the installation messages."
      ],
      "metadata": {
        "id": "0-ZgGPuRFRMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last line is upgrading the seqeval library to its latest version. As mentioned before, seqeval is used for sequence labeling evaluation."
      ],
      "metadata": {
        "id": "6a8exlTwFfpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU\n",
        "import torch; print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "_-b83ZvReofd",
        "outputId": "9b79b286-1525-4bf6-c960-d156738f6d06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you run this code, it will print out the name of your GPU. This is useful to confirm that PyTorch is properly configured to use your GPU, which can significantly speed up machine learning tasks. If no GPU is available, or PyTorch isn’t configured correctly, it will throw an error."
      ],
      "metadata": {
        "id": "k7LOxphXlS44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 : Inspect the Dataset"
      ],
      "metadata": {
        "id": "Pb0Fmds3T9DH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5il3VXU2F9m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "# Import the load_dataset function from the datasets library.\n",
        "# Function is used to load a dataset from the Hugging Face datasets hub.\n",
        "from datasets import load_dataset\n",
        "\n",
        "# loads the “conll2003” dataset and assigns it to the variable conll2003.\n",
        "conll2003 = datasets.load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "UX7VvTHFYEwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* The CoNLL-2003 dataset is a popular benchmark for Named Entity Recognition (NER), a common task in Natural Language Processing (NLP) where the goal is to classify named entities in text into pre-defined categories such as person names, organizations, locations, etc."
      ],
      "metadata": {
        "id": "pcZZLMGoGMCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Dataset\n",
        "conll2003"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP4K3o_2kcb9",
        "outputId": "384eb8dc-0a66-4b00-b037-59127250e848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  check the type of the conll2003 object.\n",
        "type(conll2003)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXxc3QAHktzO",
        "outputId": "80844553-af4a-4ac2-f5be-734d883bb8ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datasets.dataset_dict.DatasetDict"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Type is DatasetDict type is a dictionary-like object provided by the datasets library.\n",
        "\n",
        "* It allows you to access subsets of the dataset (like ‘train’, ‘test’, ‘validation’) using keys, just like you would with a Python dictionary."
      ],
      "metadata": {
        "id": "5qIcvWG3HGem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Check the shape attribute of the conll2003 dataset.\n",
        "conll2003.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xebo9cOyl4Ns",
        "outputId": "6d1131c3-139b-4700-94c6-db07d7777527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': (14041, 5), 'validation': (3250, 5), 'test': (3453, 5)}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape attribute of a DatasetDict object in the datasets library returns a dictionary where each key is the name of a subset of the dataset.\n",
        "\n",
        "Therese are ‘train’, ‘test’, ‘validation’ and each value is a tuple representing the shape of that subset."
      ],
      "metadata": {
        "id": "_VC0E2DpHS8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect First Row Of Train Data\n",
        "conll2003[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHcJuPvSl5kq",
        "outputId": "b8c8a355-026e-43ec-c5eb-d9a78ea4ec5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect NER Tags\n",
        "conll2003[\"train\"].features[\"ner_tags\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDBt6uWAl9v7",
        "outputId": "946135f2-a379-4075-9dff-0419ac3a3ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the description of datset\n",
        "conll2003['train'].description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "wScWId_AmFus",
        "outputId": "e440131a-1d7b-4adb-ba78-68b398be3cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\\nfour types of named entities: persons, locations, organizations and names of miscellaneous entities that do\\nnot belong to the previous three groups.\\n\\nThe CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\\na separate line and there is an empty line after each sentence. The first item on each line is a word, the second\\na part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\\nand the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\\nif two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\\nB-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\\ntagging scheme, whereas the original dataset uses IOB1.\\n\\nFor more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 : Verify the Alignment"
      ],
      "metadata": {
        "id": "6F7rU9hIIkhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the AutoTokenizer class from the transformers library\n",
        "# AutoTokenizer provides access to all the tokenizers available in the transformers library in a unified way\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Sets the model_checkpoint variable to the string\n",
        "# \"bert-base-cased\", which is the name of a pre-trained BERT model.\n",
        "# The “cased” part means that the model was trained on case-sensitive data\n",
        "model_checkpoint = \"bert-base-cased\"\n",
        "# Loads the tokenizer associated with the \"bert-base-cased\" model and assigns it to the variable tokenizer.\n",
        "# The from_pretrained method downloads and caches the tokenizer, and then returns an instance of it.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "TFuA7oPJmIVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test tokenizer\n",
        "inputs = tokenizer(conll2003[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
        "inputs.tokens()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIm3tLo2oCq7",
        "outputId": "d9376c77-789e-44ea-de16-7e91fcd2d999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'EU',\n",
              " 'rejects',\n",
              " 'German',\n",
              " 'call',\n",
              " 'to',\n",
              " 'boycott',\n",
              " 'British',\n",
              " 'la',\n",
              " '##mb',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check wordids\n",
        "inputs.word_ids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd5EK2bxdpSo",
        "outputId": "b9a73ca6-fd56-47b0-b5b3-d1e556213697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inputs.word_ids()  is used to get the word IDs from the inputs object. This is typically used when working with tokenized text data in natural language processing (NLP).\n",
        "\n",
        "word_ids() is a method of tokenizer instance that returns a list where each element corresponds to a token in the input text, and the value of each element is the ID of the word that the token is part of."
      ],
      "metadata": {
        "id": "My7i_e5HHt5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check NER tags\n",
        "print(conll2003[\"train\"][0][\"ner_tags\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MtqR9mEoL4w",
        "outputId": "8f78a187-05dd-4a79-9e2f-15b52bf71e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    # Initialize a list to store the adjusted labels\n",
        "    new_labels = []\n",
        "\n",
        "    # Initialize a variable to keep track of the current word's ID\n",
        "    current_word = None\n",
        "\n",
        "    # Iterate through each word ID in the word_ids list\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:\n",
        "            # Start of a new word/entity\n",
        "            current_word = word_id\n",
        "\n",
        "            # Assign -100 to labels for special tokens, else use the word's label\n",
        "            label = -100 if word_id is None else labels[word_id]\n",
        "\n",
        "            # Append the adjusted label to the new_labels list\n",
        "            new_labels.append(label)\n",
        "        elif word_id is None:\n",
        "            # Handle special tokens by assigning them a label of -100\n",
        "            new_labels.append(-100)\n",
        "        else:\n",
        "            # Token belongs to the same word/entity as the previous token\n",
        "            label = labels[word_id]\n",
        "\n",
        "            # If the label is in the form B-XXX, change it to I-XXX\n",
        "            if label % 2 == 1:\n",
        "                label += 1\n",
        "\n",
        "            # Append the adjusted label to the new_labels list\n",
        "            new_labels.append(label)\n",
        "\n",
        "    # Return the list of adjusted labels\n",
        "    return new_labels"
      ],
      "metadata": {
        "id": "lVbForwHcOeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, align_labels_with_tokens(labels, word_ids), is used to align labels with tokens, which is a common task in Named Entity Recognition (NER)."
      ],
      "metadata": {
        "id": "0HS_RAmfH4AS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = conll2003[\"train\"][0][\"ner_tags\"]\n",
        "word_ids = inputs.word_ids()\n",
        "print(labels)\n",
        "print(align_labels_with_tokens(labels, word_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sNMPhzogHI_",
        "outputId": "eb660815-2d40-4059-e4d9-79f7f9a26ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
            "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 : Tokenize The Dataset"
      ],
      "metadata": {
        "id": "B4yUebHmZRIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
        "    )\n",
        "    all_labels = examples[\"ner_tags\"]\n",
        "    new_labels = []\n",
        "    for i, labels in enumerate(all_labels):\n",
        "        word_ids = tokenized_inputs.word_ids(i)\n",
        "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = new_labels\n",
        "    return tokenized_inputs\n"
      ],
      "metadata": {
        "id": "_GVxzcSVlraq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = conll2003.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=conll2003[\"train\"].column_names,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d72a8b8604854d4e9ed5fbec7685956b",
            "fecd1cea6daf40b5aa5abafe3aa9b7fe",
            "99cf2c5d5bdd4d82b84b601e8c12c147",
            "25c7e99b5d7a48cf892e376ff1dc8136",
            "9b6942ac194f400388293d033248fd98",
            "7b8b7ef0673f4061993d3d23a02bbd34",
            "96acaa4084154627af554b0b7ba26d2d",
            "94e4b87d9af34f49bb6e93e612050511",
            "ae8028ce30dc4c3db7cce42c871b6833",
            "0a129225dee44af5b701bea1e35f435c",
            "2ae8f898d37449318acafc1f6b477d95"
          ]
        },
        "id": "vSObrtU_l1X3",
        "outputId": "8733f699-fd79-4018-9b6a-3a1df134257a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d72a8b8604854d4e9ed5fbec7685956b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 : Configure the Data Collator"
      ],
      "metadata": {
        "id": "JpliNV0oIrsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "064heVOKzGvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
        "batch[\"labels\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFmIUU5rzJ3y",
        "outputId": "ab9f87bc-374c-45bc-a278-a593bf5ee306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
              "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2):\n",
        "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlpSEmQJzT_x",
        "outputId": "87aaaac0-433d-49f5-fc05-68ed3cbe8ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n",
            "[-100, 1, 2, -100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6 : Set up the Metric Calculation"
      ],
      "metadata": {
        "id": "SFLAJcM2I4dP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "metric = evaluate.load(\"seqeval\")"
      ],
      "metadata": {
        "id": "nciWKJsUgYoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_feature = conll2003[\"train\"].features[\"ner_tags\"]\n",
        "label_names = ner_feature.feature.names\n",
        "labels = conll2003[\"train\"][0][\"ner_tags\"]\n",
        "labels = [label_names[i] for i in labels]\n",
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fqLX9cfzdJD",
        "outputId": "fd4e5692-805b-42ce-f19d-dac1d9b2b94b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = labels.copy()\n",
        "predictions[2] = \"O\"\n",
        "metric.compute(predictions=[predictions], references=[labels])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtps4dWczdzG",
        "outputId": "f2f233e0-91ec-4e96-c079-bd73cb0f6e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MISC': {'precision': 1.0,\n",
              "  'recall': 0.5,\n",
              "  'f1': 0.6666666666666666,\n",
              "  'number': 2},\n",
              " 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
              " 'overall_precision': 1.0,\n",
              " 'overall_recall': 0.6666666666666666,\n",
              " 'overall_f1': 0.8,\n",
              " 'overall_accuracy': 0.8888888888888888}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Remove ignored index (special tokens) and convert to labels\n",
        "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": all_metrics[\"overall_precision\"],\n",
        "        \"recall\": all_metrics[\"overall_recall\"],\n",
        "        \"f1\": all_metrics[\"overall_f1\"],\n",
        "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "Op2C8GNk0c4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* The function, `compute_metrics(eval_preds)`, is used to compute the precision, recall, F1 score, and accuracy of the predictions made by a model.\n",
        "\n",
        "* `logits, labels = eval_preds` line unpacks `eval_preds` into `logits` and `labels`. `logits` are the raw output values from the model, and `labels` are the true labels.\n",
        "\n",
        "* `predictions = np.argmax(logits, axis=-1)` This line uses the `np.argmax` function to find the indices of the maximum values along the last axis of `logits`. These indices represent the model's predictions.\n",
        "\n",
        "* `true_labels = [[label_names[l] for l in label if l != -100] for label in labels]` this line creates a new list of labels, `true_labels`, by iterating over `labels` and replacing each label `l` with its corresponding name from `label_names`, but only if `l` is not equal to `-100` (which is used to represent special tokens).\n",
        "\n",
        "\n",
        "`true_predictions = [\n",
        "    [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "`\n",
        "This line creates a new list of predictions, `true_predictions`, by iterating over `predictions` and `labels` together, replacing each prediction `p` with its corresponding name from `label_names`, but only if the corresponding label `l` is not equal to `-100`.\n",
        "\n",
        "```python\n",
        "all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "```\n",
        "This line computes the metrics by calling the `compute` method of the `metric` object with `true_predictions` and `true_labels` as arguments.\n",
        "\n",
        "```python\n",
        "return {\n",
        "    \"precision\": all_metrics[\"overall_precision\"],\n",
        "    \"recall\": all_metrics[\"overall_recall\"],\n",
        "    \"f1\": all_metrics[\"overall_f1\"],\n",
        "    \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
        "}\n",
        "```\n",
        "Finally, this line returns a dictionary containing the precision, recall, F1 score, and accuracy.\n",
        "\n",
        "This function is typically used in the evaluation step of a machine learning pipeline to assess the performance of a model.\n"
      ],
      "metadata": {
        "id": "NcuOdZYFJn5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7 : Initialize the Model"
      ],
      "metadata": {
        "id": "K8tvsg_qSUN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import token classification model to be trained or fine-tuned on tasks such as Named Entity Recognition (NER), Part-of-Speech tagging (POS)\n",
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "# Create two dictionaries: id2label and label2id.\n",
        "# id2label maps each label’s ID to its name.\n",
        "# label2id maps each label’s name to its ID.\n",
        "# These dictionaries are used to convert between label names and ID\n",
        "\n",
        "id2label = {i: label for i, label in enumerate(label_names)}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "# Load a pre-trained model for token classification from the checkpoint specified by model_checkpoint,\n",
        "# Configures it to use the specific labels defined by id2label and label2id.\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "# Sets the device where the PyTorch tensors will be allocated on.\n",
        "torch.device('cuda')"
      ],
      "metadata": {
        "id": "TVs-Va_Q0iap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of labels in the model’s configuration.\n",
        "# Classification tasks where the model needs to predict one label out of several possible ones.\n",
        "# num_labels attribute tells how many different labels the model can predict.\n",
        "model.config.num_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "kzlO1hGE0mXl",
        "outputId": "15bad2d2-5919-49dc-8c74-e54cde209db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-086d49ce91fe>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Classification tasks where the model needs to predict one label out of several possible ones.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# num_labels attribute tells how many different labels the model can predict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Huggingface Hub Settings (not an obligation)\n",
        "# interact with the Hugging Face Model Hub:\n",
        "\n",
        "# Import the notebook_login and create_repo functions from the huggingface_hub library.\n",
        "from huggingface_hub import notebook_login, create_repo\n",
        "\n",
        "# The notebook_login function is then called to log into the Hugging Face Model Hub from a Jupyter notebook.\n",
        "notebook_login()\n",
        "access_token = \"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\"\n"
      ],
      "metadata": {
        "id": "GawRoY1H0pYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Createa a new repository\n",
        "create_repo(\"csariyildiz/bert-finetuned-ner4\", private=False)\n",
        "\n",
        "# Push the tokenizer to the repository on the Hugging Face Model Hub.\n",
        "tokenizer.push_to_hub(\"csariyildiz/bert-finetuned-ner4\")"
      ],
      "metadata": {
        "id": "aVGkhJ5B1dyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8 : Define the Training Arguments"
      ],
      "metadata": {
        "id": "pRNegN_FOt8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  imports the accelerate library, which is a PyTorch utility for easy multi-GPU and TPU training.\n",
        "import accelerate\n",
        "\n",
        "# Import the TrainingArguments class from the transformers library.\n",
        "# Class is used to set various parameters for training a model.\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "\n",
        "    # Output directory where the model predictions and checkpoints will be written.\n",
        "    \"bert-finetuned-ner4\",\n",
        "\n",
        "    # Model checkpoint will be saved at the end of each epoch.\n",
        "    evaluation_strategy=\"epoch\",\n",
        "\n",
        "    # Model will be evaluated at the end of each epoch.\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    # Learning rate for the optimizer.\n",
        "    # Controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
        "    learning_rate=2e-5,\n",
        "\n",
        "    # Total number of training epochs to perform.\n",
        "    # An epoch is one complete pass through the entire training dataset.\n",
        "    num_train_epochs=3,\n",
        "\n",
        "    # Weight decay to apply (if not zero).\n",
        "    # Weight decay is a regularization technique by adding a small penalty, usually the L2 norm of the weights, to the loss function to reduce overfitting.\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # This means the model, tokenizer, and model configuration will be pushed to the Hugging Face Model Hub at each save.\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "BeieZ07g00_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will see that some of that turned hyperparameters used in the training of a machine learning model.\n",
        "\n",
        "* *learning_rate: 2e-05:* The learning rate is a hyperparameter that determines the step size at each iteration while moving toward a minimum of a loss function. In this case, the learning rate is set to 0.00002.\n",
        "\n",
        "* train_batch_size: 8: This is the number of training examples utilized in one iteration. The batch size can significantly impact the model’s performance and the speed of training.\n",
        "\n",
        "* eval_batch_size: 8: This is the number of evaluation examples utilized in one iteration. It’s similar to train_batch_size but is used during the evaluation phase.\n",
        "\n",
        "* seed: 42: A seed is used in generating random numbers, which can be useful for reproducibility. Here, the seed is set to 42.\n",
        "\n",
        "* optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08: This refers to the optimization algorithm used to minimize the loss function. Adam is a popular choice. Betas are coefficients used for computing running averages of gradient and its square, and epsilon is a very small number to prevent any division by zero in the implementation.\n",
        "\n",
        "* lr_scheduler_type: linear: This refers to the learning rate scheduling. In this case, a linear scheduler is used, which decreases the learning rate linearly from the initial learning rate to 0 over the course of training.\n",
        "\n",
        "* num_epochs: 3: An epoch is one complete pass through the entire training dataset. The number of epochs is a hyperparameter that defines the number of times the learning algorithm will work through the entire training dataset. In this case, the model will be trained for 3 epochs."
      ],
      "metadata": {
        "id": "NBhl5zTUPfQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9 : Begin Training"
      ],
      "metadata": {
        "id": "2ihn_uIKJDUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This line imports the Trainer class from the transformers library.\n",
        "# This class provides a simple way to train and fine-tune the models.\n",
        "from transformers import Trainer\n",
        "\n",
        "# This line creates an instance of the Trainer class with the specified parameters\n",
        "trainer = Trainer(\n",
        "    # This is the model that will be trained.\n",
        "    model=model,\n",
        "    # These are the training arguments that define the training setup.\n",
        "    args=args,\n",
        "    # This is the training dataset.\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    # This is the validation dataset.\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    # This is the function that will be used to form a batch by collating several samples together.\n",
        "    data_collator=data_collator,\n",
        "    # Function that will be used to compute metrics for evaluation.\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "Sw-HHkl903GD",
        "outputId": "380c8494-bd22-40b8-8dd4-df99e4a338a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cb52e6629dd7>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m trainer = Trainer(\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# This is the model that will be trained.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# These are the training arguments that define the training setup.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10 : Evaluate the Results"
      ],
      "metadata": {
        "id": "AK-RkGtxrQz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of a machine learning model training process.\n",
        "\n",
        "- **Epoch**: This is one complete pass through the entire training dataset. You've completed 3 epochs.\n",
        "- **Training Loss**: This is a measure of how well the model fits the training data. It decreases with each epoch, which is a good sign that the model is learning.\n",
        "- **Validation Loss**: This is a measure of the model's performance on the validation dataset. It's used to prevent overfitting to the training data.\n",
        "- **Precision, Recall, F1, Accuracy**: These are metrics used to evaluate the model's performance. Higher values are generally better.\n",
        "- **TrainOutput**: This contains additional information about the training process:\n",
        "    - `global_step`: The total number of steps (batches) during training.\n",
        "    - `training_loss`: The final training loss.\n",
        "    - `train_runtime`: The total runtime of the training in seconds.\n",
        "    - `train_samples_per_second`: The number of training samples processed per second.\n",
        "    - `train_steps_per_second`: The number of training steps taken per second.\n",
        "    - `total_flos`: The total number of floating point operations.\n",
        "    - `epoch`: The total number of epochs completed.\n",
        "\n",
        "Model has been trained successfully and has good performance based on the metrics.\n"
      ],
      "metadata": {
        "id": "jHf5goQvQ-gG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(commit_message=\"Training complete\")"
      ],
      "metadata": {
        "id": "SuG1ExhL05fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aditionals"
      ],
      "metadata": {
        "id": "j40CR8-gSljg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
        ")"
      ],
      "metadata": {
        "id": "9TNYsJxt09FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ],
      "metadata": {
        "id": "RjTdu91G1CTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "tmOWWIic1D9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ],
      "metadata": {
        "id": "s93R3orR1FiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 3\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ],
      "metadata": {
        "id": "6Bpb6_pb1HEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import Repository, get_full_repo_name\n",
        "\n",
        "model_name = \"bert-finetuned-ner-accelerate\"\n",
        "repo_name = get_full_repo_name(model_name)\n",
        "repo_name"
      ],
      "metadata": {
        "id": "LbMjPh2L1Is5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"bert-finetuned-ner-accelerate\"\n",
        "repo = Repository(output_dir, clone_from=repo_name)"
      ],
      "metadata": {
        "id": "qTjyUO3g1KVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess(predictions, labels):\n",
        "    predictions = predictions.detach().cpu().clone().numpy()\n",
        "    labels = labels.detach().cpu().clone().numpy()\n",
        "\n",
        "    # Remove ignored index (special tokens) and convert to labels\n",
        "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    return true_labels, true_predictions"
      ],
      "metadata": {
        "id": "0lwPwL_51Kfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    for batch in eval_dataloader:\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        predictions = outputs.logits.argmax(dim=-1)\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        # Necessary to pad predictions and labels for being gathered\n",
        "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
        "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
        "\n",
        "        predictions_gathered = accelerator.gather(predictions)\n",
        "        labels_gathered = accelerator.gather(labels)\n",
        "\n",
        "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
        "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "    results = metric.compute()\n",
        "    print(\n",
        "        f\"epoch {epoch}:\",\n",
        "        {\n",
        "            key: results[f\"overall_{key}\"]\n",
        "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Save and upload\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "    if accelerator.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        repo.push_to_hub(\n",
        "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
        "        )"
      ],
      "metadata": {
        "id": "fjeT4Xzc1Qlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accelerator.wait_for_everyone()\n",
        "unwrapped_model = accelerator.unwrap_model(model)\n",
        "unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)"
      ],
      "metadata": {
        "id": "lgLIVh7l1SuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Replace this with your own checkpoint\n",
        "model_checkpoint = \"csariyildiz/bert-finetuned-ner4\"\n",
        "token_classifier = pipeline(\n",
        "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
        ")\n",
        "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ],
      "metadata": {
        "id": "_IQva3g71UZc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}