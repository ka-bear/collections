{"metadata":{"deepnote_notebook_id":"a458e12a9e1b47c689218e6a20bcfcb0","deepnote_execution_queue":[],"colab":{"provenance":[]},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Example: POS Tagging\n\nAccording to [Wikipedia](https://en.wikipedia.org/wiki/Part-of-speech_tagging):\n\n> Part-of-speech tagging (POS tagging or PoS tagging or POST) is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context—i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph.\n\nFormally, given a sequence of words $\\mathbf{x} = \\left< x_1, x_2, \\ldots, x_t \\right>$ the goal is to learn a model $P(y_i \\,|\\, \\mathbf{x})$ where $y_i$ is the POS tag associated with the $x_i$.\nNote that the model is conditioned on all of $\\mathbf{x}$ not just the words that occur earlier in the sentence - this is because we can assume that the entire sentence is known at the time of tagging.\n\n### Dataset\n\nWe will train our model on the [Engligh Dependencies Treebank](https://github.com/UniversalDependencies/UD_English).\nYou can download this dataset by running the following lines:","metadata":{"cell_id":"81e027952c094ce1b1742a1222b82d77","deepnote_cell_type":"markdown","id":"2SrkISjXoKK2"}},{"cell_type":"code","source":"!pip install gdown\n!pip install ray","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:12:33.559290Z","iopub.execute_input":"2024-06-17T16:12:33.559635Z","iopub.status.idle":"2024-06-17T16:12:59.111819Z","shell.execute_reply.started":"2024-06-17T16:12:33.559605Z","shell.execute_reply":"2024-06-17T16:12:59.110660Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.2.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.2.0\nRequirement already satisfied: ray in /opt/conda/lib/python3.10/site-packages (2.9.0)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from ray) (8.1.7)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from ray) (3.13.1)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from ray) (4.20.0)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray) (1.0.7)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ray) (21.3)\nRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray) (3.20.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from ray) (6.0.1)\nRequirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray) (1.3.1)\nRequirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray) (1.4.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from ray) (2.32.3)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray) (0.16.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ray) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->ray) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->ray) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->ray) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->ray) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import gdown\nurl = \"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu\"\noutput = \"en_ewt-ud-dev.conllu\"\ngdown.download(url, output, quiet=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"UwlQccQetgMq","outputId":"502155bf-1d1b-4618-b2a0-c59b8b6fc1ed","execution":{"iopub.status.busy":"2024-06-17T16:12:59.114136Z","iopub.execute_input":"2024-06-17T16:12:59.114505Z","iopub.status.idle":"2024-06-17T16:12:59.597734Z","shell.execute_reply.started":"2024-06-17T16:12:59.114467Z","shell.execute_reply":"2024-06-17T16:12:59.596865Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu\nTo: /kaggle/working/en_ewt-ud-dev.conllu\n1.76MB [00:00, 48.5MB/s]                  \n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'en_ewt-ud-dev.conllu'"},"metadata":{}}]},{"cell_type":"code","source":"url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu\"\noutput = \"en_ewt-ud-test.conllu\"\ngdown.download(url, output, quiet=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"uQimwagRw1EI","outputId":"eb6f90af-d1c7-4247-b531-a03c0a09a25a","execution":{"iopub.status.busy":"2024-06-17T16:12:59.598922Z","iopub.execute_input":"2024-06-17T16:12:59.599386Z","iopub.status.idle":"2024-06-17T16:12:59.794284Z","shell.execute_reply.started":"2024-06-17T16:12:59.599353Z","shell.execute_reply":"2024-06-17T16:12:59.793427Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu\nTo: /kaggle/working/en_ewt-ud-test.conllu\n1.77MB [00:00, 45.1MB/s]                  \n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'en_ewt-ud-test.conllu'"},"metadata":{}}]},{"cell_type":"code","source":"url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\"\noutput = \"en_ewt-ud-train.conllu\"\ngdown.download(url, output, quiet=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"ddJ8LKLIw-Rn","outputId":"a598b0ff-67d1-4340-ca4c-187a680bf36d","execution":{"iopub.status.busy":"2024-06-17T16:12:59.796270Z","iopub.execute_input":"2024-06-17T16:12:59.796549Z","iopub.status.idle":"2024-06-17T16:13:00.041376Z","shell.execute_reply.started":"2024-06-17T16:12:59.796524Z","shell.execute_reply":"2024-06-17T16:13:00.040507Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\nTo: /kaggle/working/en_ewt-ud-train.conllu\n13.9MB [00:00, 141MB/s]                    \n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'en_ewt-ud-train.conllu'"},"metadata":{}}]},{"cell_type":"markdown","source":"The individual data instances come in chunks seperated by blank lines. Each chunk consists of a few starting comments, and then lines of tab-seperated fields. The fields we are interested in are the 1st and 3rd, which contain the tokenized word and POS tag respectively. An example chunk is shown below:\n\n```\n# sent_id = answers-20111107193044AAvUYBv_ans-0023\n# text = Hope you have a crapload of fun!\n1\tHope\thope\tVERB\tVBP\tMood=Ind|Tense=Pres|VerbForm=Fin\t0\troot\t0:root\t_\n2\tyou\tyou\tPRON\tPRP\tCase=Nom|Person=2|PronType=Prs\t3\tnsubj\t3:nsubj\t_\n3\thave\thave\tVERB\tVBP\tMood=Ind|Tense=Pres|VerbForm=Fin\t1\tccomp\t1:ccomp\t_\n4\ta\ta\tDET\tDT\tDefinite=Ind|PronType=Art\t5\tdet\t5:det\t_\n5\tcrapload\tcrapload\tNOUN\tNN\tNumber=Sing\t3\tobj\t3:obj\t_\n6\tof\tof\tADP\tIN\t_\t7\tcase\t7:case\t_\n7\tfun\tfun\tNOUN\tNN\tNumber=Sing\t5\tnmod\t5:nmod\tSpaceAfter=No\n8\t!\t!\tPUNCT\t.\t_\t1\tpunct\t1:punct\t_\n\n```","metadata":{"cell_id":"5c715f5de80e4dd6bc04fff44ffb8ff8","deepnote_cell_type":"markdown","id":"TTl_3ZhfoKK6"}},{"cell_type":"markdown","source":"As with most real world data, we are going to need to do some preprocessing before we can use it. The first thing we are going to need is a `Vocabulary` to map words/POS tags to integer ids. Here is a more full-featured implementation than what we used in the first tutorial:","metadata":{"cell_id":"8889552cda3e4df58ed9302032585655","deepnote_cell_type":"markdown","id":"vBxR-xVLoKK6"}},{"cell_type":"code","source":"from collections import Counter\n\n\nclass Vocab(object):\n    def __init__(self, iter, max_size=None, sos_token=None, eos_token=None, unk_token=None):\n        \"\"\"Initialize the vocabulary.\n        Args:\n            iter: An iterable which produces sequences of tokens used to update\n                the vocabulary.\n            max_size: (Optional) Maximum number of tokens in the vocabulary.\n            sos_token: (Optional) Token denoting the start of a sequence.\n            eos_token: (Optional) Token denoting the end of a sequence.\n            unk_token: (Optional) Token denoting an unknown element in a\n                sequence.\n        \"\"\"\n        self.max_size = max_size\n        self.pad_token = '<pad>'\n        self.sos_token = sos_token\n        self.eos_token = eos_token\n        self.unk_token = unk_token\n\n        # Add special tokens.\n        id2word = [self.pad_token]\n        if sos_token is not None:\n            id2word.append(self.sos_token)\n        if eos_token is not None:\n            id2word.append(self.eos_token)\n        if unk_token is not None:\n            id2word.append(self.unk_token)\n\n        # Update counter with token counts.\n        counter = Counter()\n        for x in iter:\n            counter.update(x)\n\n        # Extract lookup tables.\n        if max_size is not None:\n            counts = counter.most_common(max_size)\n        else:\n            counts = counter.items()\n            counts = sorted(counts, key=lambda x: x[1], reverse=True)\n        words = [x[0] for x in counts]\n        id2word.extend(words)\n        word2id = {x: i for i, x in enumerate(id2word)}\n\n        self._id2word = id2word\n        self._word2id = word2id\n\n    def __len__(self):\n        return len(self._id2word)\n\n    def word2id(self, word):\n        \"\"\"Map a word in the vocabulary to its unique integer id.\n        Args:\n            word: Word to lookup.\n        Returns:\n            id: The integer id of the word being looked up.\n        \"\"\"\n        if word in self._word2id:\n            return self._word2id[word]\n        elif self.unk_token is not None:\n            return self._word2id[self.unk_token]\n        else:\n            raise KeyError('Word \"%s\" not in vocabulary.' % word)\n\n    def id2word(self, id):\n        \"\"\"Map an integer id to its corresponding word in the vocabulary.\n        Args:\n            id: Integer id of the word being looked up.\n        Returns:\n            word: The corresponding word.\n        \"\"\"\n        return self._id2word[id]","metadata":{"cell_id":"717aa9219d174df8b1b60e34916abc78","deepnote_cell_type":"code","id":"aOkJywlDoKK6","execution":{"iopub.status.busy":"2024-06-17T16:13:00.042974Z","iopub.execute_input":"2024-06-17T16:13:00.043478Z","iopub.status.idle":"2024-06-17T16:13:00.055597Z","shell.execute_reply.started":"2024-06-17T16:13:00.043443Z","shell.execute_reply":"2024-06-17T16:13:00.054735Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Now we need to parse the .conllu files and extract the data needed for our model. The good news is that the file is only a few megabytes so we can store everything in memory. Rather than creating a generator from scratch like we did in the previous tutorial, we will instead showcase the `torch.utils.data.Dataset` class. There are two main things that a `Dataset` must have:\n\n1. A `__len__` method which let's you know how many data points are in the dataset.\n2. A `__getitem__` method which is used to support integer indexing.\n\nHere's an example of how to define these methods for the English Dependencies Treebank data.","metadata":{"cell_id":"d23e9223e56c48128ccc2da382f3c39e","deepnote_cell_type":"markdown","id":"Tqr8Hw21oKK7"}},{"cell_type":"code","source":"import re\nfrom torch.utils.data import Dataset\n\n\nclass Annotation(object):\n    def __init__(self):\n        \"\"\"A helper object for storing annotation data.\"\"\"\n        self.tokens = []\n        self.pos_tags = []\n\n\nclass CoNLLDataset(Dataset):\n    def __init__(self, fname):\n        \"\"\"Initializes the CoNLLDataset.\n        Args:\n            fname: The .conllu file to load data from.\n        \"\"\"\n        self.fname = fname\n        self.annotations = self.process_conll_file(fname)\n        self.token_vocab = Vocab([x.tokens for x in self.annotations],\n                                 unk_token='<unk>')\n        self.pos_vocab = Vocab([x.pos_tags for x in self.annotations])\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        annotation = self.annotations[idx]\n        input = [self.token_vocab.word2id(x) for x in annotation.tokens]\n        target = [self.pos_vocab.word2id(x) for x in annotation.pos_tags]\n        return input, target\n\n    def process_conll_file(self, fname):\n        # Read the entire file.\n        with open(fname, 'r') as f:\n            raw_text = f.read()\n        # Split into chunks on blank lines.\n        chunks = re.split(r'^\\n', raw_text, flags=re.MULTILINE)\n        # Process each chunk into an annotation.\n        annotations = []\n        for chunk in chunks:\n            annotation = Annotation()\n            lines = chunk.split('\\n')\n            # Iterate over all lines in the chunk.\n            for line in lines:\n                # If line is empty ignore it.\n                if len(line)==0:\n                    continue\n                # If line is a commend ignore it.\n                if line[0] == '#':\n                    continue\n                # Otherwise split on tabs and retrieve the token and the\n                # POS tag fields.\n                fields = line.split('\\t')\n                annotation.tokens.append(fields[1])\n                annotation.pos_tags.append(fields[3])\n            if (len(annotation.tokens) > 0) and (len(annotation.pos_tags) > 0):\n                annotations.append(annotation)\n        return annotations","metadata":{"cell_id":"3867af02ac2644839d8919751374f0f8","deepnote_cell_type":"code","id":"1zcRDR7ooKK7","execution":{"iopub.status.busy":"2024-06-17T16:13:00.056851Z","iopub.execute_input":"2024-06-17T16:13:00.057152Z","iopub.status.idle":"2024-06-17T16:13:03.381865Z","shell.execute_reply.started":"2024-06-17T16:13:00.057127Z","shell.execute_reply":"2024-06-17T16:13:03.380852Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"And let's see how this is used in practice.","metadata":{"cell_id":"85c38fbb082a410bb13af9edf4858c1d","deepnote_cell_type":"markdown","id":"TaU7d1LpoKK7"}},{"cell_type":"code","source":"dataset = CoNLLDataset('en_ewt-ud-train.conllu')","metadata":{"cell_id":"9a068884e273430c86cd54848222fac2","deepnote_cell_type":"code","id":"YGQ6_s49oKK7","execution":{"iopub.status.busy":"2024-06-17T16:13:03.383144Z","iopub.execute_input":"2024-06-17T16:13:03.383556Z","iopub.status.idle":"2024-06-17T16:13:03.974023Z","shell.execute_reply.started":"2024-06-17T16:13:03.383529Z","shell.execute_reply":"2024-06-17T16:13:03.973031Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"input, target = dataset[0]\nprint('Example input: %s\\n' % input)\nprint('Example target: %s\\n' % target)\nprint('Translated input: %s\\n' % ' '.join(dataset.token_vocab.id2word(x) for x in input))\nprint('Translated target: %s\\n' % ' '.join(dataset.pos_vocab.id2word(x) for x in target))","metadata":{"cell_id":"bed53de2b73d43de8965fb23d2a5a717","deepnote_cell_type":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"LPeOna-roKK7","outputId":"c38a97f4-db9f-47ce-929c-a935fd817bdf","execution":{"iopub.status.busy":"2024-06-17T16:13:03.975354Z","iopub.execute_input":"2024-06-17T16:13:03.975725Z","iopub.status.idle":"2024-06-17T16:13:03.982020Z","shell.execute_reply.started":"2024-06-17T16:13:03.975687Z","shell.execute_reply":"2024-06-17T16:13:03.981092Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Example input: [266, 16, 5249, 45, 295, 703, 1154, 4233, 10099, 595, 16, 10100, 4, 3, 6865, 35, 3, 6866, 10, 3, 498, 8, 6867, 4, 758, 3, 2224, 1605, 2]\n\nExample target: [9, 2, 9, 2, 7, 1, 3, 9, 9, 9, 2, 9, 2, 6, 1, 5, 6, 1, 5, 6, 1, 5, 9, 2, 5, 6, 7, 1, 2]\n\nTranslated input: Al - Zaman : American forces killed Shaikh Abdullah al - Ani , the preacher at the mosque in the town of Qaim , near the Syrian border .\n\nTranslated target: PROPN PUNCT PROPN PUNCT ADJ NOUN VERB PROPN PROPN PROPN PUNCT PROPN PUNCT DET NOUN ADP DET NOUN ADP DET NOUN ADP PROPN PUNCT ADP DET ADJ NOUN PUNCT\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The main upshot of using the `Dataset` class is that it makes accessing training/test observations very simple. Accordingly, this makes batch generation easy since all we need to do is randomly choose numbers and then grab those observations from the dataset - PyTorch includes a `torch.utils.data.DataLoader` object which handles this for you. In fact, if we were not working with sequential data we would be able to proceed straight to the modeling step from here. However, since we are working with sequential data there is one last pesky issue we need to handle - padding.\n\nThe issue is that when we are given a batch of outputs from `CoNLLDataset`, the sequences in the batch are likely to all be of different length. To deal with this, we define a custom `collate_annotations` function which adds padding to the end of the sequences in the batch so that they are all the same length. In addition, we'll have this function take care of loading the data into tensors and ensuring that the tensor dimensions are in the order expected by PyTorch.\n\nOh and one last annoying thing - to deal with some of the issues caused by using padded data we will be using a function called `torch.nn.utils.rnn.pack_padded_sequences` in our model later on. All you need to know now is that this function expects our sequences in the batch to be sorted in terms of descending length, and that we know the lengths of each sequence. So we will make sure that the `collate_annotations` function performs this sorting for us and returns the sequence lengths in addition to the input and target tensors.","metadata":{"cell_id":"1cc73f92ecd64f0289e61aeb8077979b","deepnote_cell_type":"markdown","id":"Ycbj4TF6oKK8"}},{"cell_type":"code","source":"import torch\nfrom torch.autograd import Variable\n\n\ndef pad(sequences, max_length, pad_value=0):\n    \"\"\"Pads a list of sequences.\n    Args:\n        sequences: A list of sequences to be padded.\n        max_length: The length to pad to.\n        pad_value: The value used for padding.\n    Returns:\n        A list of padded sequences.\n    \"\"\"\n    out = []\n    for sequence in sequences:\n        padded = sequence + [0]*(max_length - len(sequence))\n        out.append(padded)\n    return out\n\n\ndef collate_annotations(batch):\n    \"\"\"Function used to collate data returned by CoNLLDataset.\"\"\"\n    # Get inputs, targets, and lengths.\n    inputs, targets = zip(*batch)\n    lengths = [len(x) for x in inputs]\n    # Sort by length.\n    sort = sorted(zip(inputs, targets, lengths),\n                  key=lambda x: x[2],\n                  reverse=True)\n    inputs, targets, lengths = zip(*sort)\n    # Pad.\n    max_length = max(lengths)\n    inputs = pad(inputs, max_length)\n    targets = pad(targets, max_length)\n    # Transpose.\n    inputs = list(map(list, zip(*inputs)))\n    targets = list(map(list, zip(*targets)))\n    # Convert to PyTorch variables.\n    inputs = Variable(torch.LongTensor(inputs))\n    targets = Variable(torch.LongTensor(targets))\n    lengths = Variable(torch.LongTensor(lengths))\n    if torch.cuda.is_available():\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        lengths = lengths.cuda()\n    return inputs, targets, lengths","metadata":{"cell_id":"8f553fd12b5545428c86fd7e29b476cb","deepnote_cell_type":"code","id":"XPBu1WeBoKK8","execution":{"iopub.status.busy":"2024-06-17T16:13:03.983328Z","iopub.execute_input":"2024-06-17T16:13:03.983673Z","iopub.status.idle":"2024-06-17T16:13:03.996276Z","shell.execute_reply.started":"2024-06-17T16:13:03.983638Z","shell.execute_reply":"2024-06-17T16:13:03.995163Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Again let's see how this is used in practice:","metadata":{"cell_id":"f7ec9ddc62364b94b6d67469e007f3cf","deepnote_cell_type":"markdown","id":"VOYmULYaoKK9"}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n\nfor inputs, targets, lengths in DataLoader(dataset, batch_size=16, collate_fn=collate_annotations):\n    print('Inputs: %s\\n' % inputs.data)\n    print('Targets: %s\\n' % targets.data)\n    print('Lengths: %s\\n' % lengths.data)\n\n    # Usually we'd keep sampling batches, but here we'll just break\n    break","metadata":{"scrolled":true,"cell_id":"e202cebd5a3247e9876364f931771ad1","deepnote_cell_type":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"m7REdnPCoKK9","outputId":"59fd325f-47d6-480e-dd6c-1c611bbf0ca6","execution":{"iopub.status.busy":"2024-06-17T16:13:04.000012Z","iopub.execute_input":"2024-06-17T16:13:04.000526Z","iopub.status.idle":"2024-06-17T16:13:04.231339Z","shell.execute_reply.started":"2024-06-17T16:13:04.000484Z","shell.execute_reply":"2024-06-17T16:13:04.230406Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Inputs: tensor([[   28,  1083,   266,    28,    30,   106,    68,   266,   499,   625,\n         10103,   121,  1212,    28,    28,   108],\n        [10106,     3,    16,  1713,  6874,  6878, 10115,    16,  1030,   106,\n            45, 10123,     8,  3581,  1081,  1606],\n        [   10,  5252,  5249,  4237,    11,    11,    46,  5249,  4239,  1712,\n           555,     4,    69,    60,    19,    54],\n        [  180,    19,    45,     8,    10,     3,   185,    45,    51,     8,\n          1849,  6874,    60,  1370,   159,    41],\n        [   11,   343,   295, 10118, 10125,   759,   138,  5253, 10121,     7,\n          2018,  3111,   159,    10,   450,    19],\n        [ 4234,   163,   703,  3111,   180,  1031,     8,  1154,     7, 10101,\n            12,     4,   450,     3,    44, 10111],\n        [    5,     5,  1154,  2018,     6,    10,     3,     7, 10122, 10102,\n            31,   151,    44, 10112,     3,     3],\n        [    3,   408,  4233,    12,    50,     3,  2755,   807,  3112,    32,\n            51,   219,   144,     6,   607,   582],\n        [  142,  1470, 10099,  2756,    52,   207,  1851,     8,     6,    22,\n         10104,  1714,   704,   595,     8,    21],\n        [ 1029,    10,   595,    51,  6875,     8,    72,     3,    66,  3580,\n            61,    60,     8,    16,    52,    66],\n        [    4,  6871,    16,  4238,  2225,  2756,    60, 10116,   738,   140,\n           172,  2469,     3,  1290,  2019,  3110],\n        [   57,     6, 10100,  1607,   905,    10,  6873,  5254,   231,  1469,\n          1080,   136,  5250, 10113,  1371,  2020],\n        [   25,  4235,     4,  2021,     4,     3,   320,  1852,    31,    14,\n          3581,    31,     8,     8,     2,     2],\n        [   48,    61,     3,    10,    72,  1372,    14,   132,    60,   154,\n          1370,    84,     3,     3,     0,     0],\n        [   22,  1155,  6865,  5255,    23,    29,     3, 10117,    20,     5,\n            10,    22, 10105,  1082,     0,     0],\n        [   62,  3581,    35,     4,    20,  3113,    86,    82,    63,   216,\n          1850, 10124,    49,     2,     0,     0],\n        [  309,     4,     3, 10119,     3,     5,   660,    10,   111,     2,\n             2,     2,     0,     0,     0,     0],\n        [ 5251,     9,  6866,    58,  6876,     3,    10,  5255,   234,   626,\n             0,     0,     0,     0,     0,     0],\n        [10107,    96,    10,    51,    14,  2226,   904,     2,     2,     0,\n             0,     0,     0,     0,     0,     0],\n        [10108,    36,     3,    70,  6877,    16,     2,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [10109,    38,   498,     7, 10126,  1715,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 6868,   181,     8,   378,     6,     8,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [   35,     3,  6867,  2022,  5256,     3,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    7,   705,     4,    10,   906,   321,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [  362,    12,   758,     3,    69,     2,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [  596,     3,     3,   207,    10,    27,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [  513,  6872,  2224,     8,     2,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    8,   683,  1605, 10120,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    3,     5,     2,     2,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [10110, 10114,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 6869,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [  100,   555,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [   10,   409,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    3,    78,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 6870,  4236,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    2,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0]], device='cuda:0')\n\nTargets: tensor([[ 6, 14,  9,  6,  2,  6,  4,  9,  5,  2,  9,  5, 13,  6,  6,  4],\n        [ 9,  6,  2,  1,  9,  1,  3,  2,  9,  6,  2,  9,  5,  1,  7,  3],\n        [ 5,  1,  9,  1,  8,  8,  4,  9,  1,  1,  7,  2,  4,  8,  8, 14],\n        [ 9,  8,  2,  5,  5,  6, 10,  2,  8,  5,  1,  9,  8,  3,  8,  4],\n        [ 8, 10,  7,  9,  7,  7, 10,  1,  3,  6,  3,  9,  8,  5,  3,  8],\n        [ 7,  3,  1,  9,  9,  1,  5,  3,  6,  7, 14,  2,  3,  6,  5,  3],\n        [ 5, 12,  3,  3, 11,  5,  6,  6,  7,  1,  4, 13,  5,  9,  6,  6],\n        [ 6,  3,  9, 14,  3,  6,  9,  1,  1,  8,  8,  1, 13, 11,  1,  1],\n        [ 9,  1,  9,  1,  6,  1,  1,  5, 11,  8,  3,  1,  1,  9,  5,  5],\n        [ 9, 14,  9,  8, 10,  5,  4,  6,  4,  3,  5,  8,  5,  2,  6,  4],\n        [ 2,  3,  2,  3,  7,  1,  8,  9,  1,  4, 13,  3,  6,  9,  1,  1],\n        [10, 11,  9, 13,  1,  5, 10,  7, 14,  1,  7, 14,  9,  1,  1,  1],\n        [ 4,  3,  2,  1,  2,  6,  3,  9,  4,  5,  1,  4,  5,  5,  2,  2],\n        [ 8,  5,  6,  5,  4,  1,  5, 14,  8,  1,  3,  8,  6,  6,  0,  0],\n        [ 8,  1,  1,  9,  8, 14,  6,  3,  5, 12,  5,  8,  9,  1,  0,  0],\n        [14,  1,  5,  2,  5,  3,  7,  4,  4,  3,  9,  3,  2,  2,  0,  0],\n        [ 3,  2,  6, 14,  6,  5,  1,  5,  1,  2,  2,  2,  0,  0,  0,  0],\n        [ 9,  4,  1,  4,  1,  6,  5,  9, 10,  2,  0,  0,  0,  0,  0,  0],\n        [ 9, 15,  5,  8,  5,  1,  9,  2,  2,  0,  0,  0,  0,  0,  0,  0],\n        [ 9,  8,  6,  3,  9,  2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [10, 12,  1,  6,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 3,  3,  5,  7, 11,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 5,  6,  9,  1, 10,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 6,  1,  2,  5,  3,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 7, 14,  5,  6,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 1,  6,  6,  1,  5,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 1,  9,  7,  5,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 5,  3,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 6, 12,  2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 9,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 1,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [10,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 5,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 6, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 1,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n       device='cuda:0')\n\nLengths: tensor([36, 36, 29, 29, 27, 26, 20, 19, 19, 18, 17, 17, 16, 16, 13, 13],\n       device='cuda:0')\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Model\n\nWe will use the following architecture:\n\n1. Embed the input words into a 200 dimensional vector space.\n2. Feed the word embeddings into a (bidirectional) GRU.\n3. Feed the GRU outputs into a fully connected layer.\n4. Use a softmax activation to get the probabilities of the different labels.\n\nThere is one complication which arises during the forward computation. As was noted in the dataset section, the input sequences are padded. This causes an issue since we do not want to waste computational resources feeding these pad tokens into the RNN. In PyTorch, we can deal with this issue by converting the sequence data into a  `torch.nn.utils.rnn.PackedSequence` object before feeding it into the RNN. In essence, a `PackedSequence` flattens the sequence and batch dimensions of a tensor, and also contains metadata so that PyTorch knows when to re-initialize the hidden state when fed into a recurrent layer. If this seems confusing, do not worry. To use the `PackedSequence` in practice you will almost always perform the following steps:\n\n1. Before feeding data into a recurrent layer, transform it into a `PackedSequence` by using the function `torch.nn.utils.rnn.pack_padded_sequence()`.\n2. Feed the `PackedSequence` into the recurrent layer.\n3. Transform the output back into a regular tensor by using the function `torch.nn.utils.rnn.pad_packed_sequence()`.\n\nSee the model implementation below for a working example:","metadata":{"cell_id":"17e7685b51374eb4b5d988e5e70feb4f","deepnote_cell_type":"markdown","id":"dvxXuiKLoKK9"}},{"cell_type":"code","source":"from torch import nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass Tagger(nn.Module):\n    def __init__(self,\n                 input_vocab_size,\n                 output_vocab_size,\n                 embedding_dim=64,\n                 hidden_size=64,\n                 bidirectional=True):\n        \"\"\"Initializes the tagger.\n\n        Args:\n            input_vocab_size: Size of the input vocabulary.\n            output_vocab_size: Size of the output vocabulary.\n            embedding_dim: Dimension of the word embeddings.\n            hidden_size: Number of units in each LSTM hidden layer.\n            bidirectional: Whether or not to use a bidirectional rnn.\n        \"\"\"\n        super(Tagger, self).__init__()\n\n        # Store parameters\n        self.input_vocab_size = input_vocab_size\n        self.output_vocab_size = output_vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_size = hidden_size\n        self.bidirectional = bidirectional\n\n        # Define layers\n        self.word_embeddings = nn.Embedding(input_vocab_size, embedding_dim,\n                                            padding_idx=0)\n        self.rnn = nn.GRU(embedding_dim, hidden_size,\n                          bidirectional=bidirectional,\n                          dropout=0.9)\n        if bidirectional:\n            self.fc = nn.Linear(2*hidden_size, output_vocab_size)\n        else:\n            self.fc = nn.Linear(hidden_size, output_vocab_size)\n        self.activation = nn.LogSoftmax(dim=2)\n\n    def forward(self, x, lengths=None, hidden=None):\n        \"\"\"Computes a forward pass of the language model.\n\n        Args:\n            x: A LongTensor w/ dimension [seq_len, batch_size].\n            lengths: The lengths of the sequences in x.\n            hidden: Hidden state to be fed into the lstm.\n\n        Returns:\n            net: the output representation for each word in the sequence.\n            hidden: the hidden state at the last timestamp.\n        \"\"\"\n        seq_len, batch_size = x.size()\n\n        # If no hidden state is provided, then default to zeros.\n        if hidden is None:\n            if self.bidirectional:\n                num_directions = 2\n            else:\n                num_directions = 1\n            hidden = Variable(torch.zeros(num_directions, batch_size, self.hidden_size))\n            if torch.cuda.is_available():\n                hidden = hidden.cuda()\n\n        net = self.word_embeddings(x)\n        # Pack before feeding into the RNN.\n        if lengths is not None:\n            lengths = lengths.data.view(-1).tolist()\n            net = pack_padded_sequence(net, lengths)\n        net, hidden = self.rnn(net, hidden)\n        # Unpack after\n        if lengths is not None:\n            net, _ = pad_packed_sequence(net)\n        net = self.fc(net)\n        net = self.activation(net)\n\n        return net, hidden","metadata":{"cell_id":"6b0e4a55ef8e4cd187c80f26f64ccebc","deepnote_cell_type":"code","id":"xqI81PhsoKK9","execution":{"iopub.status.busy":"2024-06-17T16:13:04.232634Z","iopub.execute_input":"2024-06-17T16:13:04.233357Z","iopub.status.idle":"2024-06-17T16:13:04.245780Z","shell.execute_reply.started":"2024-06-17T16:13:04.233321Z","shell.execute_reply":"2024-06-17T16:13:04.244873Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Training\n\nTraining is pretty much exactly the same as in the previous tutorial. There is one catch - we don't want to evaluate our loss function on pad tokens. This is easily fixed by setting the weight of the pad class to zero.","metadata":{"cell_id":"d890273612704134a8f8ce72b6765672","deepnote_cell_type":"markdown","id":"y40pNO31oKK9"}},{"cell_type":"code","source":"pip install -U ipywidgets","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:13:04.247241Z","iopub.execute_input":"2024-06-17T16:13:04.247592Z","iopub.status.idle":"2024-06-17T16:13:17.343162Z","shell.execute_reply.started":"2024-06-17T16:13:04.247548Z","shell.execute_reply":"2024-06-17T16:13:17.342031Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (7.7.1)\nCollecting ipywidgets\n  Downloading ipywidgets-8.1.3-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\nRequirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.20.0)\nRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\nCollecting widgetsnbextension~=4.0.11 (from ipywidgets)\n  Downloading widgetsnbextension-4.0.11-py3-none-any.whl.metadata (1.6 kB)\nCollecting jupyterlab-widgets~=3.0.11 (from ipywidgets)\n  Downloading jupyterlab_widgets-3.0.11-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\nDownloading ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jupyterlab_widgets-3.0.11-py3-none-any.whl (214 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n  Attempting uninstall: widgetsnbextension\n    Found existing installation: widgetsnbextension 3.6.6\n    Uninstalling widgetsnbextension-3.6.6:\n      Successfully uninstalled widgetsnbextension-3.6.6\n  Attempting uninstall: jupyterlab-widgets\n    Found existing installation: jupyterlab-widgets 3.0.9\n    Uninstalling jupyterlab-widgets-3.0.9:\n      Successfully uninstalled jupyterlab-widgets-3.0.9\n  Attempting uninstall: ipywidgets\n    Found existing installation: ipywidgets 7.7.1\n    Uninstalling ipywidgets-7.7.1:\n      Successfully uninstalled ipywidgets-7.7.1\nSuccessfully installed ipywidgets-8.1.3 jupyterlab-widgets-3.0.11 widgetsnbextension-4.0.11\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from functools import partial\nimport os\nimport tempfile\nfrom pathlib import Path\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import random_split\nimport torchvision\nimport torchvision.transforms as transforms\nfrom ray import tune\nfrom ray import train\nfrom ray.train import Checkpoint, get_checkpoint\nfrom ray.tune.schedulers import ASHAScheduler\nimport ray.cloudpickle as pickle","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:13:17.344940Z","iopub.execute_input":"2024-06-17T16:13:17.345391Z","iopub.status.idle":"2024-06-17T16:13:21.010482Z","shell.execute_reply.started":"2024-06-17T16:13:17.345348Z","shell.execute_reply":"2024-06-17T16:13:21.009505Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def tuning(config):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    batch_size = config[\"batch_size\"]\n    train_dataset = CoNLLDataset('/kaggle/working/en_ewt-ud-train.conllu')\n    dev_dataset = CoNLLDataset('/kaggle/working/en_ewt-ud-dev.conllu')\n    input_vocab_size = len(train_dataset.token_vocab)\n    output_vocab_size = len(train_dataset.pos_vocab)\n    model = Tagger(input_vocab_size, output_vocab_size, config[\"embed_dim\"], config[\"hidden_size\"]).to(device)\n    weight = torch.ones(output_vocab_size)\n    weight[0] = 0\n    if torch.cuda.is_available():\n        weight = weight.cuda()\n\n    # Initialize loss function and optimizer.\n    criterion = torch.nn.NLLLoss(weight)\n    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n    \n    checkpoint = get_checkpoint()\n    checkpoint = False\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            data_path = Path(checkpoint_dir) / \"data.pkl\"\n            with open(data_path, \"rb\") as fp:\n                checkpoint_state = pickle.load(fp)\n            start_epoch = checkpoint_state[\"epoch\"]\n            model.load_state_dict(checkpoint_state[\"net_state_dict\"])\n            optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n    else:\n        start_epoch = 0\n    \n    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                         collate_fn=collate_annotations)\n    valloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False,\n                        collate_fn=collate_annotations)\n    \n    for epoch in range(start_epoch, 10):  # loop over the dataset multiple times\n        running_loss = 0.0\n        epoch_steps = 0\n        i = 0\n        model.train()\n        for inputs, targets, lengths in trainloader:\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs, _ = model(inputs, lengths=lengths)\n\n            outputs = outputs.view(-1, output_vocab_size)\n            targets = targets.view(-1)\n            loss = criterion(outputs, targets)\n            running_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            epoch_steps += 1\n#             if i % 200 == 199:  # print every 200 mini-batches\n#                 print(\n#                     \"[%d, %5d] loss: %.3f\"\n#                     % (epoch + 1, i + 1, running_loss / 200)\n#                 )\n#                 running_loss = 0.0\n            i += 1\n        # Validation loss\n        val_loss = 0.0\n        val_steps = 0\n        total = 0\n        correct = 0\n        with torch.no_grad():\n            model.eval()\n            for inputs, targets, lengths in trainloader:\n                outputs, _ = model(inputs, lengths=lengths)\n\n                outputs = outputs.view(-1, output_vocab_size)\n                targets = targets.view(-1)\n\n                loss = criterion(outputs, targets)\n                val_loss += loss.item()\n                val_steps += 1\n#                 print(\"loss:\", val_loss / val_steps)\n\n        checkpoint_data = {\n            \"epoch\": epoch,\n            \"net_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n        }\n        with tempfile.TemporaryDirectory() as checkpoint_dir:\n            data_path = Path(checkpoint_dir) / \"data.pkl\"\n            with open(data_path, \"wb\") as fp:\n                pickle.dump(checkpoint_data, fp)\n\n            checkpoint = Checkpoint.from_directory(checkpoint_dir)\n            train.report(\n                {\"loss\": val_loss / val_steps},\n                checkpoint=checkpoint,\n            )\n\n    print(\"Finished Training\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:15:03.468820Z","iopub.execute_input":"2024-06-17T16:15:03.469208Z","iopub.status.idle":"2024-06-17T16:15:03.487215Z","shell.execute_reply.started":"2024-06-17T16:15:03.469179Z","shell.execute_reply":"2024-06-17T16:15:03.486305Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"config = {\n    \"embed_dim\": tune.choice([2 ** i for i in range(6, 10)]),\n    \"hidden_size\": tune.choice([2 ** i for i in range(6, 11)]),\n    \"lr\": tune.loguniform(1e-5, 1e-3),\n    \"batch_size\": tune.choice([32, 64, 128, 256])\n}","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:15:03.647125Z","iopub.execute_input":"2024-06-17T16:15:03.647495Z","iopub.status.idle":"2024-06-17T16:15:03.653288Z","shell.execute_reply.started":"2024-06-17T16:15:03.647465Z","shell.execute_reply":"2024-06-17T16:15:03.652217Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from functools import partial\n\nscheduler = ASHAScheduler(\n    metric=\"loss\",\n    mode=\"min\",\n    max_t=12000, # max time before cutting off training for each instance (in seconds)\n    grace_period=30, # do not cut off training younger than this many seconds\n    reduction_factor=2,\n)\n\nresult = tune.run(\n    partial(tuning),\n    resources_per_trial={\"cpu\": 1, \"gpu\": 0.25},\n    config=config,\n    num_samples=48,\n    scheduler=scheduler,\n    checkpoint_at_end=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:15:04.201593Z","iopub.execute_input":"2024-06-17T16:15:04.201997Z","iopub.status.idle":"2024-06-17T16:53:11.354420Z","shell.execute_reply.started":"2024-06-17T16:15:04.201966Z","shell.execute_reply":"2024-06-17T16:53:11.353624Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"2024-06-17 16:15:04,204\tINFO tune.py:583 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div class=\"tuneStatus\">\n  <div style=\"display: flex;flex-direction: row\">\n    <div style=\"display: flex;flex-direction: column;\">\n      <h3>Tune Status</h3>\n      <table>\n<tbody>\n<tr><td>Current time:</td><td>2024-06-17 16:53:11</td></tr>\n<tr><td>Running for: </td><td>00:38:06.80        </td></tr>\n<tr><td>Memory:      </td><td>3.4/31.4 GiB       </td></tr>\n</tbody>\n</table>\n    </div>\n    <div class=\"vDivider\"></div>\n    <div class=\"systemInfo\">\n      <h3>System Info</h3>\n      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 7680.000: None | Iter 3840.000: None | Iter 1920.000: None | Iter 960.000: None | Iter 480.000: None | Iter 240.000: None | Iter 120.000: None | Iter 60.000: None | Iter 30.000: None<br>Logical resource usage: 1.0/4 CPUs, 0.25/1 GPUs (0.0/1.0 accelerator_type:P100)\n    </div>\n    \n  </div>\n  <div class=\"hDivider\"></div>\n  <div class=\"trialStatus\">\n    <h3>Trial Status</h3>\n    <table>\n<thead>\n<tr><th>Trial name        </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  embed_dim</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      loss</th></tr>\n</thead>\n<tbody>\n<tr><td>tuning_c16fa_00000</td><td>TERMINATED</td><td>172.19.2.2:1099</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">         1024</td><td style=\"text-align: right;\">3.37498e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        176.445 </td><td style=\"text-align: right;\">0.623635  </td></tr>\n<tr><td>tuning_c16fa_00001</td><td>TERMINATED</td><td>172.19.2.2:1101</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">6.57673e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        179.45  </td><td style=\"text-align: right;\">0.351704  </td></tr>\n<tr><td>tuning_c16fa_00002</td><td>TERMINATED</td><td>172.19.2.2:1103</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.000129328</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        139.954 </td><td style=\"text-align: right;\">0.43629   </td></tr>\n<tr><td>tuning_c16fa_00003</td><td>TERMINATED</td><td>172.19.2.2:1107</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">1.71778e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        176.315 </td><td style=\"text-align: right;\">1.05099   </td></tr>\n<tr><td>tuning_c16fa_00004</td><td>TERMINATED</td><td>172.19.2.2:1273</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">1.76021e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        190.106 </td><td style=\"text-align: right;\">0.897592  </td></tr>\n<tr><td>tuning_c16fa_00005</td><td>TERMINATED</td><td>172.19.2.2:1360</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">7.60462e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        145.105 </td><td style=\"text-align: right;\">0.784403  </td></tr>\n<tr><td>tuning_c16fa_00006</td><td>TERMINATED</td><td>172.19.2.2:1363</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">        512</td><td style=\"text-align: right;\">         1024</td><td style=\"text-align: right;\">1.27219e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        343.209 </td><td style=\"text-align: right;\">0.470761  </td></tr>\n<tr><td>tuning_c16fa_00007</td><td>TERMINATED</td><td>172.19.2.2:1364</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">3.68937e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         57.2047</td><td style=\"text-align: right;\">1.54627   </td></tr>\n<tr><td>tuning_c16fa_00008</td><td>TERMINATED</td><td>172.19.2.2:1495</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">         1024</td><td style=\"text-align: right;\">0.000731538</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        201.053 </td><td style=\"text-align: right;\">0.00719291</td></tr>\n<tr><td>tuning_c16fa_00009</td><td>TERMINATED</td><td>172.19.2.2:1557</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">        512</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">3.15856e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        163.568 </td><td style=\"text-align: right;\">0.51786   </td></tr>\n<tr><td>tuning_c16fa_00010</td><td>TERMINATED</td><td>172.19.2.2:1615</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">6.52566e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        155.11  </td><td style=\"text-align: right;\">0.452102  </td></tr>\n<tr><td>tuning_c16fa_00011</td><td>TERMINATED</td><td>172.19.2.2:1682</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">         64</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">1.22845e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         75.3535</td><td style=\"text-align: right;\">2.28662   </td></tr>\n<tr><td>tuning_c16fa_00012</td><td>TERMINATED</td><td>172.19.2.2:1773</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">         64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.00090937 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        211.795 </td><td style=\"text-align: right;\">0.0715278 </td></tr>\n<tr><td>tuning_c16fa_00013</td><td>TERMINATED</td><td>172.19.2.2:1775</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">         1024</td><td style=\"text-align: right;\">0.000196138</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        327.236 </td><td style=\"text-align: right;\">0.0637918 </td></tr>\n<tr><td>tuning_c16fa_00014</td><td>TERMINATED</td><td>172.19.2.2:1867</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">3.00752e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         74.9582</td><td style=\"text-align: right;\">1.28407   </td></tr>\n<tr><td>tuning_c16fa_00015</td><td>TERMINATED</td><td>172.19.2.2:1927</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">        512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">0.000236617</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        148.941 </td><td style=\"text-align: right;\">0.0691943 </td></tr>\n<tr><td>tuning_c16fa_00016</td><td>TERMINATED</td><td>172.19.2.2:1995</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">0.000620723</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        100.086 </td><td style=\"text-align: right;\">0.174216  </td></tr>\n<tr><td>tuning_c16fa_00017</td><td>TERMINATED</td><td>172.19.2.2:2058</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">        512</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">1.21471e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        177.401 </td><td style=\"text-align: right;\">1.26582   </td></tr>\n<tr><td>tuning_c16fa_00018</td><td>TERMINATED</td><td>172.19.2.2:2119</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">        512</td><td style=\"text-align: right;\">         1024</td><td style=\"text-align: right;\">0.000290188</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        206.264 </td><td style=\"text-align: right;\">0.0700382 </td></tr>\n<tr><td>tuning_c16fa_00019</td><td>TERMINATED</td><td>172.19.2.2:2120</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.88241e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        210.073 </td><td style=\"text-align: right;\">0.530541  </td></tr>\n<tr><td>tuning_c16fa_00020</td><td>TERMINATED</td><td>172.19.2.2:2216</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">         1024</td><td style=\"text-align: right;\">7.88735e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        369.48  </td><td style=\"text-align: right;\">0.222554  </td></tr>\n<tr><td>tuning_c16fa_00021</td><td>TERMINATED</td><td>172.19.2.2:2276</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">4.98646e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        190.231 </td><td style=\"text-align: right;\">0.645551  </td></tr>\n<tr><td>tuning_c16fa_00022</td><td>TERMINATED</td><td>172.19.2.2:2341</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">        512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.86218e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        141.267 </td><td style=\"text-align: right;\">0.49536   </td></tr>\n<tr><td>tuning_c16fa_00023</td><td>TERMINATED</td><td>172.19.2.2:2394</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.00017978 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        155.912 </td><td style=\"text-align: right;\">0.335313  </td></tr>\n<tr><td>tuning_c16fa_00024</td><td>TERMINATED</td><td>172.19.2.2:2465</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">         64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.47831e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        274.218 </td><td style=\"text-align: right;\">0.854803  </td></tr>\n<tr><td>tuning_c16fa_00025</td><td>TERMINATED</td><td>172.19.2.2:2519</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">         64</td><td style=\"text-align: right;\">         1024</td><td style=\"text-align: right;\">0.000212668</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        198.378 </td><td style=\"text-align: right;\">0.69046   </td></tr>\n<tr><td>tuning_c16fa_00026</td><td>TERMINATED</td><td>172.19.2.2:2585</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">         1024</td><td style=\"text-align: right;\">2.17126e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        236.986 </td><td style=\"text-align: right;\">0.979152  </td></tr>\n<tr><td>tuning_c16fa_00027</td><td>TERMINATED</td><td>172.19.2.2:2652</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">        512</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">5.6406e-05 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        257.162 </td><td style=\"text-align: right;\">0.202032  </td></tr>\n<tr><td>tuning_c16fa_00028</td><td>TERMINATED</td><td>172.19.2.2:2714</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">3.70616e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        143.176 </td><td style=\"text-align: right;\">0.609283  </td></tr>\n<tr><td>tuning_c16fa_00029</td><td>TERMINATED</td><td>172.19.2.2:2776</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.000229865</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         88.8029</td><td style=\"text-align: right;\">0.534202  </td></tr>\n<tr><td>tuning_c16fa_00030</td><td>TERMINATED</td><td>172.19.2.2:2835</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">         1024</td><td style=\"text-align: right;\">5.23375e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        193.056 </td><td style=\"text-align: right;\">0.720947  </td></tr>\n<tr><td>tuning_c16fa_00031</td><td>TERMINATED</td><td>172.19.2.2:2915</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">        512</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">6.89749e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        113.764 </td><td style=\"text-align: right;\">0.853869  </td></tr>\n<tr><td>tuning_c16fa_00032</td><td>TERMINATED</td><td>172.19.2.2:2916</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">0.000201959</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        127.532 </td><td style=\"text-align: right;\">0.379963  </td></tr>\n<tr><td>tuning_c16fa_00033</td><td>TERMINATED</td><td>172.19.2.2:3011</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">0.000421389</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        100.778 </td><td style=\"text-align: right;\">0.153459  </td></tr>\n<tr><td>tuning_c16fa_00034</td><td>TERMINATED</td><td>172.19.2.2:3074</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.000741687</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         72.0747</td><td style=\"text-align: right;\">0.0787327 </td></tr>\n<tr><td>tuning_c16fa_00035</td><td>TERMINATED</td><td>172.19.2.2:3126</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">          128</td><td style=\"text-align: right;\">3.25267e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        197.188 </td><td style=\"text-align: right;\">0.626831  </td></tr>\n<tr><td>tuning_c16fa_00036</td><td>TERMINATED</td><td>172.19.2.2:3193</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">         64</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.000167157</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         66.6374</td><td style=\"text-align: right;\">0.659934  </td></tr>\n<tr><td>tuning_c16fa_00037</td><td>TERMINATED</td><td>172.19.2.2:3254</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.000430524</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         68.4142</td><td style=\"text-align: right;\">0.319556  </td></tr>\n<tr><td>tuning_c16fa_00038</td><td>TERMINATED</td><td>172.19.2.2:3322</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">         1024</td><td style=\"text-align: right;\">7.17756e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        393.213 </td><td style=\"text-align: right;\">0.414151  </td></tr>\n<tr><td>tuning_c16fa_00039</td><td>TERMINATED</td><td>172.19.2.2:3382</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">0.000267893</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        136.799 </td><td style=\"text-align: right;\">0.296198  </td></tr>\n<tr><td>tuning_c16fa_00040</td><td>TERMINATED</td><td>172.19.2.2:3442</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">         1024</td><td style=\"text-align: right;\">2.87528e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        327.601 </td><td style=\"text-align: right;\">0.562759  </td></tr>\n<tr><td>tuning_c16fa_00041</td><td>TERMINATED</td><td>172.19.2.2:3509</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">         64</td><td style=\"text-align: right;\">         1024</td><td style=\"text-align: right;\">0.000122762</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        388.846 </td><td style=\"text-align: right;\">0.386253  </td></tr>\n<tr><td>tuning_c16fa_00042</td><td>TERMINATED</td><td>172.19.2.2:3569</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">4.62522e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        125.442 </td><td style=\"text-align: right;\">1.01166   </td></tr>\n<tr><td>tuning_c16fa_00043</td><td>TERMINATED</td><td>172.19.2.2:3634</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">2.46138e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        252.935 </td><td style=\"text-align: right;\">0.716243  </td></tr>\n<tr><td>tuning_c16fa_00044</td><td>TERMINATED</td><td>172.19.2.2:3697</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">3.92474e-05</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        144.199 </td><td style=\"text-align: right;\">0.65305   </td></tr>\n<tr><td>tuning_c16fa_00045</td><td>TERMINATED</td><td>172.19.2.2:3757</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">         64</td><td style=\"text-align: right;\">          512</td><td style=\"text-align: right;\">0.000686073</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        158.113 </td><td style=\"text-align: right;\">0.0216966 </td></tr>\n<tr><td>tuning_c16fa_00046</td><td>TERMINATED</td><td>172.19.2.2:3820</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">         64</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">0.000752867</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        102.163 </td><td style=\"text-align: right;\">0.102788  </td></tr>\n<tr><td>tuning_c16fa_00047</td><td>TERMINATED</td><td>172.19.2.2:3880</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">          256</td><td style=\"text-align: right;\">0.000405635</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         93.978 </td><td style=\"text-align: right;\">0.0104768 </td></tr>\n</tbody>\n</table>\n  </div>\n</div>\n<style>\n.tuneStatus {\n  color: var(--jp-ui-font-color1);\n}\n.tuneStatus .systemInfo {\n  display: flex;\n  flex-direction: column;\n}\n.tuneStatus td {\n  white-space: nowrap;\n}\n.tuneStatus .trialStatus {\n  display: flex;\n  flex-direction: column;\n}\n.tuneStatus h3 {\n  font-weight: bold;\n}\n.tuneStatus .hDivider {\n  border-bottom-width: var(--jp-border-width);\n  border-bottom-color: var(--jp-border-color0);\n  border-bottom-style: solid;\n}\n.tuneStatus .vDivider {\n  border-left-width: var(--jp-border-width);\n  border-left-color: var(--jp-border-color0);\n  border-left-style: solid;\n  margin: 0.5em 1em 0.5em 1em;\n}\n</style>\n"},"metadata":{}},{"name":"stderr","text":"\u001b[36m(func pid=1099)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=1099)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1107)\u001b[0m [1,   200] loss: 2.867\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div class=\"trialProgress\">\n  <h3>Trial Progress</h3>\n  <table>\n<thead>\n<tr><th>Trial name        </th><th style=\"text-align: right;\">      loss</th><th>should_checkpoint  </th></tr>\n</thead>\n<tbody>\n<tr><td>tuning_c16fa_00000</td><td style=\"text-align: right;\">0.623635  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00001</td><td style=\"text-align: right;\">0.351704  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00002</td><td style=\"text-align: right;\">0.43629   </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00003</td><td style=\"text-align: right;\">1.05099   </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00004</td><td style=\"text-align: right;\">0.897592  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00005</td><td style=\"text-align: right;\">0.784403  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00006</td><td style=\"text-align: right;\">0.470761  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00007</td><td style=\"text-align: right;\">1.54627   </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00008</td><td style=\"text-align: right;\">0.00719291</td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00009</td><td style=\"text-align: right;\">0.51786   </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00010</td><td style=\"text-align: right;\">0.452102  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00011</td><td style=\"text-align: right;\">2.28662   </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00012</td><td style=\"text-align: right;\">0.0715278 </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00013</td><td style=\"text-align: right;\">0.0637918 </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00014</td><td style=\"text-align: right;\">1.28407   </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00015</td><td style=\"text-align: right;\">0.0691943 </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00016</td><td style=\"text-align: right;\">0.174216  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00017</td><td style=\"text-align: right;\">1.26582   </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00018</td><td style=\"text-align: right;\">0.0700382 </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00019</td><td style=\"text-align: right;\">0.530541  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00020</td><td style=\"text-align: right;\">0.222554  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00021</td><td style=\"text-align: right;\">0.645551  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00022</td><td style=\"text-align: right;\">0.49536   </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00023</td><td style=\"text-align: right;\">0.335313  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00024</td><td style=\"text-align: right;\">0.854803  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00025</td><td style=\"text-align: right;\">0.69046   </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00026</td><td style=\"text-align: right;\">0.979152  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00027</td><td style=\"text-align: right;\">0.202032  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00028</td><td style=\"text-align: right;\">0.609283  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00029</td><td style=\"text-align: right;\">0.534202  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00030</td><td style=\"text-align: right;\">0.720947  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00031</td><td style=\"text-align: right;\">0.853869  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00032</td><td style=\"text-align: right;\">0.379963  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00033</td><td style=\"text-align: right;\">0.153459  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00034</td><td style=\"text-align: right;\">0.0787327 </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00035</td><td style=\"text-align: right;\">0.626831  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00036</td><td style=\"text-align: right;\">0.659934  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00037</td><td style=\"text-align: right;\">0.319556  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00038</td><td style=\"text-align: right;\">0.414151  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00039</td><td style=\"text-align: right;\">0.296198  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00040</td><td style=\"text-align: right;\">0.562759  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00041</td><td style=\"text-align: right;\">0.386253  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00042</td><td style=\"text-align: right;\">1.01166   </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00043</td><td style=\"text-align: right;\">0.716243  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00044</td><td style=\"text-align: right;\">0.65305   </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00045</td><td style=\"text-align: right;\">0.0216966 </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00046</td><td style=\"text-align: right;\">0.102788  </td><td>True               </td></tr>\n<tr><td>tuning_c16fa_00047</td><td style=\"text-align: right;\">0.0104768 </td><td>True               </td></tr>\n</tbody>\n</table>\n</div>\n<style>\n.trialProgress {\n  display: flex;\n  flex-direction: column;\n  color: var(--jp-ui-font-color1);\n}\n.trialProgress h3 {\n  font-weight: bold;\n}\n.trialProgress td {\n  white-space: nowrap;\n}\n</style>\n"},"metadata":{}},{"name":"stderr","text":"\u001b[36m(func pid=1103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00002_2_batch_size=64,embed_dim=256,hidden_size=64,lr=0.0001_2024-06-17_16-15-04/checkpoint_000000)\n\u001b[36m(func pid=1107)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(func pid=1107)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1107)\u001b[0m [2,   200] loss: 2.487\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00002_2_batch_size=64,embed_dim=256,hidden_size=64,lr=0.0001_2024-06-17_16-15-04/checkpoint_000001)\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(func pid=1107)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00003_3_batch_size=32,embed_dim=128,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000001)\n\u001b[36m(func pid=1099)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00000_0_batch_size=128,embed_dim=256,hidden_size=1024,lr=0.0000_2024-06-17_16-15-04/checkpoint_000001)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1107)\u001b[0m [3,   200] loss: 2.076\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1099)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00000_0_batch_size=128,embed_dim=256,hidden_size=1024,lr=0.0000_2024-06-17_16-15-04/checkpoint_000002)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1107)\u001b[0m [4,   200] loss: 1.803\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00002_2_batch_size=64,embed_dim=256,hidden_size=64,lr=0.0001_2024-06-17_16-15-04/checkpoint_000004)\u001b[32m [repeated 4x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1107)\u001b[0m [5,   200] loss: 1.603\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00002_2_batch_size=64,embed_dim=256,hidden_size=64,lr=0.0001_2024-06-17_16-15-04/checkpoint_000005)\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(func pid=1101)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00001_1_batch_size=32,embed_dim=256,hidden_size=256,lr=0.0001_2024-06-17_16-15-04/checkpoint_000004)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1107)\u001b[0m [6,   200] loss: 1.446\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00002_2_batch_size=64,embed_dim=256,hidden_size=64,lr=0.0001_2024-06-17_16-15-04/checkpoint_000006)\n\u001b[36m(func pid=1107)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00003_3_batch_size=32,embed_dim=128,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000005)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1107)\u001b[0m [7,   200] loss: 1.329\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00002_2_batch_size=64,embed_dim=256,hidden_size=64,lr=0.0001_2024-06-17_16-15-04/checkpoint_000008)\u001b[32m [repeated 4x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1107)\u001b[0m [8,   200] loss: 1.226\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1103)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00002_2_batch_size=64,embed_dim=256,hidden_size=64,lr=0.0001_2024-06-17_16-15-04/checkpoint_000009)\u001b[32m [repeated 4x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1103)\u001b[0m Finished Training\n\u001b[36m(func pid=1101)\u001b[0m [8,   200] loss: 0.437\n\u001b[36m(func pid=1107)\u001b[0m [9,   200] loss: 1.154\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1273)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=1273)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=1101)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00001_1_batch_size=32,embed_dim=256,hidden_size=256,lr=0.0001_2024-06-17_16-15-04/checkpoint_000007)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1273)\u001b[0m [1,   200] loss: 2.808\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1099)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00000_0_batch_size=128,embed_dim=256,hidden_size=1024,lr=0.0000_2024-06-17_16-15-04/checkpoint_000008)\n\u001b[36m(func pid=1107)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00003_3_batch_size=32,embed_dim=128,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000008)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1107)\u001b[0m [10,   200] loss: 1.093\n\u001b[36m(func pid=1101)\u001b[0m [10,   200] loss: 0.379\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00004_4_batch_size=32,embed_dim=128,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1099)\u001b[0m Finished Training\n\u001b[36m(func pid=1273)\u001b[0m [2,   200] loss: 2.122\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1101)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00001_1_batch_size=32,embed_dim=256,hidden_size=256,lr=0.0001_2024-06-17_16-15-04/checkpoint_000009)\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(func pid=1360)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=1360)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=1273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00004_4_batch_size=32,embed_dim=128,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000001)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1273)\u001b[0m [3,   200] loss: 1.719\n\u001b[36m(func pid=1101)\u001b[0m Finished Training\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1364)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00007_7_batch_size=256,embed_dim=256,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000000)\n\u001b[36m(func pid=1364)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=1364)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1363)\u001b[0m [1,   200] loss: 2.375\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1364)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00007_7_batch_size=256,embed_dim=256,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000001)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1273)\u001b[0m [4,   200] loss: 1.462\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1364)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00007_7_batch_size=256,embed_dim=256,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000002)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=1364)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00007_7_batch_size=256,embed_dim=256,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000003)\n\u001b[36m(func pid=1360)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00005_5_batch_size=64,embed_dim=128,hidden_size=64,lr=0.0001_2024-06-17_16-15-04/checkpoint_000001)\n\u001b[36m(func pid=1364)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00007_7_batch_size=256,embed_dim=256,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000005)\u001b[32m [repeated 4x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1273)\u001b[0m [5,   200] loss: 1.289\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1364)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00007_7_batch_size=256,embed_dim=256,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000006)\n\u001b[36m(func pid=1360)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00005_5_batch_size=64,embed_dim=128,hidden_size=64,lr=0.0001_2024-06-17_16-15-04/checkpoint_000002)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1363)\u001b[0m [2,   200] loss: 1.228\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1364)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00007_7_batch_size=256,embed_dim=256,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000007)\n\u001b[36m(func pid=1273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00004_4_batch_size=32,embed_dim=128,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000004)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1273)\u001b[0m [6,   200] loss: 1.172\n\u001b[36m(func pid=1364)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1364)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00007_7_batch_size=256,embed_dim=256,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000009)\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(func pid=1363)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00006_6_batch_size=32,embed_dim=512,hidden_size=1024,lr=0.0000_2024-06-17_16-15-04/checkpoint_000001)\n\u001b[36m(func pid=1360)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00005_5_batch_size=64,embed_dim=128,hidden_size=64,lr=0.0001_2024-06-17_16-15-04/checkpoint_000004)\n\u001b[36m(func pid=1495)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=1495)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1273)\u001b[0m [7,   200] loss: 1.086\n\u001b[36m(func pid=1363)\u001b[0m [3,   200] loss: 0.890\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1360)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00005_5_batch_size=64,embed_dim=128,hidden_size=64,lr=0.0001_2024-06-17_16-15-04/checkpoint_000005)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=1273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00004_4_batch_size=32,embed_dim=128,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000006)\n\u001b[36m(func pid=1495)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00008_8_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0007_2024-06-17_16-15-04/checkpoint_000000)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1273)\u001b[0m [8,   200] loss: 1.019\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1363)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00006_6_batch_size=32,embed_dim=512,hidden_size=1024,lr=0.0000_2024-06-17_16-15-04/checkpoint_000002)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=1495)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00008_8_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0007_2024-06-17_16-15-04/checkpoint_000001)\n\u001b[36m(func pid=1273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00004_4_batch_size=32,embed_dim=128,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000007)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1363)\u001b[0m [4,   200] loss: 0.748\n\u001b[36m(func pid=1273)\u001b[0m [9,   200] loss: 0.972\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1360)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00005_5_batch_size=64,embed_dim=128,hidden_size=64,lr=0.0001_2024-06-17_16-15-04/checkpoint_000008)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=1363)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00006_6_batch_size=32,embed_dim=512,hidden_size=1024,lr=0.0000_2024-06-17_16-15-04/checkpoint_000003)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1273)\u001b[0m [10,   200] loss: 0.920\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1360)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00005_5_batch_size=64,embed_dim=128,hidden_size=64,lr=0.0001_2024-06-17_16-15-04/checkpoint_000009)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1360)\u001b[0m Finished Training\n\u001b[36m(func pid=1363)\u001b[0m [5,   200] loss: 0.663\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1495)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00008_8_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0007_2024-06-17_16-15-04/checkpoint_000003)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1273)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1557)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=1557)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=1273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00004_4_batch_size=32,embed_dim=128,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000009)\n\u001b[36m(func pid=1615)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=1615)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=1495)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00008_8_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0007_2024-06-17_16-15-04/checkpoint_000004)\n\u001b[36m(func pid=1557)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00009_9_batch_size=64,embed_dim=512,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1363)\u001b[0m [6,   200] loss: 0.614\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1615)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00010_10_batch_size=64,embed_dim=256,hidden_size=256,lr=0.0001_2024-06-17_16-15-04/checkpoint_000000)\n\u001b[36m(func pid=1495)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00008_8_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0007_2024-06-17_16-15-04/checkpoint_000005)\n\u001b[36m(func pid=1557)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00009_9_batch_size=64,embed_dim=512,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000001)\n\u001b[36m(func pid=1615)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00010_10_batch_size=64,embed_dim=256,hidden_size=256,lr=0.0001_2024-06-17_16-15-04/checkpoint_000001)\n\u001b[36m(func pid=1615)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00010_10_batch_size=64,embed_dim=256,hidden_size=256,lr=0.0001_2024-06-17_16-15-04/checkpoint_000002)\u001b[32m [repeated 4x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1363)\u001b[0m [7,   200] loss: 0.569\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1557)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00009_9_batch_size=64,embed_dim=512,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000003)\n\u001b[36m(func pid=1495)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00008_8_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0007_2024-06-17_16-15-04/checkpoint_000007)\n\u001b[36m(func pid=1615)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00010_10_batch_size=64,embed_dim=256,hidden_size=256,lr=0.0001_2024-06-17_16-15-04/checkpoint_000003)\n\u001b[36m(func pid=1557)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00009_9_batch_size=64,embed_dim=512,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000004)\n\u001b[36m(func pid=1557)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00009_9_batch_size=64,embed_dim=512,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000005)\u001b[32m [repeated 4x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1363)\u001b[0m [8,   200] loss: 0.537\n\u001b[36m(func pid=1495)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1495)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00008_8_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0007_2024-06-17_16-15-04/checkpoint_000009)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=1557)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00009_9_batch_size=64,embed_dim=512,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000006)\n\u001b[36m(func pid=1682)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=1682)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=1615)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00010_10_batch_size=64,embed_dim=256,hidden_size=256,lr=0.0001_2024-06-17_16-15-04/checkpoint_000006)\n\u001b[36m(func pid=1682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00011_11_batch_size=128,embed_dim=64,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1363)\u001b[0m [9,   200] loss: 0.512\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00011_11_batch_size=128,embed_dim=64,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(func pid=1682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00011_11_batch_size=128,embed_dim=64,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000002)\n\u001b[36m(func pid=1557)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00009_9_batch_size=64,embed_dim=512,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000008)\n\u001b[36m(func pid=1682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00011_11_batch_size=128,embed_dim=64,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000003)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=1557)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00009_9_batch_size=64,embed_dim=512,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000009)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1557)\u001b[0m Finished Training\n\u001b[36m(func pid=1363)\u001b[0m [10,   200] loss: 0.493\n\u001b[36m(func pid=1615)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00011_11_batch_size=128,embed_dim=64,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000005)\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(func pid=1775)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=1775)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=1682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00011_11_batch_size=128,embed_dim=64,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000006)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1773)\u001b[0m [1,   200] loss: 1.625\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00011_11_batch_size=128,embed_dim=64,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000007)\n\u001b[36m(func pid=1773)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=1773)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1775)\u001b[0m [1,   200] loss: 1.388\n\u001b[36m(func pid=1363)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00011_11_batch_size=128,embed_dim=64,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000008)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1682)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1682)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00011_11_batch_size=128,embed_dim=64,hidden_size=128,lr=0.0000_2024-06-17_16-15-04/checkpoint_000009)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1773)\u001b[0m [2,   200] loss: 0.663\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1867)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=1867)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=1775)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00013_13_batch_size=32,embed_dim=128,hidden_size=1024,lr=0.0002_2024-06-17_16-15-04/checkpoint_000000)\n\u001b[36m(func pid=1927)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=1927)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1775)\u001b[0m [2,   200] loss: 0.707\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1867)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00014_14_batch_size=256,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000000)\n\u001b[36m(func pid=1867)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00014_14_batch_size=256,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000001)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1773)\u001b[0m [3,   200] loss: 0.466\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1867)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00014_14_batch_size=256,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000002)\n\u001b[36m(func pid=1927)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00015_15_batch_size=128,embed_dim=512,hidden_size=512,lr=0.0002_2024-06-17_16-15-04/checkpoint_000000)\n\u001b[36m(func pid=1775)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00013_13_batch_size=32,embed_dim=128,hidden_size=1024,lr=0.0002_2024-06-17_16-15-04/checkpoint_000001)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=1867)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00014_14_batch_size=256,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000004)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1773)\u001b[0m [4,   200] loss: 0.356\n\u001b[36m(func pid=1775)\u001b[0m [3,   200] loss: 0.567\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1867)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00014_14_batch_size=256,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000005)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=1867)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00014_14_batch_size=256,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000006)\n\u001b[36m(func pid=1927)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00015_15_batch_size=128,embed_dim=512,hidden_size=512,lr=0.0002_2024-06-17_16-15-04/checkpoint_000002)\n\u001b[36m(func pid=1867)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00014_14_batch_size=256,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000007)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1773)\u001b[0m [5,   200] loss: 0.276\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1867)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00014_14_batch_size=256,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000008)\n\u001b[36m(func pid=1775)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00013_13_batch_size=32,embed_dim=128,hidden_size=1024,lr=0.0002_2024-06-17_16-15-04/checkpoint_000002)\n\u001b[36m(func pid=1867)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00014_14_batch_size=256,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-15-04/checkpoint_000009)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1867)\u001b[0m Finished Training\n\u001b[36m(func pid=1775)\u001b[0m [4,   200] loss: 0.467\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1927)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00015_15_batch_size=128,embed_dim=512,hidden_size=512,lr=0.0002_2024-06-17_16-15-04/checkpoint_000004)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=1995)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=1995)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=1927)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00015_15_batch_size=128,embed_dim=512,hidden_size=512,lr=0.0002_2024-06-17_16-15-04/checkpoint_000005)\n\u001b[36m(func pid=1995)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00016_16_batch_size=256,embed_dim=128,hidden_size=512,lr=0.0006_2024-06-17_16-15-12/checkpoint_000000)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1773)\u001b[0m [7,   200] loss: 0.176\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1995)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00016_16_batch_size=256,embed_dim=128,hidden_size=512,lr=0.0006_2024-06-17_16-15-12/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1775)\u001b[0m [5,   200] loss: 0.383\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1995)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00016_16_batch_size=256,embed_dim=128,hidden_size=512,lr=0.0006_2024-06-17_16-15-12/checkpoint_000002)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=1927)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00015_15_batch_size=128,embed_dim=512,hidden_size=512,lr=0.0002_2024-06-17_16-15-04/checkpoint_000007)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1773)\u001b[0m [8,   200] loss: 0.143\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1775)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00013_13_batch_size=32,embed_dim=128,hidden_size=1024,lr=0.0002_2024-06-17_16-15-04/checkpoint_000004)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=1773)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00012_12_batch_size=32,embed_dim=64,hidden_size=64,lr=0.0009_2024-06-17_16-15-04/checkpoint_000007)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1775)\u001b[0m [6,   200] loss: 0.320\n\u001b[36m(func pid=1773)\u001b[0m [9,   200] loss: 0.114\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1927)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00015_15_batch_size=128,embed_dim=512,hidden_size=512,lr=0.0002_2024-06-17_16-15-04/checkpoint_000009)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1927)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1773)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00012_12_batch_size=32,embed_dim=64,hidden_size=64,lr=0.0009_2024-06-17_16-15-04/checkpoint_000008)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2058)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2058)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1773)\u001b[0m [10,   200] loss: 0.091\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1995)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00016_16_batch_size=256,embed_dim=128,hidden_size=512,lr=0.0006_2024-06-17_16-15-12/checkpoint_000008)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1775)\u001b[0m [7,   200] loss: 0.252\n\u001b[36m(func pid=1995)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=1995)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00016_16_batch_size=256,embed_dim=128,hidden_size=512,lr=0.0006_2024-06-17_16-15-12/checkpoint_000009)\n\u001b[36m(func pid=1773)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00012_12_batch_size=32,embed_dim=64,hidden_size=64,lr=0.0009_2024-06-17_16-15-04/checkpoint_000009)\n\u001b[36m(func pid=1775)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00013_13_batch_size=32,embed_dim=128,hidden_size=1024,lr=0.0002_2024-06-17_16-15-04/checkpoint_000006)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2119)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2119)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1775)\u001b[0m [8,   200] loss: 0.195\n\u001b[36m(func pid=1773)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00017_17_batch_size=64,embed_dim=512,hidden_size=128,lr=0.0000_2024-06-17_16-15-13/checkpoint_000002)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2120)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2120)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=2119)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00018_18_batch_size=256,embed_dim=512,hidden_size=1024,lr=0.0003_2024-06-17_16-15-13/checkpoint_000000)\n\u001b[36m(func pid=2120)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00019_19_batch_size=64,embed_dim=256,hidden_size=512,lr=0.0000_2024-06-17_16-15-13/checkpoint_000000)\n\u001b[36m(func pid=2058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00017_17_batch_size=64,embed_dim=512,hidden_size=128,lr=0.0000_2024-06-17_16-15-13/checkpoint_000003)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1775)\u001b[0m [9,   200] loss: 0.141\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2119)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00018_18_batch_size=256,embed_dim=512,hidden_size=1024,lr=0.0003_2024-06-17_16-15-13/checkpoint_000001)\n\u001b[36m(func pid=2120)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00019_19_batch_size=64,embed_dim=256,hidden_size=512,lr=0.0000_2024-06-17_16-15-13/checkpoint_000001)\n\u001b[36m(func pid=2058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00017_17_batch_size=64,embed_dim=512,hidden_size=128,lr=0.0000_2024-06-17_16-15-13/checkpoint_000004)\n\u001b[36m(func pid=2119)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00018_18_batch_size=256,embed_dim=512,hidden_size=1024,lr=0.0003_2024-06-17_16-15-13/checkpoint_000002)\n\u001b[36m(func pid=2058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00017_17_batch_size=64,embed_dim=512,hidden_size=128,lr=0.0000_2024-06-17_16-15-13/checkpoint_000005)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1775)\u001b[0m [10,   200] loss: 0.096\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2119)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00018_18_batch_size=256,embed_dim=512,hidden_size=1024,lr=0.0003_2024-06-17_16-15-13/checkpoint_000003)\n\u001b[36m(func pid=2120)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00019_19_batch_size=64,embed_dim=256,hidden_size=512,lr=0.0000_2024-06-17_16-15-13/checkpoint_000003)\n\u001b[36m(func pid=2119)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00018_18_batch_size=256,embed_dim=512,hidden_size=1024,lr=0.0003_2024-06-17_16-15-13/checkpoint_000004)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=1775)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2216)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2216)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=2058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00017_17_batch_size=64,embed_dim=512,hidden_size=128,lr=0.0000_2024-06-17_16-15-13/checkpoint_000007)\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(func pid=2119)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00018_18_batch_size=256,embed_dim=512,hidden_size=1024,lr=0.0003_2024-06-17_16-15-13/checkpoint_000005)\n\u001b[36m(func pid=2120)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00019_19_batch_size=64,embed_dim=256,hidden_size=512,lr=0.0000_2024-06-17_16-15-13/checkpoint_000005)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2216)\u001b[0m [1,   200] loss: 1.575\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2119)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00018_18_batch_size=256,embed_dim=512,hidden_size=1024,lr=0.0003_2024-06-17_16-15-13/checkpoint_000006)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2058)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00017_17_batch_size=64,embed_dim=512,hidden_size=128,lr=0.0000_2024-06-17_16-15-13/checkpoint_000009)\n\u001b[36m(func pid=2120)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00019_19_batch_size=64,embed_dim=256,hidden_size=512,lr=0.0000_2024-06-17_16-15-13/checkpoint_000006)\n\u001b[36m(func pid=2216)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00020_20_batch_size=32,embed_dim=256,hidden_size=1024,lr=0.0001_2024-06-17_16-17-41/checkpoint_000000)\n\u001b[36m(func pid=2276)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2276)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=2119)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00018_18_batch_size=256,embed_dim=512,hidden_size=1024,lr=0.0003_2024-06-17_16-15-13/checkpoint_000007)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2216)\u001b[0m [2,   200] loss: 0.702\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2119)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00018_18_batch_size=256,embed_dim=512,hidden_size=1024,lr=0.0003_2024-06-17_16-15-13/checkpoint_000008)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2216)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00020_20_batch_size=32,embed_dim=256,hidden_size=1024,lr=0.0001_2024-06-17_16-17-41/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(func pid=2119)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00018_18_batch_size=256,embed_dim=512,hidden_size=1024,lr=0.0003_2024-06-17_16-15-13/checkpoint_000009)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2119)\u001b[0m Finished Training\n\u001b[36m(func pid=2216)\u001b[0m [3,   200] loss: 0.578\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2276)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00021_21_batch_size=64,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-18-18/checkpoint_000001)\n\u001b[36m(func pid=2341)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2341)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=2120)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00019_19_batch_size=64,embed_dim=256,hidden_size=512,lr=0.0000_2024-06-17_16-15-13/checkpoint_000009)\n\u001b[36m(func pid=2276)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00021_21_batch_size=64,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-18-18/checkpoint_000002)\n\u001b[36m(func pid=2394)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=2216)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00020_20_batch_size=32,embed_dim=256,hidden_size=1024,lr=0.0001_2024-06-17_16-17-41/checkpoint_000002)\n\u001b[36m(func pid=2341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00022_22_batch_size=128,embed_dim=512,hidden_size=512,lr=0.0000_2024-06-17_16-18-18/checkpoint_000000)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2216)\u001b[0m [4,   200] loss: 0.500\n\u001b[36m(func pid=2120)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2276)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00021_21_batch_size=64,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-18-18/checkpoint_000003)\n\u001b[36m(func pid=2394)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00023_23_batch_size=64,embed_dim=256,hidden_size=64,lr=0.0002_2024-06-17_16-18-19/checkpoint_000000)\n\u001b[36m(func pid=2341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00022_22_batch_size=128,embed_dim=512,hidden_size=512,lr=0.0000_2024-06-17_16-18-18/checkpoint_000001)\n\u001b[36m(func pid=2394)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00023_23_batch_size=64,embed_dim=256,hidden_size=64,lr=0.0002_2024-06-17_16-18-19/checkpoint_000001)\n\u001b[36m(func pid=2394)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00023_23_batch_size=64,embed_dim=256,hidden_size=64,lr=0.0002_2024-06-17_16-18-19/checkpoint_000002)\u001b[32m [repeated 4x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2216)\u001b[0m [5,   200] loss: 0.446\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00022_22_batch_size=128,embed_dim=512,hidden_size=512,lr=0.0000_2024-06-17_16-18-18/checkpoint_000004)\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(func pid=2276)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00021_21_batch_size=64,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-18-18/checkpoint_000006)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00022_22_batch_size=128,embed_dim=512,hidden_size=512,lr=0.0000_2024-06-17_16-18-18/checkpoint_000005)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2216)\u001b[0m [6,   200] loss: 0.404\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2276)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00021_21_batch_size=64,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-18-18/checkpoint_000007)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2394)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00023_23_batch_size=64,embed_dim=256,hidden_size=64,lr=0.0002_2024-06-17_16-18-19/checkpoint_000005)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2276)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00021_21_batch_size=64,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-18-18/checkpoint_000008)\n\u001b[36m(func pid=2341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00022_22_batch_size=128,embed_dim=512,hidden_size=512,lr=0.0000_2024-06-17_16-18-18/checkpoint_000007)\n\u001b[36m(func pid=2394)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00023_23_batch_size=64,embed_dim=256,hidden_size=64,lr=0.0002_2024-06-17_16-18-19/checkpoint_000006)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2216)\u001b[0m [7,   200] loss: 0.362\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00022_22_batch_size=128,embed_dim=512,hidden_size=512,lr=0.0000_2024-06-17_16-18-18/checkpoint_000008)\n\u001b[36m(func pid=2276)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00021_21_batch_size=64,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-18-18/checkpoint_000009)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2276)\u001b[0m Finished Training\n\u001b[36m(func pid=2341)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2341)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00022_22_batch_size=128,embed_dim=512,hidden_size=512,lr=0.0000_2024-06-17_16-18-18/checkpoint_000009)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2465)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2465)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=2394)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00023_23_batch_size=64,embed_dim=256,hidden_size=64,lr=0.0002_2024-06-17_16-18-19/checkpoint_000008)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2519)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2519)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2216)\u001b[0m [8,   200] loss: 0.326\n\u001b[36m(func pid=2394)\u001b[0m Finished Training\n\u001b[36m(func pid=2465)\u001b[0m [1,   200] loss: 2.576\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2394)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00023_23_batch_size=64,embed_dim=256,hidden_size=64,lr=0.0002_2024-06-17_16-18-19/checkpoint_000009)\n\u001b[36m(func pid=2585)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2585)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=2519)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00025_25_batch_size=256,embed_dim=64,hidden_size=1024,lr=0.0002_2024-06-17_16-20-52/checkpoint_000000)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2465)\u001b[0m [2,   200] loss: 1.616\n\u001b[36m(func pid=2216)\u001b[0m [9,   200] loss: 0.287\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2519)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00025_25_batch_size=256,embed_dim=64,hidden_size=1024,lr=0.0002_2024-06-17_16-20-52/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(func pid=2585)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00026_26_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0000_2024-06-17_16-21-00/checkpoint_000000)\n\u001b[36m(func pid=2465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00024_24_batch_size=32,embed_dim=64,hidden_size=512,lr=0.0000_2024-06-17_16-19-24/checkpoint_000001)\n\u001b[36m(func pid=2519)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00025_25_batch_size=256,embed_dim=64,hidden_size=1024,lr=0.0002_2024-06-17_16-20-52/checkpoint_000002)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2465)\u001b[0m [3,   200] loss: 1.350\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2216)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00020_20_batch_size=32,embed_dim=256,hidden_size=1024,lr=0.0001_2024-06-17_16-17-41/checkpoint_000008)\n\u001b[36m(func pid=2519)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00025_25_batch_size=256,embed_dim=64,hidden_size=1024,lr=0.0002_2024-06-17_16-20-52/checkpoint_000003)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2216)\u001b[0m [10,   200] loss: 0.255\n\u001b[36m(func pid=2465)\u001b[0m [4,   200] loss: 1.212\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2585)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00026_26_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0000_2024-06-17_16-21-00/checkpoint_000002)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00024_24_batch_size=32,embed_dim=64,hidden_size=512,lr=0.0000_2024-06-17_16-19-24/checkpoint_000003)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2216)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2585)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00026_26_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0000_2024-06-17_16-21-00/checkpoint_000003)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2465)\u001b[0m [5,   200] loss: 1.120\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2652)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2652)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=2519)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00025_25_batch_size=256,embed_dim=64,hidden_size=1024,lr=0.0002_2024-06-17_16-20-52/checkpoint_000006)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2652)\u001b[0m [1,   200] loss: 1.841\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00024_24_batch_size=32,embed_dim=64,hidden_size=512,lr=0.0000_2024-06-17_16-19-24/checkpoint_000004)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2465)\u001b[0m [6,   200] loss: 1.051\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2519)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00025_25_batch_size=256,embed_dim=64,hidden_size=1024,lr=0.0002_2024-06-17_16-20-52/checkpoint_000007)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00027_27_batch_size=32,embed_dim=512,hidden_size=512,lr=0.0001_2024-06-17_16-22-54/checkpoint_000000)\n\u001b[36m(func pid=2585)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00026_26_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0000_2024-06-17_16-21-00/checkpoint_000005)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2652)\u001b[0m [2,   200] loss: 0.735\n\u001b[36m(func pid=2465)\u001b[0m [7,   200] loss: 1.008\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00027_27_batch_size=32,embed_dim=512,hidden_size=512,lr=0.0001_2024-06-17_16-22-54/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2519)\u001b[0m Finished Training\n\u001b[36m(func pid=2652)\u001b[0m [3,   200] loss: 0.558\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2714)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2714)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=2465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00024_24_batch_size=32,embed_dim=64,hidden_size=512,lr=0.0000_2024-06-17_16-19-24/checkpoint_000006)\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(func pid=2585)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00026_26_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0000_2024-06-17_16-21-00/checkpoint_000007)\n\u001b[36m(func pid=2652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00027_27_batch_size=32,embed_dim=512,hidden_size=512,lr=0.0001_2024-06-17_16-22-54/checkpoint_000002)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2652)\u001b[0m [4,   200] loss: 0.468\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2585)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00026_26_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0000_2024-06-17_16-21-00/checkpoint_000008)\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(func pid=2714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00028_28_batch_size=64,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-23-44/checkpoint_000001)\n\u001b[36m(func pid=2652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00027_27_batch_size=32,embed_dim=512,hidden_size=512,lr=0.0001_2024-06-17_16-22-54/checkpoint_000003)\n\u001b[36m(func pid=2585)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00026_26_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0000_2024-06-17_16-21-00/checkpoint_000009)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2585)\u001b[0m Finished Training\n\u001b[36m(func pid=2465)\u001b[0m [9,   200] loss: 0.917\n\u001b[36m(func pid=2465)\u001b[0m [10,   200] loss: 0.882\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2776)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2776)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=2714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00028_28_batch_size=64,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-23-44/checkpoint_000002)\n\u001b[36m(func pid=2714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00028_28_batch_size=64,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-23-44/checkpoint_000003)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2465)\u001b[0m Finished Training\n\u001b[36m(func pid=2652)\u001b[0m [5,   200] loss: 0.405\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2776)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00029_29_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0002_2024-06-17_16-23-44/checkpoint_000000)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2652)\u001b[0m [6,   200] loss: 0.355\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2835)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2835)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=2776)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00029_29_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0002_2024-06-17_16-23-44/checkpoint_000001)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00028_28_batch_size=64,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-23-44/checkpoint_000005)\n\u001b[36m(func pid=2776)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00029_29_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0002_2024-06-17_16-23-44/checkpoint_000002)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2652)\u001b[0m [7,   200] loss: 0.317\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2835)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00030_30_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0001_2024-06-17_16-24-10/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00028_28_batch_size=64,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-23-44/checkpoint_000006)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00027_27_batch_size=32,embed_dim=512,hidden_size=512,lr=0.0001_2024-06-17_16-22-54/checkpoint_000006)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2652)\u001b[0m [8,   200] loss: 0.284\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2776)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00029_29_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0002_2024-06-17_16-23-44/checkpoint_000006)\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(func pid=2714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00028_28_batch_size=64,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-23-44/checkpoint_000008)\n\u001b[36m(func pid=2776)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00029_29_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0002_2024-06-17_16-23-44/checkpoint_000007)\n\u001b[36m(func pid=2776)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00029_29_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0002_2024-06-17_16-23-44/checkpoint_000008)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2652)\u001b[0m [9,   200] loss: 0.250\n\u001b[36m(func pid=2714)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2714)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00028_28_batch_size=64,embed_dim=256,hidden_size=256,lr=0.0000_2024-06-17_16-23-44/checkpoint_000009)\n\u001b[36m(func pid=2776)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00029_29_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0002_2024-06-17_16-23-44/checkpoint_000009)\n\u001b[36m(func pid=2652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00027_27_batch_size=32,embed_dim=512,hidden_size=512,lr=0.0001_2024-06-17_16-22-54/checkpoint_000008)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2915)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2915)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2652)\u001b[0m [10,   200] loss: 0.223\n\u001b[36m(func pid=2776)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2835)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00030_30_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0001_2024-06-17_16-24-10/checkpoint_000004)\n\u001b[36m(func pid=2916)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=2916)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=2915)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00031_31_batch_size=128,embed_dim=512,hidden_size=64,lr=0.0001_2024-06-17_16-24-18/checkpoint_000000)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2652)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2652)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00027_27_batch_size=32,embed_dim=512,hidden_size=512,lr=0.0001_2024-06-17_16-22-54/checkpoint_000009)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2915)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00031_31_batch_size=128,embed_dim=512,hidden_size=64,lr=0.0001_2024-06-17_16-24-18/checkpoint_000002)\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(func pid=3011)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=3011)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=2835)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00030_30_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0001_2024-06-17_16-24-10/checkpoint_000006)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=3011)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00033_33_batch_size=256,embed_dim=256,hidden_size=512,lr=0.0004_2024-06-17_16-26-57/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2915)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00031_31_batch_size=128,embed_dim=512,hidden_size=64,lr=0.0001_2024-06-17_16-24-18/checkpoint_000004)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2835)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00030_30_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0001_2024-06-17_16-24-10/checkpoint_000007)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2915)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00031_31_batch_size=128,embed_dim=512,hidden_size=64,lr=0.0001_2024-06-17_16-24-18/checkpoint_000006)\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(func pid=2835)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00030_30_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0001_2024-06-17_16-24-10/checkpoint_000008)\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(func pid=3011)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00033_33_batch_size=256,embed_dim=256,hidden_size=512,lr=0.0004_2024-06-17_16-26-57/checkpoint_000004)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2915)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00031_31_batch_size=128,embed_dim=512,hidden_size=64,lr=0.0001_2024-06-17_16-24-18/checkpoint_000008)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=2835)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00030_30_batch_size=128,embed_dim=128,hidden_size=1024,lr=0.0001_2024-06-17_16-24-10/checkpoint_000009)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2835)\u001b[0m Finished Training\n\u001b[36m(func pid=2915)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=2916)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00032_32_batch_size=128,embed_dim=128,hidden_size=512,lr=0.0002_2024-06-17_16-25-34/checkpoint_000008)\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(func pid=3074)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=3074)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=3011)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00033_33_batch_size=256,embed_dim=256,hidden_size=512,lr=0.0004_2024-06-17_16-26-57/checkpoint_000007)\n\u001b[36m(func pid=2916)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00032_32_batch_size=128,embed_dim=128,hidden_size=512,lr=0.0002_2024-06-17_16-25-34/checkpoint_000009)\n\u001b[36m(func pid=3126)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=3126)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=2916)\u001b[0m Finished Training\n\u001b[36m(func pid=3126)\u001b[0m [1,   200] loss: 2.705\n\u001b[36m(func pid=3011)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3011)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00033_33_batch_size=256,embed_dim=256,hidden_size=512,lr=0.0004_2024-06-17_16-26-57/checkpoint_000009)\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(func pid=3193)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=3193)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=3074)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00034_34_batch_size=128,embed_dim=128,hidden_size=256,lr=0.0007_2024-06-17_16-27-24/checkpoint_000002)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3126)\u001b[0m [2,   200] loss: 1.893\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3254)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=3254)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=3074)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00034_34_batch_size=128,embed_dim=128,hidden_size=256,lr=0.0007_2024-06-17_16-27-24/checkpoint_000003)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=3254)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00037_37_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0004_2024-06-17_16-30-04/checkpoint_000000)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3126)\u001b[0m [3,   200] loss: 1.419\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3254)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00037_37_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0004_2024-06-17_16-30-04/checkpoint_000001)\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(func pid=3254)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00037_37_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0004_2024-06-17_16-30-04/checkpoint_000002)\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(func pid=3254)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00037_37_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0004_2024-06-17_16-30-04/checkpoint_000003)\u001b[32m [repeated 4x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3126)\u001b[0m [4,   200] loss: 1.157\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3254)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00037_37_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0004_2024-06-17_16-30-04/checkpoint_000004)\u001b[32m [repeated 3x across cluster]\u001b[0m\n\u001b[36m(func pid=3254)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00037_37_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0004_2024-06-17_16-30-04/checkpoint_000005)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3074)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3254)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00037_37_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0004_2024-06-17_16-30-04/checkpoint_000006)\u001b[32m [repeated 4x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3126)\u001b[0m [5,   200] loss: 0.993\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3254)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00037_37_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0004_2024-06-17_16-30-04/checkpoint_000007)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=3322)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=3322)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3193)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3254)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00037_37_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0004_2024-06-17_16-30-04/checkpoint_000008)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3126)\u001b[0m [6,   200] loss: 0.888\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3382)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=3382)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=3254)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00037_37_batch_size=128,embed_dim=128,hidden_size=64,lr=0.0004_2024-06-17_16-30-04/checkpoint_000009)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3254)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3126)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00035_35_batch_size=32,embed_dim=256,hidden_size=128,lr=0.0000_2024-06-17_16-27-25/checkpoint_000005)\n\u001b[36m(func pid=3442)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=3442)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=3382)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00039_39_batch_size=128,embed_dim=128,hidden_size=512,lr=0.0003_2024-06-17_16-31-04/checkpoint_000000)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3126)\u001b[0m [7,   200] loss: 0.807\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3322)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00038_38_batch_size=32,embed_dim=128,hidden_size=1024,lr=0.0001_2024-06-17_16-31-00/checkpoint_000000)\n\u001b[36m(func pid=3382)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00039_39_batch_size=128,embed_dim=128,hidden_size=512,lr=0.0003_2024-06-17_16-31-04/checkpoint_000001)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3322)\u001b[0m [2,   200] loss: 0.942\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3126)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00035_35_batch_size=32,embed_dim=256,hidden_size=128,lr=0.0000_2024-06-17_16-27-25/checkpoint_000006)\n\u001b[36m(func pid=3382)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00039_39_batch_size=128,embed_dim=128,hidden_size=512,lr=0.0003_2024-06-17_16-31-04/checkpoint_000002)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3126)\u001b[0m [8,   200] loss: 0.744\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3382)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00039_39_batch_size=128,embed_dim=128,hidden_size=512,lr=0.0003_2024-06-17_16-31-04/checkpoint_000003)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=3126)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00035_35_batch_size=32,embed_dim=256,hidden_size=128,lr=0.0000_2024-06-17_16-27-25/checkpoint_000007)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3126)\u001b[0m [9,   200] loss: 0.693\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3382)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00039_39_batch_size=128,embed_dim=128,hidden_size=512,lr=0.0003_2024-06-17_16-31-04/checkpoint_000004)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3322)\u001b[0m [3,   200] loss: 0.799\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3442)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00040_40_batch_size=64,embed_dim=256,hidden_size=1024,lr=0.0000_2024-06-17_16-33-24/checkpoint_000001)\n\u001b[36m(func pid=3382)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00039_39_batch_size=128,embed_dim=128,hidden_size=512,lr=0.0003_2024-06-17_16-31-04/checkpoint_000005)\n\u001b[36m(func pid=3126)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00035_35_batch_size=32,embed_dim=256,hidden_size=128,lr=0.0000_2024-06-17_16-27-25/checkpoint_000008)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3126)\u001b[0m [10,   200] loss: 0.656\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3322)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00038_38_batch_size=32,embed_dim=128,hidden_size=1024,lr=0.0001_2024-06-17_16-31-00/checkpoint_000002)\n\u001b[36m(func pid=3382)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00039_39_batch_size=128,embed_dim=128,hidden_size=512,lr=0.0003_2024-06-17_16-31-04/checkpoint_000006)\n\u001b[36m(func pid=3442)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00040_40_batch_size=64,embed_dim=256,hidden_size=1024,lr=0.0000_2024-06-17_16-33-24/checkpoint_000002)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3322)\u001b[0m [4,   200] loss: 0.709\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3382)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00039_39_batch_size=128,embed_dim=128,hidden_size=512,lr=0.0003_2024-06-17_16-31-04/checkpoint_000007)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3126)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3509)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=3509)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=3126)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00035_35_batch_size=32,embed_dim=256,hidden_size=128,lr=0.0000_2024-06-17_16-27-25/checkpoint_000009)\n\u001b[36m(func pid=3382)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00039_39_batch_size=128,embed_dim=128,hidden_size=512,lr=0.0003_2024-06-17_16-31-04/checkpoint_000008)\n\u001b[36m(func pid=3322)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00038_38_batch_size=32,embed_dim=128,hidden_size=1024,lr=0.0001_2024-06-17_16-31-00/checkpoint_000003)\n\u001b[36m(func pid=3442)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00040_40_batch_size=64,embed_dim=256,hidden_size=1024,lr=0.0000_2024-06-17_16-33-24/checkpoint_000003)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3509)\u001b[0m [1,   200] loss: 1.808\n\u001b[36m(func pid=3382)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3382)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00039_39_batch_size=128,embed_dim=128,hidden_size=512,lr=0.0003_2024-06-17_16-31-04/checkpoint_000009)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3322)\u001b[0m [5,   200] loss: 0.646\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3569)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=3569)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=3509)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00041_41_batch_size=32,embed_dim=64,hidden_size=1024,lr=0.0001_2024-06-17_16-33-30/checkpoint_000000)\n\u001b[36m(func pid=3569)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00042_42_batch_size=256,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-33-48/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3509)\u001b[0m [2,   200] loss: 1.041\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3569)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00042_42_batch_size=256,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-33-48/checkpoint_000002)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3322)\u001b[0m [6,   200] loss: 0.595\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3569)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00042_42_batch_size=256,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-33-48/checkpoint_000003)\n\u001b[36m(func pid=3442)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00040_40_batch_size=64,embed_dim=256,hidden_size=1024,lr=0.0000_2024-06-17_16-33-24/checkpoint_000005)\n\u001b[36m(func pid=3569)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00042_42_batch_size=256,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-33-48/checkpoint_000004)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=3322)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00038_38_batch_size=32,embed_dim=128,hidden_size=1024,lr=0.0001_2024-06-17_16-31-00/checkpoint_000005)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3509)\u001b[0m [3,   200] loss: 0.892\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3569)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00042_42_batch_size=256,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-33-48/checkpoint_000005)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3322)\u001b[0m [7,   200] loss: 0.552\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3569)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00042_42_batch_size=256,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-33-48/checkpoint_000006)\n\u001b[36m(func pid=3442)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00040_40_batch_size=64,embed_dim=256,hidden_size=1024,lr=0.0000_2024-06-17_16-33-24/checkpoint_000006)\n\u001b[36m(func pid=3569)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00042_42_batch_size=256,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-33-48/checkpoint_000007)\n\u001b[36m(func pid=3509)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00041_41_batch_size=32,embed_dim=64,hidden_size=1024,lr=0.0001_2024-06-17_16-33-30/checkpoint_000002)\n\u001b[36m(func pid=3569)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00042_42_batch_size=256,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-33-48/checkpoint_000008)\n\u001b[36m(func pid=3322)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00038_38_batch_size=32,embed_dim=128,hidden_size=1024,lr=0.0001_2024-06-17_16-31-00/checkpoint_000006)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3509)\u001b[0m [4,   200] loss: 0.786\n\u001b[36m(func pid=3569)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3569)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00042_42_batch_size=256,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-33-48/checkpoint_000009)\n\u001b[36m(func pid=3442)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00040_40_batch_size=64,embed_dim=256,hidden_size=1024,lr=0.0000_2024-06-17_16-33-24/checkpoint_000007)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3322)\u001b[0m [8,   200] loss: 0.512\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3634)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=3634)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=3509)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00041_41_batch_size=32,embed_dim=64,hidden_size=1024,lr=0.0001_2024-06-17_16-33-30/checkpoint_000003)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3634)\u001b[0m [1,   200] loss: 2.583\n\u001b[36m(func pid=3509)\u001b[0m [5,   200] loss: 0.702\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3322)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00038_38_batch_size=32,embed_dim=128,hidden_size=1024,lr=0.0001_2024-06-17_16-31-00/checkpoint_000007)\n\u001b[36m(func pid=3442)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00040_40_batch_size=64,embed_dim=256,hidden_size=1024,lr=0.0000_2024-06-17_16-33-24/checkpoint_000008)\n\u001b[36m(func pid=3634)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00043_43_batch_size=32,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-35-39/checkpoint_000000)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3322)\u001b[0m [9,   200] loss: 0.478\n\u001b[36m(func pid=3634)\u001b[0m [2,   200] loss: 1.542\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3509)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00041_41_batch_size=32,embed_dim=64,hidden_size=1024,lr=0.0001_2024-06-17_16-33-30/checkpoint_000004)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3442)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3442)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00040_40_batch_size=64,embed_dim=256,hidden_size=1024,lr=0.0000_2024-06-17_16-33-24/checkpoint_000009)\n\u001b[36m(func pid=3634)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00043_43_batch_size=32,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-35-39/checkpoint_000001)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3509)\u001b[0m [6,   200] loss: 0.634\n\u001b[36m(func pid=3634)\u001b[0m [3,   200] loss: 1.192\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3697)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=3697)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=3322)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00038_38_batch_size=32,embed_dim=128,hidden_size=1024,lr=0.0001_2024-06-17_16-31-00/checkpoint_000008)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3322)\u001b[0m [10,   200] loss: 0.444\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3697)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00044_44_batch_size=128,embed_dim=256,hidden_size=512,lr=0.0000_2024-06-17_16-36-57/checkpoint_000000)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3634)\u001b[0m [4,   200] loss: 1.044\n\u001b[36m(func pid=3509)\u001b[0m [7,   200] loss: 0.570\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3697)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00044_44_batch_size=128,embed_dim=256,hidden_size=512,lr=0.0000_2024-06-17_16-36-57/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3322)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3757)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=3757)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=3322)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00038_38_batch_size=32,embed_dim=128,hidden_size=1024,lr=0.0001_2024-06-17_16-31-00/checkpoint_000009)\n\u001b[36m(func pid=3634)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00043_43_batch_size=32,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-35-39/checkpoint_000003)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3634)\u001b[0m [5,   200] loss: 0.945\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3509)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00041_41_batch_size=32,embed_dim=64,hidden_size=1024,lr=0.0001_2024-06-17_16-33-30/checkpoint_000006)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=3697)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00044_44_batch_size=128,embed_dim=256,hidden_size=512,lr=0.0000_2024-06-17_16-36-57/checkpoint_000003)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3509)\u001b[0m [8,   200] loss: 0.516\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3757)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00045_45_batch_size=64,embed_dim=64,hidden_size=512,lr=0.0007_2024-06-17_16-37-54/checkpoint_000000)\n\u001b[36m(func pid=3697)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00044_44_batch_size=128,embed_dim=256,hidden_size=512,lr=0.0000_2024-06-17_16-36-57/checkpoint_000004)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3634)\u001b[0m [6,   200] loss: 0.889\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3697)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00044_44_batch_size=128,embed_dim=256,hidden_size=512,lr=0.0000_2024-06-17_16-36-57/checkpoint_000005)\n\u001b[36m(func pid=3757)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00045_45_batch_size=64,embed_dim=64,hidden_size=512,lr=0.0007_2024-06-17_16-37-54/checkpoint_000001)\n\u001b[36m(func pid=3634)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00043_43_batch_size=32,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-35-39/checkpoint_000005)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3509)\u001b[0m [9,   200] loss: 0.473\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3757)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00045_45_batch_size=64,embed_dim=64,hidden_size=512,lr=0.0007_2024-06-17_16-37-54/checkpoint_000002)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=3697)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00044_44_batch_size=128,embed_dim=256,hidden_size=512,lr=0.0000_2024-06-17_16-36-57/checkpoint_000007)\n\u001b[36m(func pid=3634)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00043_43_batch_size=32,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-35-39/checkpoint_000006)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3634)\u001b[0m [8,   200] loss: 0.797\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=3509)\u001b[0m [10,   200] loss: 0.428\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3697)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00044_44_batch_size=128,embed_dim=256,hidden_size=512,lr=0.0000_2024-06-17_16-36-57/checkpoint_000009)\u001b[32m [repeated 4x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3697)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3634)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00043_43_batch_size=32,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-35-39/checkpoint_000007)\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=3820)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=3820)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3634)\u001b[0m [9,   200] loss: 0.771\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3509)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00041_41_batch_size=32,embed_dim=64,hidden_size=1024,lr=0.0001_2024-06-17_16-33-30/checkpoint_000009)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3509)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3757)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00045_45_batch_size=64,embed_dim=64,hidden_size=512,lr=0.0007_2024-06-17_16-37-54/checkpoint_000005)\n\u001b[36m(func pid=3634)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00043_43_batch_size=32,embed_dim=128,hidden_size=512,lr=0.0000_2024-06-17_16-35-39/checkpoint_000008)\n\u001b[36m(func pid=3880)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n\u001b[36m(func pid=3880)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n\u001b[36m(func pid=3757)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00045_45_batch_size=64,embed_dim=64,hidden_size=512,lr=0.0007_2024-06-17_16-37-54/checkpoint_000006)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3634)\u001b[0m [10,   200] loss: 0.738\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=3880)\u001b[0m [1,   200] loss: 1.209\n\u001b[36m(func pid=3820)\u001b[0m [2,   200] loss: 0.713\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3757)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00045_45_batch_size=64,embed_dim=64,hidden_size=512,lr=0.0007_2024-06-17_16-37-54/checkpoint_000007)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3634)\u001b[0m Finished Training\n\u001b[36m(func pid=3880)\u001b[0m [2,   200] loss: 0.456\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3757)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00045_45_batch_size=64,embed_dim=64,hidden_size=512,lr=0.0007_2024-06-17_16-37-54/checkpoint_000008)\u001b[32m [repeated 4x across cluster]\u001b[0m\n\u001b[36m(func pid=3880)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00047_47_batch_size=32,embed_dim=256,hidden_size=256,lr=0.0004_2024-06-17_16-39-30/checkpoint_000001)\n\u001b[36m(func pid=3820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00046_46_batch_size=32,embed_dim=64,hidden_size=64,lr=0.0008_2024-06-17_16-38-07/checkpoint_000002)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3757)\u001b[0m Finished Training\n\u001b[36m(func pid=3820)\u001b[0m [3,   200] loss: 0.519\n\u001b[36m(func pid=3880)\u001b[0m [3,   200] loss: 0.312\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00046_46_batch_size=32,embed_dim=64,hidden_size=64,lr=0.0008_2024-06-17_16-38-07/checkpoint_000003)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3820)\u001b[0m [5,   200] loss: 0.325\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00046_46_batch_size=32,embed_dim=64,hidden_size=64,lr=0.0008_2024-06-17_16-38-07/checkpoint_000004)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3820)\u001b[0m [6,   200] loss: 0.268\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00046_46_batch_size=32,embed_dim=64,hidden_size=64,lr=0.0008_2024-06-17_16-38-07/checkpoint_000005)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3820)\u001b[0m [7,   200] loss: 0.219\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00046_46_batch_size=32,embed_dim=64,hidden_size=64,lr=0.0008_2024-06-17_16-38-07/checkpoint_000006)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3820)\u001b[0m [8,   200] loss: 0.183\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00046_46_batch_size=32,embed_dim=64,hidden_size=64,lr=0.0008_2024-06-17_16-38-07/checkpoint_000007)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3820)\u001b[0m [9,   200] loss: 0.153\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00046_46_batch_size=32,embed_dim=64,hidden_size=64,lr=0.0008_2024-06-17_16-38-07/checkpoint_000008)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3820)\u001b[0m [10,   200] loss: 0.124\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=3820)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3820)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00046_46_batch_size=32,embed_dim=64,hidden_size=64,lr=0.0008_2024-06-17_16-38-07/checkpoint_000009)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36m(func pid=3880)\u001b[0m [10,   200] loss: 0.016\u001b[32m [repeated 2x across cluster]\u001b[0m\n\u001b[36m(func pid=3880)\u001b[0m Finished Training\n","output_type":"stream"},{"name":"stderr","text":"\u001b[36m(func pid=3880)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/tuning_2024-06-17_16-15-04/tuning_c16fa_00047_47_batch_size=32,embed_dim=256,hidden_size=256,lr=0.0004_2024-06-17_16-39-30/checkpoint_000009)\u001b[32m [repeated 2x across cluster]\u001b[0m\n2024-06-17 16:53:11,057\tINFO tune.py:1042 -- Total run time: 2286.85 seconds (2286.79 seconds for the tuning loop).\n","output_type":"stream"}]},{"cell_type":"code","source":"best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\nprint(f\"Best trial config: {best_trial.config}\")\nprint(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:54:36.087325Z","iopub.execute_input":"2024-06-17T16:54:36.087690Z","iopub.status.idle":"2024-06-17T16:54:36.093558Z","shell.execute_reply.started":"2024-06-17T16:54:36.087660Z","shell.execute_reply":"2024-06-17T16:54:36.092566Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Best trial config: {'embed_dim': 128, 'hidden_size': 1024, 'lr': 0.0007315376158978725, 'batch_size': 128}\nBest trial final validation loss: 0.007192907085148048\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n# Load datasets.\ntrain_dataset = CoNLLDataset('/kaggle/working/en_ewt-ud-train.conllu')\ndev_dataset = CoNLLDataset('/kaggle/working/en_ewt-ud-dev.conllu')\n\ndev_dataset.token_vocab = train_dataset.token_vocab\ndev_dataset.pos_vocab = train_dataset.pos_vocab\n\n# Hyperparameters / constants.\ninput_vocab_size = len(train_dataset.token_vocab)\noutput_vocab_size = len(train_dataset.pos_vocab)\nbatch_size = 64\nepochs = 10\n\n# Initialize the model.\nmodel = Tagger(input_vocab_size, output_vocab_size, 128, 256)\nif torch.cuda.is_available():\n    model = model.cuda()\n\n# Loss function weights.\nweight = torch.ones(output_vocab_size)\nweight[0] = 0\nif torch.cuda.is_available():\n    weight = weight.cuda()\n\n# Initialize loss function and optimizer.\nloss_function = torch.nn.NLLLoss(weight)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.00106512)\n\n# Main training loop.\ndata_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                         collate_fn=collate_annotations)\ndev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False,\n                        collate_fn=collate_annotations)\nlosses = []\ni = 0\nfor epoch in range(epochs):\n    for inputs, targets, lengths in data_loader:\n        optimizer.zero_grad()\n        outputs, _ = model(inputs, lengths=lengths)\n    \n        outputs = outputs.view(-1, output_vocab_size)\n        targets = targets.view(-1)\n\n        loss = loss_function(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.item())\n        if (i % 10) == 0:\n            # Compute dev loss over entire dev set.\n            # NOTE: This is expensive. In your work you may want to only use a\n            # subset of the dev set.\n            dev_losses = []\n            for inputs, targets, lengths in dev_loader:\n                outputs, _ = model(inputs, lengths=lengths)\n                outputs = outputs.view(-1, output_vocab_size)\n                targets = targets.view(-1)\n                loss = loss_function(outputs, targets)\n                dev_losses.append(loss.item())\n            avg_train_loss = np.mean(losses)\n            avg_dev_loss = np.mean(dev_losses)\n            losses = []\n            print('Iteration %i - Train Loss: %0.6f - Dev Loss: %0.6f' % (i, avg_train_loss, avg_dev_loss), end='\\r')\n            torch.save(model, 'pos_tagger.pt')\n        i += 1\n\ntorch.save(model, 'pos_tagger.final.pt')","metadata":{"cell_id":"b5f27faa9d0f4d7db8314a2408fdf446","deepnote_cell_type":"code","id":"WL1FdHbkoKK9","outputId":"3a472734-ecad-42bd-f286-e51b57d54964","execution":{"iopub.status.busy":"2024-06-17T16:14:47.731980Z","iopub.execute_input":"2024-06-17T16:14:47.734107Z","iopub.status.idle":"2024-06-17T16:15:01.294252Z","shell.execute_reply.started":"2024-06-17T16:14:47.734077Z","shell.execute_reply":"2024-06-17T16:15:01.292871Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"Iteration 360 - Train Loss: 0.406959 - Dev Loss: 0.489795\r","output_type":"stream"},{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt \n\nplt.plot(losses)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:15:01.295134Z","iopub.status.idle":"2024-06-17T16:15:01.295489Z","shell.execute_reply.started":"2024-06-17T16:15:01.295319Z","shell.execute_reply":"2024-06-17T16:15:01.295333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Load datasets.\ntrain_dataset = CoNLLDataset('/kaggle/working/en_ewt-ud-train.conllu')\ndev_dataset = CoNLLDataset('/kaggle/working/en_ewt-ud-dev.conllu')\n\ndev_dataset.token_vocab = train_dataset.token_vocab\ndev_dataset.pos_vocab = train_dataset.pos_vocab\n\n# Hyperparameters / constants.\ninput_vocab_size = len(train_dataset.token_vocab)\noutput_vocab_size = len(train_dataset.pos_vocab)\nbatch_size = 64\nepochs = 10\n\n# Initialize the model.\nmodel = Tagger(input_vocab_size, output_vocab_size, 128, 128)\nif torch.cuda.is_available():\n    model = model.cuda()\n\n# Loss function weights.\nweight = torch.ones(output_vocab_size)\nweight[0] = 0\nif torch.cuda.is_available():\n    weight = weight.cuda()\n\n# Initialize loss function and optimizer.\nloss_function = torch.nn.NLLLoss(weight)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n\n# Main training loop.\ndata_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                         collate_fn=collate_annotations)\ndev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False,\n                        collate_fn=collate_annotations)\nlosses = []\ni = 0\nfor epoch in range(epochs):\n    for inputs, targets, lengths in data_loader:\n        optimizer.zero_grad()\n        outputs, _ = model(inputs, lengths=lengths)\n    \n        outputs = outputs.view(-1, output_vocab_size)\n        targets = targets.view(-1)\n\n        loss = loss_function(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.item())\n        if (i % 10) == 0:\n            # Compute dev loss over entire dev set.\n            # NOTE: This is expensive. In your work you may want to only use a\n            # subset of the dev set.\n            dev_losses = []\n            for inputs, targets, lengths in dev_loader:\n                outputs, _ = model(inputs, lengths=lengths)\n                outputs = outputs.view(-1, output_vocab_size)\n                targets = targets.view(-1)\n                loss = loss_function(outputs, targets)\n                dev_losses.append(loss.item())\n            avg_train_loss = np.mean(losses)\n            avg_dev_loss = np.mean(dev_losses)\n            losses = []\n            print('Iteration %i - Train Loss: %0.6f - Dev Loss: %0.6f' % (i, avg_train_loss, avg_dev_loss), end='\\r')\n            torch.save(model, 'pos_tagger.pt')\n        i += 1\n\ntorch.save(model, 'pos_tagger.final.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:15:01.297188Z","iopub.status.idle":"2024-06-17T16:15:01.297603Z","shell.execute_reply.started":"2024-06-17T16:15:01.297392Z","shell.execute_reply":"2024-06-17T16:15:01.297409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(losses)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T16:15:01.299336Z","iopub.status.idle":"2024-06-17T16:15:01.299648Z","shell.execute_reply.started":"2024-06-17T16:15:01.299494Z","shell.execute_reply":"2024-06-17T16:15:01.299507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation\n\nFor tagging tasks the typical evaluation metric are accuracy and f1-score (e.g. the harmonic mean of precision and recall):\n\n$$ \\text{f1-score} = 2 \\frac{\\text{precision} * \\text{recall}}{\\text{precision} + \\text{recall}} $$\n\nHere are the results for our final model:","metadata":{"cell_id":"7203c2b15a3944099f252e6299d26fba","deepnote_cell_type":"markdown","id":"TJ5SiU0foKK-"}},{"cell_type":"code","source":"# Collect the predictions and targets\ny_true = []\ny_pred = []\n\nfor inputs, targets, lengths in dev_loader:\n    outputs, _ = model(inputs, lengths=lengths)\n    _, preds = torch.max(outputs, dim=2)\n    targets = targets.view(-1)\n    preds = preds.view(-1)\n    if torch.cuda.is_available():\n        targets = targets.cpu()\n        preds = preds.cpu()\n    y_true.append(targets.data.numpy())\n    y_pred.append(preds.data.numpy())\n\n# Stack into numpy arrays\ny_true = np.concatenate(y_true)\ny_pred = np.concatenate(y_pred)\n\n# Compute accuracy\nacc = np.mean(y_true[y_true != 0] == y_pred[y_true != 0])\nprint('Accuracy - %0.6f\\n' % acc)\n\n# Evaluate f1-score\nfrom sklearn.metrics import f1_score\nscore = f1_score(y_true, y_pred, average=None)\nprint('F1-scores:\\n')\nfor label, score in zip(dev_dataset.pos_vocab._id2word[1:], score[1:]):\n    print('%s - %0.6f' % (label, score))","metadata":{"cell_id":"dc1ddda44d1f4771888dd4a00c4fe222","deepnote_cell_type":"code","id":"UK0JuMR2oKK-","outputId":"896d8953-2efa-4362-e090-ae5ddabb929b","execution":{"iopub.status.busy":"2024-06-17T16:15:01.300868Z","iopub.status.idle":"2024-06-17T16:15:01.301218Z","shell.execute_reply.started":"2024-06-17T16:15:01.301061Z","shell.execute_reply":"2024-06-17T16:15:01.301075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\nNow let's look at some of the model's predictions.","metadata":{"cell_id":"80db8ab9f03c4c29a0b4e3bd597a7782","deepnote_cell_type":"markdown","id":"bI15KIqWoKK-"}},{"cell_type":"code","source":"model = torch.load('pos_tagger.final.pt')\n\ndef inference(sentence):\n    # Convert words to id tensor.\n    ids = [[dataset.token_vocab.word2id(x)] for x in sentence]\n    ids = Variable(torch.LongTensor(ids))\n    if torch.cuda.is_available():\n        ids = ids.cuda()\n    # Get model output.\n    output, _ = model(ids)\n    _, preds = torch.max(output, dim=2)\n    if torch.cuda.is_available():\n        preds = preds.cpu()\n    preds = preds.data.view(-1).numpy()\n    pos_tags = [dataset.pos_vocab.id2word(x) for x in preds]\n    for word, tag in zip(sentence, pos_tags):\n        print('%s - %s' % (word, tag))","metadata":{"cell_id":"22433cbac35b46e1a0d8a86121da06ef","deepnote_cell_type":"code","id":"q6uOBHu8oKK-","execution":{"iopub.status.busy":"2024-06-17T16:15:01.302553Z","iopub.status.idle":"2024-06-17T16:15:01.302864Z","shell.execute_reply.started":"2024-06-17T16:15:01.302712Z","shell.execute_reply":"2024-06-17T16:15:01.302725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = \"sdfgkj asd;glkjsdg ;lkj  .\".split()\ninference(sentence)","metadata":{"cell_id":"ee8488453add4dcab4e6d0e273f96d37","deepnote_cell_type":"code","id":"6HipFvTdoKK-","execution":{"iopub.status.busy":"2024-06-17T16:15:01.303962Z","iopub.status.idle":"2024-06-17T16:15:01.304281Z","shell.execute_reply.started":"2024-06-17T16:15:01.304124Z","shell.execute_reply":"2024-06-17T16:15:01.304138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Example: Sentiment Analysis\n\nAccording to [Wikipedia](https://en.wikipedia.org/wiki/Sentiment_analysis):\n\n>Opinion mining (sometimes known as sentiment analysis or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.\n\nFormally, given a sequence of words $\\mathbf{x} = \\left< x_1, x_2, \\ldots, x_t \\right>$ the goal is to learn a model $P(y \\,|\\, \\mathbf{x})$ where $y$ is the sentiment associated to the sentence. This is very similar to the problem above, with the exception that we only want a single output for each sentence not a sentence. Accordingly, we will only highlight the neccessary changes that need to be made.\n\n### Dataset\n\nWe will be using the Kaggle 'Sentiment Analysis on Movie Reviews' dataset [[link](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data)]. You will need to agree to the Kaggle terms of service in order to download this data. The following code can be used to process this data.","metadata":{"cell_id":"154a31deefb7443d9df9d63ec377bad0","deepnote_cell_type":"markdown","id":"RhJn0Z1XoKK-"}},{"cell_type":"code","source":"import torch\nfrom collections import Counter\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset\n\n\nclass Annotation(object):\n    def __init__(self):\n        \"\"\"A helper object for storing annotation data.\"\"\"\n        self.tokens = []\n        self.sentiment = None\n\n\nclass SentimentDataset(Dataset):\n    def __init__(self, fname):\n        \"\"\"Initializes the SentimentDataset.\n        Args:\n            fname: The .tsv file to load data from.\n        \"\"\"\n        self.fname = fname\n        self.annotations = self.process_tsv_file(fname)\n        self.token_vocab = Vocab([x.tokens for x in self.annotations],\n                                 unk_token='<unk>')\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        annotation = self.annotations[idx]\n        input = [self.token_vocab.word2id(x) for x in annotation.tokens]\n        target = annotation.sentiment\n        return input, target\n\n    def process_tsv_file(self, fname):\n        # Read the entire file.\n        with open(fname, 'r') as f:\n            lines = f.readlines()\n        annotations = []\n        observed_ids = set()\n        for line in lines[1:]:\n            annotation = Annotation()\n            _, sentence_id, sentence, sentiment = line.split('\\t')\n            sentence_id = sentence_id\n            if sentence_id in observed_ids:\n                continue\n            else:\n                observed_ids.add(sentence_id)\n            annotation.tokens = sentence.split()\n            annotation.sentiment = int(sentiment)\n            if len(annotation.tokens) > 0:\n                annotations.append(annotation)\n        return annotations\n\n\ndef pad(sequences, max_length, pad_value=0):\n    \"\"\"Pads a list of sequences.\n    Args:\n        sequences: A list of sequences to be padded.\n        max_length: The length to pad to.\n        pad_value: The value used for padding.\n    Returns:\n        A list of padded sequences.\n    \"\"\"\n    out = []\n    for sequence in sequences:\n        padded = sequence + [0]*(max_length - len(sequence))\n        out.append(padded)\n    return out\n\n\ndef collate_annotations(batch):\n    \"\"\"Function used to collate data returned by CoNLLDataset.\"\"\"\n    # Get inputs, targets, and lengths.\n    inputs, targets = zip(*batch)\n    lengths = [len(x) for x in inputs]\n    # Sort by length.\n    sort = sorted(zip(inputs, targets, lengths),\n                  key=lambda x: x[2],\n                  reverse=True)\n    inputs, targets, lengths = zip(*sort)\n    # Pad.\n    max_length = max(lengths)\n    inputs = pad(inputs, max_length)\n    # Transpose.\n    inputs = list(map(list, zip(*inputs)))\n    # Convert to PyTorch variables.\n    inputs = Variable(torch.LongTensor(inputs))\n    targets = Variable(torch.LongTensor(targets))\n    lengths = Variable(torch.LongTensor(lengths))\n    if torch.cuda.is_available():\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n        lengths = lengths.cuda()\n    return inputs, targets, lengths","metadata":{"cell_id":"1778e46f18e34623bdb0df6de45274af","deepnote_cell_type":"code","id":"_rh_a1nYoKK-","execution":{"iopub.status.busy":"2024-06-17T16:15:01.305491Z","iopub.status.idle":"2024-06-17T16:15:01.305826Z","shell.execute_reply.started":"2024-06-17T16:15:01.305658Z","shell.execute_reply":"2024-06-17T16:15:01.305672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model\n\nThe model architecture we will use for sentiment classification is almost exactly the same as the one we used for tagging. The only difference is that we want the model to produce a single output at the end, not a sequence of outputs. While there are many ways to do this, a simple approach is to just use the final hidden state of the recurrent layer as the input to the fully connected layer. This approach is particularly nice in PyTorch since the forward pass of the recurrent layer returns the final hidden states as its second output (see the note in the code below if this is unclear), so we do not need to do any fancy indexing tricks to get them.\n\nFormally, the model architecture we will use is:\n\n1. Embed the input words into a 200 dimensional vector space.\n2. Feed the word embeddings into a GRU.\n3. Feed the final hidden state output by the GRU into a fully connected layer.\n4. Use a softmax activation to get the probabilities of the different labels.","metadata":{"cell_id":"78399039dd9f4fddaddff7d9499a5e46","deepnote_cell_type":"markdown","id":"5WvjvzPnoKK_"}},{"cell_type":"code","source":"from torch import nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n\nclass SentimentClassifier(nn.Module):\n    def __init__(self,\n                 input_vocab_size,\n                 output_vocab_size,\n                 embedding_dim=64,\n                 hidden_size=64):\n        \"\"\"Initializes the tagger.\n\n        Args:\n            input_vocab_size: Size of the input vocabulary.\n            output_vocab_size: Size of the output vocabulary.\n            embedding_dim: Dimension of the word embeddings.\n            hidden_size: Number of units in each LSTM hidden layer.\n        \"\"\"\n        # Always do this!!!\n        super(SentimentClassifier, self).__init__()\n\n        # Store parameters\n        self.input_vocab_size = input_vocab_size\n        self.output_vocab_size = output_vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_size = hidden_size\n\n        # Define layers\n        self.word_embeddings = nn.Embedding(input_vocab_size, embedding_dim,\n                                            padding_idx=0)\n        self.rnn = nn.GRU(embedding_dim, hidden_size, dropout=0.9)\n        self.fc = nn.Linear(hidden_size, output_vocab_size)\n        self.activation = nn.LogSoftmax(dim=2)\n\n    def forward(self, x, lengths=None, hidden=None):\n        \"\"\"Computes a forward pass of the language model.\n\n        Args:\n            x: A LongTensor w/ dimension [seq_len, batch_size].\n            lengths: The lengths of the sequences in x.\n            hidden: Hidden state to be fed into the lstm.\n\n        Returns:\n            net: the output representation for each word in the sequence.\n            hidden: Hidden state of the last timestamp.\n        \"\"\"\n        seq_len, batch_size = x.size()\n\n        # If no hidden state is provided, then default to zeros.\n        if hidden is None:\n            hidden = Variable(torch.zeros(1, batch_size, self.hidden_size))\n            if torch.cuda.is_available():\n                hidden = hidden.cuda()\n\n        net = self.word_embeddings(x)\n        if lengths is not None:\n            lengths_list = lengths.data.view(-1).tolist()\n            net = pack_padded_sequence(net, lengths_list)\n        net, hidden = self.rnn(net, hidden)\n        # NOTE: we are using hidden as the input to the fully-connected layer, not net!!!\n        net = self.fc(hidden)\n        net = self.activation(net)\n\n        return net, hidden","metadata":{"cell_id":"b32c94c702c043eeae6eec5cdfbc2f6e","deepnote_cell_type":"code","id":"9A7Fj252oKK_","execution":{"iopub.status.busy":"2024-06-17T16:15:01.306980Z","iopub.status.idle":"2024-06-17T16:15:01.307304Z","shell.execute_reply.started":"2024-06-17T16:15:01.307143Z","shell.execute_reply":"2024-06-17T16:15:01.307157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training\n\nThis code should look pretty familiar by now...","metadata":{"cell_id":"aec2eebc0821456cbcae8fb5ad822b2f","deepnote_cell_type":"markdown","id":"9YGN2M8coKK_"}},{"cell_type":"code","source":"import numpy as np\nfrom torch.utils.data import DataLoader\n\n# Load dataset.\nsentiment_dataset = SentimentDataset('train.tsv')\n\n# Hyperparameters / constants.\ninput_vocab_size = len(sentiment_dataset.token_vocab)\noutput_vocab_size = 5\nbatch_size = 16\nepochs = 7\n\n# Initialize the model.\nmodel = SentimentClassifier(input_vocab_size, output_vocab_size)\nif torch.cuda.is_available():\n    model = model.cuda()\n\n# Initialize loss function and optimizer.\nloss_function = torch.nn.NLLLoss()\noptimizer = torch.optim.Adam(model.parameters())\n\n# Main training loop.\ndata_loader = DataLoader(sentiment_dataset, batch_size=batch_size, shuffle=True,\n                         collate_fn=collate_annotations)\nlosses = []\ni = 0\nfor epoch in range(epochs):\n    for inputs, targets, lengths in data_loader:\n        optimizer.zero_grad()\n        outputs, _ = model(inputs, lengths=lengths)\n\n        outputs = outputs.view(-1, output_vocab_size)\n        targets = targets.view(-1)\n\n        loss = loss_function(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.data[0])\n        if (i % 100) == 0:\n            average_loss = np.mean(losses)\n            losses = []\n            print('Iteration %i - Loss: %0.6f' % (i, average_loss), end='\\r')\n        if (i % 1000) == 0:\n            torch.save(model, 'sentiment_classifier.pt')\n        i += 1\n\ntorch.save(model, 'sentiment_classifier.final.pt')","metadata":{"scrolled":true,"cell_id":"2458de9ec1f64d7c803cef4aa93315b9","deepnote_cell_type":"code","id":"YhlQ16-PoKK_","execution":{"iopub.status.busy":"2024-06-17T16:15:01.309072Z","iopub.status.idle":"2024-06-17T16:15:01.309508Z","shell.execute_reply.started":"2024-06-17T16:15:01.309280Z","shell.execute_reply":"2024-06-17T16:15:01.309298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\nLastly, let's examine some model outputs:","metadata":{"cell_id":"6f8573ec9a504e5f9aff56e83e51a147","deepnote_cell_type":"markdown","id":"MiQFBaIfoKK_"}},{"cell_type":"code","source":"model = torch.load('sentiment_classifier.final.pt')\n\ndef inference(sentence):\n    # Convert words to id tensor.\n    ids = [[sentiment_dataset.token_vocab.word2id(x)] for x in sentence]\n    ids = Variable(torch.LongTensor(ids))\n    if torch.cuda.is_available():\n        ids = ids.cuda()\n    # Get model output.\n    output, _ = model(ids)\n    _, pred = torch.max(output, dim=2)\n    if torch.cuda.is_available():\n        pred = pred.cpu()\n    pred = pred.data.view(-1).numpy()\n    print('Sentence: %s' % ' '.join(sentence))\n    print('Sentiment (0=negative, 4=positive): %i' % pred)","metadata":{"cell_id":"02aa42580b12415cb8c911f0f8e1d3a9","deepnote_cell_type":"code","id":"TKh0flnIoKK_","execution":{"iopub.status.busy":"2024-06-17T16:15:01.312262Z","iopub.status.idle":"2024-06-17T16:15:01.312733Z","shell.execute_reply.started":"2024-06-17T16:15:01.312491Z","shell.execute_reply":"2024-06-17T16:15:01.312510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = 'Zot zot  .'.split()\ninference(sentence)","metadata":{"cell_id":"30f5bb3b65be454d8e4f1c3e549524b7","deepnote_cell_type":"code","id":"uhpeZPpaoKK_","outputId":"b48b6758-f5bd-46a6-ec5a-a108331111a6","execution":{"iopub.status.busy":"2024-06-17T16:15:01.314286Z","iopub.status.idle":"2024-06-17T16:15:01.314633Z","shell.execute_reply.started":"2024-06-17T16:15:01.314470Z","shell.execute_reply":"2024-06-17T16:15:01.314484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a606c68a-0fb4-4c6a-9886-fddaecf4a93b' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown","id":"mWXd8125oKLA"}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}