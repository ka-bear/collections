{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sabkx/venv-metal\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "train_iter = iter(AG_NEWS(split=\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabkx/venv-metal/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/sabkx/venv-metal/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_iter = AG_NEWS(split=\"train\")\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text))\n",
    "        processed_text = F.pad(processed_text, (0, 50-processed_text.size(0)))\n",
    "        text_list.append(processed_text)\n",
    "    return torch.tensor(label_list), torch.stack(text_list)\n",
    "\n",
    "train_iter = AG_NEWS(split=\"train\")\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=64, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks chatGPT for the code :DDDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[[-4.7706e-01, -1.1555e-01, -2.7620e-04,  ...,  1.5167e+00,\n",
      "           1.0012e+00,  1.7228e+00],\n",
      "         [ 1.7881e-01, -8.2281e-01, -6.0000e-01,  ...,  5.5161e-01,\n",
      "          -2.1384e-01,  2.5889e+00],\n",
      "         [-8.3890e-01,  2.9913e-01,  4.5329e-01,  ...,  3.8309e-03,\n",
      "          -4.3858e-02,  1.1039e+00],\n",
      "         ...,\n",
      "         [ 1.6570e-01, -1.1907e+00, -1.4193e+00,  ...,  4.6234e-01,\n",
      "          -1.1105e-01,  2.1113e-01],\n",
      "         [-1.0796e+00,  4.1052e-01, -9.9326e-01,  ..., -1.0755e+00,\n",
      "           1.1072e+00,  1.6945e-02],\n",
      "         [ 6.6218e-01, -7.2390e-01,  9.8125e-01,  ...,  7.3946e-01,\n",
      "          -1.4586e-01, -2.0322e-01]],\n",
      "\n",
      "        [[-9.2782e-01,  5.2059e-02, -6.5602e-01,  ..., -9.2437e-01,\n",
      "          -1.4547e+00,  5.2213e-01],\n",
      "         [ 1.3634e+00, -5.7781e-01, -9.7092e-01,  ..., -3.7101e-01,\n",
      "           9.3096e-01,  4.7224e-01],\n",
      "         [-8.2703e-01, -1.0037e+00,  2.2094e+00,  ..., -2.2256e-01,\n",
      "          -4.2365e-01,  4.2937e-01],\n",
      "         ...,\n",
      "         [ 2.0182e-01,  1.1745e+00, -8.5649e-01,  ...,  8.4254e-01,\n",
      "           1.2600e+00,  4.1860e-01],\n",
      "         [-1.7383e+00, -1.6762e+00, -1.6929e+00,  ...,  3.1035e-01,\n",
      "           6.1789e-01, -1.5623e+00],\n",
      "         [ 5.0177e-01, -9.1616e-01,  4.4522e-02,  ..., -6.2980e-01,\n",
      "           5.1009e-01,  3.7839e-02]],\n",
      "\n",
      "        [[-3.8025e-01, -8.2816e-01,  5.3900e-01,  ..., -4.6871e-01,\n",
      "           1.0308e+00, -1.5565e+00],\n",
      "         [-1.2412e+00, -1.2130e-01, -1.0790e+00,  ...,  9.6736e-01,\n",
      "          -1.6757e+00,  2.6579e-01],\n",
      "         [ 1.4251e+00, -1.0680e-01, -1.6495e+00,  ..., -5.4478e-02,\n",
      "           1.5324e+00, -7.2597e-01],\n",
      "         ...,\n",
      "         [-9.0887e-03, -2.0871e-01,  2.6359e-01,  ..., -6.8969e-01,\n",
      "           1.0127e-01,  2.7615e-01],\n",
      "         [ 1.0468e+00, -1.0980e+00, -4.4954e-01,  ...,  5.8449e-01,\n",
      "           1.2149e-01, -5.6177e-01],\n",
      "         [ 5.9199e-01,  1.4862e+00, -7.0598e-01,  ...,  1.1030e+00,\n",
      "          -7.4108e-01,  9.8245e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.7023e+00, -1.0106e+00,  1.8401e-01,  ..., -5.1292e-01,\n",
      "           9.6784e-01,  1.1566e+00],\n",
      "         [ 1.2651e+00, -2.0628e-01,  1.0675e+00,  ..., -4.9299e-01,\n",
      "          -7.5983e-02,  1.1578e+00],\n",
      "         [-3.7191e-01, -6.9402e-01, -1.4819e+00,  ...,  7.5523e-01,\n",
      "          -2.6758e-01,  7.5465e-01],\n",
      "         ...,\n",
      "         [-7.8578e-01, -4.0134e-01, -7.4107e-01,  ..., -6.2095e-01,\n",
      "          -1.0640e+00, -1.1098e+00],\n",
      "         [-1.9870e+00, -7.4521e-01, -7.7487e-01,  ...,  1.0777e+00,\n",
      "          -7.5561e-01, -2.2502e+00],\n",
      "         [ 7.3156e-02,  2.2122e-01,  1.1994e+00,  ...,  4.6400e-01,\n",
      "          -1.4437e+00, -2.0320e+00]],\n",
      "\n",
      "        [[-4.5180e-01,  2.9950e-01,  5.7281e-01,  ...,  8.0340e-01,\n",
      "          -5.4587e-01, -2.2302e+00],\n",
      "         [-2.4964e-01,  5.5741e-01, -6.8832e-01,  ..., -1.6373e+00,\n",
      "           1.0644e+00,  5.7147e-01],\n",
      "         [ 6.9632e-01, -2.9264e-01, -1.3872e+00,  ...,  2.9095e-01,\n",
      "          -8.1680e-01, -8.5148e-02],\n",
      "         ...,\n",
      "         [-1.2414e+00,  2.5627e-01, -4.0885e-01,  ..., -7.5812e-01,\n",
      "          -4.2736e-01, -2.4364e-01],\n",
      "         [-1.3850e+00, -8.4768e-01,  1.2498e+00,  ..., -1.5898e+00,\n",
      "          -4.0304e-01,  1.1412e+00],\n",
      "         [ 4.4770e-01, -1.7319e+00, -6.1855e-01,  ...,  2.0730e+00,\n",
      "           1.0981e+00, -8.7957e-01]],\n",
      "\n",
      "        [[-1.3568e+00, -3.8236e-01,  4.4842e-01,  ...,  2.6167e-01,\n",
      "           7.2698e-01,  2.2257e-01],\n",
      "         [-9.0432e-01,  1.1841e+00,  1.3976e+00,  ..., -4.4508e-01,\n",
      "           9.5470e-01, -3.0685e-01],\n",
      "         [-4.1684e-02,  3.0761e-01,  8.0342e-02,  ...,  1.1074e+00,\n",
      "           1.1557e-01, -7.6885e-01],\n",
      "         ...,\n",
      "         [ 4.8308e-01, -1.2360e+00, -2.0850e+00,  ..., -8.4725e-01,\n",
      "          -7.9347e-01, -4.0769e-01],\n",
      "         [ 7.1151e-01, -9.9292e-01,  5.1558e-01,  ..., -1.3142e-01,\n",
      "           1.0983e+00, -4.2143e-01],\n",
      "         [-3.4062e-01,  1.9634e+00, -4.7680e-01,  ..., -9.3726e-01,\n",
      "           1.5415e+00,  1.2730e+00]]])\n",
      "Self-attention output: tensor([[[-0.0116, -0.1237,  0.0260,  ...,  0.1152, -0.0210,  0.1260],\n",
      "         [ 0.1271, -0.1277,  0.0151,  ...,  0.0988, -0.0578,  0.1706],\n",
      "         [-0.0241, -0.1286, -0.0134,  ...,  0.1167,  0.0036,  0.1448],\n",
      "         ...,\n",
      "         [-0.0215, -0.1126,  0.0420,  ...,  0.1074, -0.0435,  0.1415],\n",
      "         [-0.0369, -0.0902,  0.0097,  ...,  0.0951, -0.0280,  0.1436],\n",
      "         [-0.0540, -0.1123,  0.0187,  ...,  0.0770,  0.0213,  0.1320]],\n",
      "\n",
      "        [[-0.0366,  0.0551,  0.0747,  ..., -0.0775,  0.1523,  0.1188],\n",
      "         [-0.1006,  0.0305,  0.0505,  ..., -0.1543,  0.1043,  0.0538],\n",
      "         [-0.0578,  0.0618,  0.0655,  ..., -0.1224,  0.0813,  0.1011],\n",
      "         ...,\n",
      "         [-0.1144,  0.0563,  0.0168,  ..., -0.1136,  0.0963,  0.1098],\n",
      "         [-0.0832,  0.0256,  0.0125,  ..., -0.1083,  0.1121,  0.0805],\n",
      "         [-0.0691,  0.0781,  0.0138,  ..., -0.1140,  0.0752,  0.0879]],\n",
      "\n",
      "        [[-0.1257,  0.0576, -0.0561,  ..., -0.1204,  0.0623,  0.0040],\n",
      "         [-0.1686,  0.0850, -0.0707,  ..., -0.1055,  0.0595, -0.0094],\n",
      "         [-0.1524,  0.0753, -0.0942,  ..., -0.0867,  0.0637,  0.0029],\n",
      "         ...,\n",
      "         [-0.0937,  0.1230, -0.0187,  ..., -0.0623, -0.0073,  0.0241],\n",
      "         [-0.1483,  0.0726, -0.0700,  ..., -0.1044,  0.0902,  0.0037],\n",
      "         [-0.1056,  0.0447, -0.0477,  ..., -0.0618,  0.0514, -0.0007]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0342, -0.1240,  0.0089,  ..., -0.0651,  0.1165,  0.1578],\n",
      "         [ 0.0105, -0.0743,  0.0338,  ..., -0.0670,  0.1187,  0.1579],\n",
      "         [-0.0392, -0.1225,  0.0024,  ..., -0.0544,  0.1492,  0.1322],\n",
      "         ...,\n",
      "         [-0.0238, -0.1099,  0.0288,  ..., -0.0098,  0.1236,  0.1486],\n",
      "         [-0.0197, -0.1307, -0.0243,  ..., -0.0797,  0.1456,  0.0708],\n",
      "         [-0.0022, -0.0711, -0.0150,  ..., -0.0852,  0.1531,  0.1536]],\n",
      "\n",
      "        [[-0.0524,  0.0519, -0.1055,  ..., -0.1338,  0.0418, -0.0360],\n",
      "         [-0.0725,  0.0795, -0.0784,  ..., -0.1822,  0.0100, -0.0505],\n",
      "         [-0.0446,  0.0583, -0.0700,  ..., -0.1687,  0.0286, -0.0026],\n",
      "         ...,\n",
      "         [-0.0736,  0.0548, -0.1066,  ..., -0.1480,  0.0377, -0.0745],\n",
      "         [-0.0369,  0.0998, -0.1028,  ..., -0.1216,  0.0941, -0.0648],\n",
      "         [-0.0712,  0.1083, -0.0306,  ..., -0.1713, -0.0065, -0.0101]],\n",
      "\n",
      "        [[ 0.1781,  0.0248,  0.0814,  ...,  0.0030,  0.0335, -0.0820],\n",
      "         [ 0.1270,  0.0231,  0.1394,  ...,  0.0417,  0.0265, -0.1295],\n",
      "         [ 0.1755,  0.0232,  0.0511,  ..., -0.0344,  0.0653, -0.1181],\n",
      "         ...,\n",
      "         [ 0.1648,  0.0320,  0.0953,  ..., -0.0177,  0.0461, -0.1059],\n",
      "         [ 0.2458,  0.0564,  0.0905,  ..., -0.0315,  0.0147, -0.0551],\n",
      "         [ 0.2299,  0.0645,  0.0998,  ..., -0.0469, -0.0184, -0.0503]]],\n",
      "       grad_fn=<BmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Define the weights for query, key, and value matrices\n",
    "        self.W_q = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_k = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_v = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # Compute queries, keys, and values\n",
    "        Q = self.W_q(x)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        K = self.W_k(x)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        V = self.W_v(x)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # Q @ K.T: (batch_size, seq_len, hidden_dim) @ (batch_size, hidden_dim, seq_len)\n",
    "        # Output shape: (batch_size, seq_len, seq_len)\n",
    "        attention_scores = torch.bmm(Q, K.transpose(1, 2)) / (self.hidden_dim ** 0.5)\n",
    "        \n",
    "        # Apply softmax to get the attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # Shape: (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        # Compute the weighted sum of values\n",
    "        # attention_weights @ V: (batch_size, seq_len, seq_len) @ (batch_size, seq_len, hidden_dim)\n",
    "        # Output shape: (batch_size, seq_len, hidden_dim)\n",
    "        attention_output = torch.bmm(attention_weights, V)\n",
    "        \n",
    "        return attention_output\n",
    "\n",
    "# Example usage\n",
    "batch_size = 64\n",
    "seq_len = 50\n",
    "embedding_dim = 128\n",
    "\n",
    "# Random input tensor\n",
    "x = torch.randn(batch_size, seq_len, embedding_dim)\n",
    "\n",
    "# Instantiate and apply self-attention\n",
    "self_attention = SelfAttention(embedding_dim)\n",
    "output = self_attention(x)\n",
    "\n",
    "print(\"Input:\", x)\n",
    "print(\"Self-attention output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_with_Attention(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, embedding_dim, num_classes):\n",
    "        super(LSTM_with_Attention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm_1 = nn.LSTM(input_size=embedding_dim, \n",
    "                              hidden_size=hidden_dim,\n",
    "                              num_layers=1,\n",
    "                              bidirectional=False,\n",
    "                              dropout=0,\n",
    "                              batch_first=True)\n",
    "        self.attention_1 = SelfAttention(hidden_dim=hidden_dim)\n",
    "        self.lstm_2 = nn.LSTM(input_size=hidden_dim, \n",
    "                              hidden_size=hidden_dim,\n",
    "                              num_layers=1,\n",
    "                              bidirectional=False,\n",
    "                              dropout=0,\n",
    "                              batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"x:\", x.shape)\n",
    "        embedded = self.embedding(x)\n",
    "        # print(embedded.shape)\n",
    "        output, (ht, ct) = self.lstm_1(embedded)\n",
    "        # print(\"output:\", output.shape)\n",
    "        # print(\"ht[-1]:\", ht[-1].shape)\n",
    "        # print(\"ct:\", ct.shape)\n",
    "        # print(\"self.fc(ht[-1]):\", self.fc(ht[-1]).shape)\n",
    "        attention_output = self.attention_1(output)\n",
    "        # print(\"attention_output:\", attention_output.shape)\n",
    "        output2, (ht2, ct2) = self.lstm_2(attention_output)\n",
    "        # print(ht2[-1].shape)\n",
    "        return self.fc(ht2[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split=\"train\")\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = LSTM_with_Attention(vocab_size, 128, emsize, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1782 batches | accuracy    0.252\n",
      "| epoch   1 |  1000/ 1782 batches | accuracy    0.256\n",
      "| epoch   1 |  1500/ 1782 batches | accuracy    0.254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     32\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     accu_val \u001b[38;5;241m=\u001b[39m evaluate(valid_dataloader)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m total_accu \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m total_accu \u001b[38;5;241m>\u001b[39m accu_val:\n",
      "Cell \u001b[0;32mIn[90], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (label, text) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m     predicted_label \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(predicted_label, label)\n\u001b[1;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[67], line 25\u001b[0m, in \u001b[0;36mLSTM_with_Attention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(embedded.shape)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m output, (ht, ct) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# print(\"output:\", output.shape)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print(\"ht[-1]:\", ht[-1].shape)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# print(\"ct:\", ct.shape)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# print(\"self.fc(ht[-1]):\", self.fc(ht[-1]).shape)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_1(output)\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.12/site-packages/torch/nn/modules/rnn.py:911\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    908\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 911\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    915\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5 # learning rate\n",
    "BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
