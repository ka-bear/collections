{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30627,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "\n",
    "**Note: All the images are from the Reference lists section or the internet**\n",
    "\n",
    "We are going to fine-tuning a t5-small quantized model for English to French translation-related tasks using the LoRA method. We use the peft and transformers libraries for training.\n",
    "\n",
    "\n",
    "# About quantization\n",
    "\n",
    "Please check the list below:\n",
    "\n",
    "* [Quantization Technologies](https://www.kaggle.com/code/aisuko/quantization-technologies)\n",
    "* [Zero Degradation matrix multiplication](https://www.kaggle.com/code/aisuko/zero-degradation-matrix-multiplication)\n",
    "* [Lighter models on GPU for inference](https://www.kaggle.com/code/aisuko/lighter-models-on-gpu-for-inference)\n",
    "\n",
    "\n",
    "# About LoRA(Low Rank Adaptation)\n",
    "\n",
    "> A technique that accelerates the fine-tuning of large models while consuming less memory.\n",
    "\n",
    "\n",
    "**The idea is to freeze the original pre-trained weights(Matrices) and introduce new updata matrices**. These new matrics are trained on new data while keeping the overall number of changes low. The original weights matrix doesn't receive any adjustments. And finally, both the original and the adapted weights are combined.\n",
    "\n",
    "![](https://files.mastodon.social/media_attachments/files/111/702/004/494/881/797/original/a26697e010f0096b.webp)\n",
    "\n",
    "LoRA makes fine-tuning more efficient by drastically reducing the number of **trainable parameters**. In principle, LoRA can be applied to any subset of weight matrices in a neural network to reduce the number of trainable parameters. However, for simplicity and further parameter efficiency, **in Transformer models LoRA is typically applied to attention blocks only**. **The resulting number of trainable parameters in a LoRA model depends on the size of the low-rank update matrices, which is determined manily by the rank `r` and the shape of the original weight matrix**.\n",
    "\n",
    "\n",
    "The differences between QLoRA and LoRA in real word case see notebook [fine-tuning llama2 with QLoRA](https://www.kaggle.com/code/aisuko/fine-tuning-llama2-with-qlora?scriptVersionId=158763163&cellId=1)."
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install transformers==4.36.2\n",
    "!pip install bitsandbytes==0.41.3\n",
    "!pip install accelerate==0.25.0\n",
    "!pip install datasets==2.15.0\n",
    "!pip install evaluate==0.4.1\n",
    "!pip install peft==0.7.1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:38:29.007773Z",
     "iopub.execute_input": "2024-06-18T13:38:29.008039Z",
     "iopub.status.idle": "2024-06-18T13:39:56.777225Z",
     "shell.execute_reply.started": "2024-06-18T13:38:29.008015Z",
     "shell.execute_reply": "2024-06-18T13:39:56.776135Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "login(token=\"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\")\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"]=\"b47babba2becb2d7866813aeb59313346b518185\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"Fine-tuning t5-small-on-opus100\"\n",
    "os.environ[\"WANDB_NAME\"] = \"ft-t5-small-on-opus100\""
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:39:56.779555Z",
     "iopub.execute_input": "2024-06-18T13:39:56.779907Z",
     "iopub.status.idle": "2024-06-18T13:39:57.333558Z",
     "shell.execute_reply.started": "2024-06-18T13:39:56.779876Z",
     "shell.execute_reply": "2024-06-18T13:39:57.332619Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading Dataset\n",
    "\n",
    "We are going to use the optus 100 dataset for training which gives us access to more than 100 different languages."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "configs=get_dataset_config_names(\"wmt14\")\n",
    "print(configs)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:39:57.334577Z",
     "iopub.execute_input": "2024-06-18T13:39:57.334821Z",
     "iopub.status.idle": "2024-06-18T13:40:02.607529Z",
     "shell.execute_reply.started": "2024-06-18T13:39:57.334799Z",
     "shell.execute_reply": "2024-06-18T13:40:02.606572Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5c95379ee074a199ee82749cacba090"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfa2a81616e14f33992f3d31789b2349"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "['cs-en', 'de-en', 'fr-en', 'hi-en', 'ru-en']\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the \"en-fr\" data for language translation. Let's download and load the dataset though the `load_dataset`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset=load_dataset(\"wmt14\", \"de-en\")\n",
    "dataset"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:40:02.610320Z",
     "iopub.execute_input": "2024-06-18T13:40:02.610967Z",
     "iopub.status.idle": "2024-06-18T13:40:58.875437Z",
     "shell.execute_reply.started": "2024-06-18T13:40:02.610930Z",
     "shell.execute_reply": "2024-06-18T13:40:58.874550Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1b2b1e34af04936a8b054b89e1ae4af"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f13777de95f341658bc1a32533268aa9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/280M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19bdd9cd12f04f439f3807c7f07294ba"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/265M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0ae52b71f114538beebf249f7896d94"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/273M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "123a2661eeae49a49f45c3de379cff5d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/474k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e7f8446b9024d97adfe252cb97a0a2f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/509k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f480b1e00a048579b74fcef0401365d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26a5b96f58084e3a9fac43b0558f4dc7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/4508785 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba2acb2ee503410982a46cedcd7cb4e7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b078781b92b4cf09906f2f1811803d8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/3003 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78e9d08d715149b4820b6f73efa73d4d"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 4,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['translation'],\n        num_rows: 4508785\n    })\n    validation: Dataset({\n        features: ['translation'],\n        num_rows: 3000\n    })\n    test: Dataset({\n        features: ['translation'],\n        num_rows: 3003\n    })\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Tokenization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name=\"google/mt5-large\"\n",
    "prompt=\"My name is Kaggle, nice to see you.\"\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name, load_in_8bit=True, device_map=\"auto\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:40:58.876642Z",
     "iopub.execute_input": "2024-06-18T13:40:58.877002Z",
     "iopub.status.idle": "2024-06-18T13:41:07.139316Z",
     "shell.execute_reply.started": "2024-06-18T13:40:58.876967Z",
     "shell.execute_reply": "2024-06-18T13:41:07.138511Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0eb7dfd838794ee695780515293faacc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b56eeb61346a4041a49bebc4aea5cf17"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52271108866845248ee836e881e71d22"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae4184a3f0a34b878fcbf020a6522ad9"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# use a sample of around 2000 instead of the complete dataset as training dataset\n",
    "train_dataset=dataset['train'].shuffle(seed=42).select(range(5000))\n",
    "\n",
    "# as evaluation dataset\n",
    "eval_dataset=dataset['validation']\n",
    "\n",
    "prefix = \"translate English to German: \"\n",
    "def preprocess_func(data):\n",
    "    inputs = [prefix + ex['en'] for ex in data['translation']]\n",
    "    targets = [ex['de'] for ex in data['translation']]\n",
    "    \n",
    "    # Tokenize each row of inputs and outputs with padding\n",
    "    model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    labels = tokenizer(targets, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "# We tokenize the entire dataset\n",
    "train_dataset=train_dataset.map(preprocess_func, batched=True)\n",
    "eval_dataset=eval_dataset.map(preprocess_func, batched=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T14:29:36.786138Z",
     "iopub.execute_input": "2024-06-18T14:29:36.786498Z",
     "iopub.status.idle": "2024-06-18T14:29:36.930408Z",
     "shell.execute_reply.started": "2024-06-18T14:29:36.786471Z",
     "shell.execute_reply": "2024-06-18T14:29:36.929339Z"
    },
    "trusted": true
   },
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing the model\n",
    "\n",
    "First, we will load the model with 8 bit(quantization the model). And then, we are using the LoRA. Here are the description of the LoraConfig:\n",
    "\n",
    "* **r**: the rank of the update matrices, expressed in **int**. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "* **target_modules**: **The modules(for example, attention blocks)** to apply the LoRA update matrices.\n",
    "* **alpha**: LoRA scaling factor\n",
    "* **bias**: Specifies if the bias parameters should be trained. Can be 'none','all' or 'lora_only'.\n",
    "* **module_to_save**: List of modules apart from LoRA layers to be set as trainable and saved in the final checkpoint. These typically include model's custome head that is randomly initialized for the fine-tuning task.\n",
    "* **layer_to_transform**: List of layers to be transformed by LoRA. If not specified, all layers in `target_modules` are transformed.\n",
    "* **layers_pattern**: Pattern to match layer names in `target_modules`, if `layer_to_transform` is specified. By default `PeftModel` will look at common layer pattern(`layers`,`h`, `blocks`, etc.), use it for exotic and custom models.\n",
    "* **rank_pattern**: The mapping from layer names or regexp expression to ranks which are different from the default tank specified by `r`.\n",
    "* **alpha_pattern**: The mapping from layer names or regexp expression to alphas which are different from the default alpha specified by `lora_alpha`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import PeftModel, prepare_model_for_kbit_training, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "model=AutoModelForSeq2SeqLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:51:05.158030Z",
     "iopub.execute_input": "2024-06-18T13:51:05.158883Z",
     "iopub.status.idle": "2024-06-18T13:51:08.791416Z",
     "shell.execute_reply.started": "2024-06-18T13:51:05.158847Z",
     "shell.execute_reply": "2024-06-18T13:51:08.789758Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 9\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoModelForSeq2SeqLM\n\u001B[1;32m      5\u001B[0m bnb_config\u001B[38;5;241m=\u001B[39mBitsAndBytesConfig(\n\u001B[1;32m      6\u001B[0m     load_in_8bit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m      7\u001B[0m )\n\u001B[0;32m----> 9\u001B[0m model\u001B[38;5;241m=\u001B[39m\u001B[43mAutoModelForSeq2SeqLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbnb_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    564\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    565\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 566\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    568\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    569\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    570\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    571\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    572\u001B[0m )\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3646\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3642\u001B[0m         device_map_without_lm_head \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m   3643\u001B[0m             key: device_map[key] \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m device_map\u001B[38;5;241m.\u001B[39mkeys() \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m modules_to_not_convert\n\u001B[1;32m   3644\u001B[0m         }\n\u001B[1;32m   3645\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map_without_lm_head\u001B[38;5;241m.\u001B[39mvalues() \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisk\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map_without_lm_head\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[0;32m-> 3646\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   3647\u001B[0m \u001B[38;5;250m                \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3648\u001B[0m \u001B[38;5;124;03m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001B[39;00m\n\u001B[1;32m   3649\u001B[0m \u001B[38;5;124;03m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001B[39;00m\n\u001B[1;32m   3650\u001B[0m \u001B[38;5;124;03m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001B[39;00m\n\u001B[1;32m   3651\u001B[0m \u001B[38;5;124;03m                `device_map` to `from_pretrained`. Check\u001B[39;00m\n\u001B[1;32m   3652\u001B[0m \u001B[38;5;124;03m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001B[39;00m\n\u001B[1;32m   3653\u001B[0m \u001B[38;5;124;03m                for more details.\u001B[39;00m\n\u001B[1;32m   3654\u001B[0m \u001B[38;5;124;03m                \"\"\"\u001B[39;00m\n\u001B[1;32m   3655\u001B[0m             )\n\u001B[1;32m   3656\u001B[0m         \u001B[38;5;28;01mdel\u001B[39;00m device_map_without_lm_head\n\u001B[1;32m   3658\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m device_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mValueError\u001B[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ],
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Freeze the original parameters\n",
    "model=prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config=LoraConfig(\n",
    "    # the task to train for (sequence-to-sequence language modeling in this case)\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    # the dimension of the low-rank matrices\n",
    "    r=4,\n",
    "    # the scaling factor for the low-rank matrices\n",
    "    lora_alpha=16,\n",
    "    # the dropout probability of the LoRA layers\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=[\"k\",\"q\",\"v\",\"o\"],\n",
    ")\n",
    "\n",
    "peft_model=get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:51:08.792123Z",
     "iopub.status.idle": "2024-06-18T13:51:08.792502Z",
     "shell.execute_reply.started": "2024-06-18T13:51:08.792322Z",
     "shell.execute_reply": "2024-06-18T13:51:08.792340Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy=evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels=p\n",
    "    predictions=np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:43:09.070724Z",
     "iopub.execute_input": "2024-06-18T13:43:09.071442Z",
     "iopub.status.idle": "2024-06-18T13:43:21.559945Z",
     "shell.execute_reply.started": "2024-06-18T13:43:09.071406Z",
     "shell.execute_reply": "2024-06-18T13:43:21.559036Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9fe3ea53e694c8f8d90d98064b4fd43"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ttraining"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# ignore tokenizer pad token in the loss\n",
    "label_pad_token_id=-100\n",
    "\n",
    "# padding the sentence of the entire datasets\n",
    "data_collator=DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=peft_model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:51:15.400434Z",
     "iopub.execute_input": "2024-06-18T13:51:15.400803Z",
     "iopub.status.idle": "2024-06-18T13:51:15.407455Z",
     "shell.execute_reply.started": "2024-06-18T13:51:15.400774Z",
     "shell.execute_reply": "2024-06-18T13:51:15.406304Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "training_args=Seq2SeqTrainingArguments(\n",
    "    output_dir=os.getenv(\"WANDB_NAME\"),\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=1,\n",
    "    logging_dir=os.getenv(\"WANDB_NAME\")+\"/logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=os.getenv(\"WANDB_NAME\"),\n",
    ")\n",
    "\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer=Seq2SeqTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache=False\n",
    "trainer.train()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:51:15.895060Z",
     "iopub.execute_input": "2024-06-18T13:51:15.895759Z",
     "iopub.status.idle": "2024-06-18T14:06:30.407327Z",
     "shell.execute_reply.started": "2024-06-18T13:51:15.895720Z",
     "shell.execute_reply": "2024-06-18T14:06:30.406421Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [313/313 15:08, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>313</td>\n      <td>2.186900</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    },
    {
     "execution_count": 20,
     "output_type": "execute_result",
     "data": {
      "text/plain": "TrainOutput(global_step=313, training_loss=2.186900958466454, metrics={'train_runtime': 911.2216, 'train_samples_per_second': 5.487, 'train_steps_per_second': 0.343, 'total_flos': 3747167600640000.0, 'train_loss': 2.186900958466454, 'epoch': 1.0})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_metric\n",
    "bleu = load_metric(\"bleu\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T14:08:29.930230Z",
     "iopub.execute_input": "2024-06-18T14:08:29.930634Z",
     "iopub.status.idle": "2024-06-18T14:08:30.721967Z",
     "shell.execute_reply.started": "2024-06-18T14:08:29.930604Z",
     "shell.execute_reply": "2024-06-18T14:08:30.720777Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "text": "/tmp/ipykernel_43/718894183.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  bleu = load_metric(\"bleu\")\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/2.48k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed21a1f70dee4b5a9ce2897895a3f0f4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46d45dab1d864eb789129fe42aa3183d"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T14:07:15.671101Z",
     "iopub.execute_input": "2024-06-18T14:07:15.671954Z",
     "iopub.status.idle": "2024-06-18T14:07:15.677656Z",
     "shell.execute_reply.started": "2024-06-18T14:07:15.671920Z",
     "shell.execute_reply": "2024-06-18T14:07:15.676600Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "del train_dataset\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T14:29:50.831124Z",
     "iopub.execute_input": "2024-06-18T14:29:50.831472Z",
     "iopub.status.idle": "2024-06-18T14:29:50.837582Z",
     "shell.execute_reply.started": "2024-06-18T14:29:50.831447Z",
     "shell.execute_reply": "2024-06-18T14:29:50.836327Z"
    },
    "trusted": true
   },
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T14:29:52.169672Z",
     "iopub.execute_input": "2024-06-18T14:29:52.170035Z",
     "iopub.status.idle": "2024-06-18T14:29:52.644601Z",
     "shell.execute_reply.started": "2024-06-18T14:29:52.170006Z",
     "shell.execute_reply": "2024-06-18T14:29:52.643554Z"
    },
    "trusted": true
   },
   "execution_count": 49,
   "outputs": [
    {
     "execution_count": 49,
     "output_type": "execute_result",
     "data": {
      "text/plain": "559"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([torch.tensor(example['input_ids']) for example in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(example['attention_mask']) for example in batch])\n",
    "    labels = torch.stack([torch.tensor(example['labels']) for example in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(eval_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T14:29:53.613113Z",
     "iopub.execute_input": "2024-06-18T14:29:53.613490Z",
     "iopub.status.idle": "2024-06-18T14:29:53.621681Z",
     "shell.execute_reply.started": "2024-06-18T14:29:53.613458Z",
     "shell.execute_reply": "2024-06-18T14:29:53.620758Z"
    },
    "trusted": true
   },
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "[[i] for i in translations]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T14:43:33.961702Z",
     "iopub.execute_input": "2024-06-18T14:43:33.962090Z",
     "iopub.status.idle": "2024-06-18T14:43:34.009843Z",
     "shell.execute_reply.started": "2024-06-18T14:43:33.962058Z",
     "shell.execute_reply": "2024-06-18T14:43:34.008467Z"
    },
    "trusted": true
   },
   "execution_count": 54,
   "outputs": [
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[54], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m [[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtranslations\u001B[49m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'translations' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'translations' is not defined",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "def evaluate_model(model, dataloader, metric):\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    references = []\n",
    "    i = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        if i==1:break\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels = batch['labels'].to(model.device)\n",
    "        \n",
    "        # Generate translation\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=128, num_beams=4, early_stopping=True)\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        reference_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        translations.extend(generated_texts)\n",
    "        references.extend([[ref] for ref in reference_texts])\n",
    "        i += 1\n",
    "    # Compute the BLEU score\n",
    "    print([[i] for i in translations])\n",
    "    print(references)\n",
    "    results = metric.compute(predictions=[[i] for i in translations], references=references)\n",
    "    return results\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_model(model, dataloader, bleu)\n",
    "print(\"BLEU score:\", results[\"score\"])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T14:44:53.029454Z",
     "iopub.execute_input": "2024-06-18T14:44:53.030132Z",
     "iopub.status.idle": "2024-06-18T14:45:08.496782Z",
     "shell.execute_reply.started": "2024-06-18T14:44:53.030097Z",
     "shell.execute_reply": "2024-06-18T14:45:08.495433Z"
    },
    "trusted": true
   },
   "execution_count": 56,
   "outputs": [
    {
     "name": "stderr",
     "text": "  0%|          | 1/375 [00:15<1:34:57, 15.24s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[[\"A Republican Strategie to counter Obama's Reelection.\"], ['Die konservativen Führer begründeten ihre Politik mit der Begründung, dass sie ihre Politik mit dem Ziel der Bekämpfung von Electoralfraud unterstützen.'], ['Auch der Brennan Centre hält dieses als Mythos an, dass electoral fraud ist seltener in den USA als in den USA.'], ['Die republikanischen Advokaten identifizierten nur 300 Fälle von politischer Verfälschung in den USA in einem Jahr.'], ['Eine Sache ist sicher: Diese neuen Richtlinien haben einen negativen Einfluss auf die Wahlentscheidungen.'], ['Ich denke, dass die Maßnahmen Teile des Amerikanischen Demokratiesystems zerstören.'], ['Die Amerikanische Staaten sind verantwortlich für die Organisation der Bundeswahlen in den Vereinigten Staaten.'], ['Das ist in diesem Sinne, dass die meisten amerikanischen Regierungen seit 2009 neue Gesetze verabschiedet haben, die das Registrieren oder Voten erschweren.']]\n[['Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten'], ['Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.'], ['Allerdings hält das Brennan Center letzteres für einen Mythos, indem es bekräftigt, dass der Wahlbetrug in den USA seltener ist als die Anzahl der vom Blitzschlag getöteten Menschen.'], ['Die Rechtsanwälte der Republikaner haben in 10 Jahren in den USA übrigens nur 300 Fälle von Wahlbetrug verzeichnet.'], ['Eins ist sicher: diese neuen Bestimmungen werden sich negativ auf die Wahlbeteiligung auswirken.'], ['In diesem Sinne untergraben diese Maßnahmen teilweise das demokratische System der USA.'], ['Im Gegensatz zu Kanada sind die US-Bundesstaaten für die Durchführung der Wahlen in den einzelnen Staaten verantwortlich.'], ['In diesem Sinne hat die Mehrheit der amerikanischen Regierungen seit 2009 neue Gesetze verkündet, die das Verfahren für die Registrierung oder den Urnengang erschweren.']]\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[56], line 31\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbleu\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBLEU score:\u001B[39m\u001B[38;5;124m\"\u001B[39m, results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "Cell \u001B[0;32mIn[56], line 27\u001B[0m, in \u001B[0;36mevaluate_model\u001B[0;34m(model, dataloader, metric)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m([[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m translations])\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(references)\n\u001B[0;32m---> 27\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mmetric\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtranslations\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreferences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreferences\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m results\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/datasets/metric.py:442\u001B[0m, in \u001B[0;36mMetric.compute\u001B[0;34m(self, predictions, references, **kwargs)\u001B[0m\n\u001B[1;32m    439\u001B[0m compute_kwargs \u001B[38;5;241m=\u001B[39m {k: kwargs[k] \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m kwargs \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures}\n\u001B[1;32m    441\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(v \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[0;32m--> 442\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_finalize()\n\u001B[1;32m    445\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_file_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/datasets/metric.py:494\u001B[0m, in \u001B[0;36mMetric.add_batch\u001B[0;34m(self, predictions, references, **kwargs)\u001B[0m\n\u001B[1;32m    492\u001B[0m batch \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredictions\u001B[39m\u001B[38;5;124m\"\u001B[39m: predictions, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreferences\u001B[39m\u001B[38;5;124m\"\u001B[39m: references, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[1;32m    493\u001B[0m batch \u001B[38;5;241m=\u001B[39m {intput_name: batch[intput_name] \u001B[38;5;28;01mfor\u001B[39;00m intput_name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures}\n\u001B[0;32m--> 494\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minfo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    496\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_writer()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/datasets/features/features.py:1900\u001B[0m, in \u001B[0;36mFeatures.encode_batch\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m   1898\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, column \u001B[38;5;129;01min\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m   1899\u001B[0m     column \u001B[38;5;241m=\u001B[39m cast_to_python_objects(column)\n\u001B[0;32m-> 1900\u001B[0m     encoded_batch[key] \u001B[38;5;241m=\u001B[39m [encode_nested_example(\u001B[38;5;28mself\u001B[39m[key], obj) \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m column]\n\u001B[1;32m   1901\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m encoded_batch\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/datasets/features/features.py:1900\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1898\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, column \u001B[38;5;129;01min\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m   1899\u001B[0m     column \u001B[38;5;241m=\u001B[39m cast_to_python_objects(column)\n\u001B[0;32m-> 1900\u001B[0m     encoded_batch[key] \u001B[38;5;241m=\u001B[39m [\u001B[43mencode_nested_example\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m column]\n\u001B[1;32m   1901\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m encoded_batch\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/datasets/features/features.py:1293\u001B[0m, in \u001B[0;36mencode_nested_example\u001B[0;34m(schema, obj, level)\u001B[0m\n\u001B[1;32m   1288\u001B[0m             \u001B[38;5;66;03m# be careful when comparing tensors here\u001B[39;00m\n\u001B[1;32m   1289\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1290\u001B[0m                 \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(first_elmt, \u001B[38;5;28mlist\u001B[39m)\n\u001B[1;32m   1291\u001B[0m                 \u001B[38;5;129;01mor\u001B[39;00m encode_nested_example(schema\u001B[38;5;241m.\u001B[39mfeature, first_elmt, level\u001B[38;5;241m=\u001B[39mlevel \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m!=\u001B[39m first_elmt\n\u001B[1;32m   1292\u001B[0m             ):\n\u001B[0;32m-> 1293\u001B[0m                 \u001B[38;5;28;01mreturn\u001B[39;00m [encode_nested_example(schema\u001B[38;5;241m.\u001B[39mfeature, o, level\u001B[38;5;241m=\u001B[39mlevel \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m o \u001B[38;5;129;01min\u001B[39;00m obj]\n\u001B[1;32m   1294\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(obj)\n\u001B[1;32m   1295\u001B[0m \u001B[38;5;66;03m# Object with special encoding:\u001B[39;00m\n\u001B[1;32m   1296\u001B[0m \u001B[38;5;66;03m# ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/datasets/features/features.py:1293\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1288\u001B[0m             \u001B[38;5;66;03m# be careful when comparing tensors here\u001B[39;00m\n\u001B[1;32m   1289\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1290\u001B[0m                 \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(first_elmt, \u001B[38;5;28mlist\u001B[39m)\n\u001B[1;32m   1291\u001B[0m                 \u001B[38;5;129;01mor\u001B[39;00m encode_nested_example(schema\u001B[38;5;241m.\u001B[39mfeature, first_elmt, level\u001B[38;5;241m=\u001B[39mlevel \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m!=\u001B[39m first_elmt\n\u001B[1;32m   1292\u001B[0m             ):\n\u001B[0;32m-> 1293\u001B[0m                 \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mencode_nested_example\u001B[49m\u001B[43m(\u001B[49m\u001B[43mschema\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m o \u001B[38;5;129;01min\u001B[39;00m obj]\n\u001B[1;32m   1294\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(obj)\n\u001B[1;32m   1295\u001B[0m \u001B[38;5;66;03m# Object with special encoding:\u001B[39;00m\n\u001B[1;32m   1296\u001B[0m \u001B[38;5;66;03m# ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/datasets/features/features.py:1282\u001B[0m, in \u001B[0;36mencode_nested_example\u001B[0;34m(schema, obj, level)\u001B[0m\n\u001B[1;32m   1280\u001B[0m \u001B[38;5;66;03m# schema.feature is not a dict\u001B[39;00m\n\u001B[1;32m   1281\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, \u001B[38;5;28mstr\u001B[39m):  \u001B[38;5;66;03m# don't interpret a string as a list\u001B[39;00m\n\u001B[0;32m-> 1282\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGot a string but expected a list instead: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mobj\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1283\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1284\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(obj) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[0;31mValueError\u001B[0m: Got a string but expected a list instead: 'Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten'"
     ],
     "ename": "ValueError",
     "evalue": "Got a string but expected a list instead: 'Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten'",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# trainer.evaluate() # out of GPU memory"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:47:30.114432Z",
     "iopub.execute_input": "2024-06-18T05:47:30.114884Z",
     "iopub.status.idle": "2024-06-18T05:48:09.622385Z",
     "shell.execute_reply.started": "2024-06-18T05:47:30.114849Z",
     "shell.execute_reply": "2024-06-18T05:48:09.619249Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# out of GPU memory\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:166\u001B[0m, in \u001B[0;36mSeq2SeqTrainer.evaluate\u001B[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgather_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mgather\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gen_kwargs \u001B[38;5;241m=\u001B[39m gen_kwargs\n\u001B[0;32m--> 166\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3019\u001B[0m, in \u001B[0;36mTrainer.evaluate\u001B[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   3016\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m   3018\u001B[0m eval_loop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprediction_loop \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39muse_legacy_prediction_loop \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation_loop\n\u001B[0;32m-> 3019\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43meval_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3020\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3021\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdescription\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEvaluation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3022\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001B[39;49;00m\n\u001B[1;32m   3023\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# self.args.prediction_loss_only\u001B[39;49;00m\n\u001B[1;32m   3024\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprediction_loss_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   3025\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3026\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3027\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3029\u001B[0m total_batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39meval_batch_size \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mworld_size\n\u001B[1;32m   3030\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric_key_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_jit_compilation_time\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m output\u001B[38;5;241m.\u001B[39mmetrics:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3234\u001B[0m, in \u001B[0;36mTrainer.evaluation_loop\u001B[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   3232\u001B[0m         logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess_logits_for_metrics(logits, labels)\n\u001B[1;32m   3233\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgather_function((logits))\n\u001B[0;32m-> 3234\u001B[0m     preds_host \u001B[38;5;241m=\u001B[39m logits \u001B[38;5;28;01mif\u001B[39;00m preds_host \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mnested_concat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds_host\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3236\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3237\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgather_function((labels))\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:121\u001B[0m, in \u001B[0;36mnested_concat\u001B[0;34m(tensors, new_tensors, padding_index)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(tensors) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mtype\u001B[39m(\n\u001B[1;32m    118\u001B[0m     new_tensors\n\u001B[1;32m    119\u001B[0m ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(tensors)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(new_tensors)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m--> 121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnested_concat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_index\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_tensors\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001B[38;5;241m=\u001B[39mpadding_index)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:121\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(tensors) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mtype\u001B[39m(\n\u001B[1;32m    118\u001B[0m     new_tensors\n\u001B[1;32m    119\u001B[0m ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(tensors)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(new_tensors)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m--> 121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(tensors)(\u001B[43mnested_concat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_index\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m t, n \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(tensors, new_tensors))\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001B[38;5;241m=\u001B[39mpadding_index)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:123\u001B[0m, in \u001B[0;36mnested_concat\u001B[0;34m(tensors, new_tensors, padding_index)\u001B[0m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(tensors)(nested_concat(t, n, padding_index\u001B[38;5;241m=\u001B[39mpadding_index) \u001B[38;5;28;01mfor\u001B[39;00m t, n \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(tensors, new_tensors))\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m--> 123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch_pad_and_concatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_tensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, Mapping):\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(tensors)(\n\u001B[1;32m    126\u001B[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001B[38;5;241m=\u001B[39mpadding_index) \u001B[38;5;28;01mfor\u001B[39;00m k, t \u001B[38;5;129;01min\u001B[39;00m tensors\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[1;32m    127\u001B[0m     )\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:88\u001B[0m, in \u001B[0;36mtorch_pad_and_concatenate\u001B[0;34m(tensor1, tensor2, padding_index)\u001B[0m\n\u001B[1;32m     85\u001B[0m new_shape \u001B[38;5;241m=\u001B[39m (tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m+\u001B[39m tensor2\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mmax\u001B[39m(tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], tensor2\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])) \u001B[38;5;241m+\u001B[39m tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m:]\n\u001B[1;32m     87\u001B[0m \u001B[38;5;66;03m# Now let's fill the result tensor\u001B[39;00m\n\u001B[0;32m---> 88\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mtensor1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnew_full\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m result[: tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], : tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]] \u001B[38;5;241m=\u001B[39m tensor1\n\u001B[1;32m     90\u001B[0m result[tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] :, : tensor2\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]] \u001B[38;5;241m=\u001B[39m tensor2\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 2.82 GiB (GPU 0; 15.89 GiB total capacity; 10.94 GiB already allocated; 2.83 GiB free; 12.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ],
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.82 GiB (GPU 0; 15.89 GiB total capacity; 10.94 GiB already allocated; 2.83 GiB free; 12.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "kwargs={\n",
    "    'model_name': f'{os.getenv(\"WANDB_NAME\")}',\n",
    "    'finetuned_from': model_name,\n",
    "    'tasks': 'Translation',\n",
    "#     'dataset_tags':'',\n",
    "    'dataset':'opus100'\n",
    "}\n",
    "\n",
    "tokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))\n",
    "trainer.push_to_hub(**kwargs)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:04:04.179122Z",
     "iopub.status.idle": "2024-06-18T05:04:04.179427Z",
     "shell.execute_reply.started": "2024-06-18T05:04:04.179268Z",
     "shell.execute_reply": "2024-06-18T05:04:04.179282Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "peft_model.config.use_cache=True\n",
    "context=tokenizer([\"Hello\"], return_tensors=\"pt\")\n",
    "output=peft_model.generate(**context)\n",
    "\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T06:24:13.111362Z",
     "iopub.execute_input": "2024-06-18T06:24:13.111730Z",
     "iopub.status.idle": "2024-06-18T06:24:17.278900Z",
     "shell.execute_reply.started": "2024-06-18T06:24:13.111699Z",
     "shell.execute_reply": "2024-06-18T06:24:17.277865Z"
    },
    "trusted": true
   },
   "execution_count": 34,
   "outputs": [
    {
     "execution_count": 34,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'- Vous ne êtes pas en dehors de la ville '"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "model = PeftModel.from_pretrained(model, \"aisuko/ft-t5-small-on-opus100\")\n",
    "\n",
    "output=model.generate(**context)\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:04:04.184449Z",
     "iopub.status.idle": "2024-06-18T05:04:04.184844Z",
     "shell.execute_reply.started": "2024-06-18T05:04:04.184647Z",
     "shell.execute_reply": "2024-06-18T05:04:04.184664Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References List\n",
    "\n",
    "* https://towardsdev.com/fine-tune-quantized-language-model-using-lora-with-peft-transformers-on-t4-gpu-287da2d5d7f1\n",
    "* https://huggingface.co/docs/peft/quicktour"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
