{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INCOMPLETE !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A # to make holes \n",
    "from torchvision.transforms import v2 # for other transforms \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "from PIL import Image \n",
    "\n",
    "seed = 10 \n",
    "torch.manual_seed(seed) \n",
    "torch.cuda.manual_seed(seed) \n",
    "torch.cuda.manual_seed_all(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cut_tfm = A.CoarseDropout(num_holes_range=(1,3), p=1)\n",
    "tv_tfms = v2.Compose([\n",
    "    v2.ToTensor(), \n",
    "    v2.ToDtype(torch.float32), \n",
    "    v2.RandomHorizontalFlip(p=0.5), \n",
    "    v2.RandomAffine(degrees=(-10, 10), scale=(0.9, 1.1))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tv_tfms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MNIST \n\u001b[1;32m----> 3\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m MNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, target_transform\u001b[38;5;241m=\u001b[39m\u001b[43mtv_tfms\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_preprocess\u001b[39m(torch_img, device\u001b[38;5;241m=\u001b[39mdevice): \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(cut_tfm(image\u001b[38;5;241m=\u001b[39mtorch_img\u001b[38;5;241m.\u001b[39mnumpy())[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'tv_tfms' is not defined"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST \n",
    "\n",
    "train_ds = MNIST('./data/', train=True, download=True, target_transform=tv_tfms)\n",
    "\n",
    "def train_preprocess(torch_img, device=device): \n",
    "    return torch.tensor(cut_tfm(image=torch_img.numpy())['image'], device=device) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conv, then deconv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageVAE_enc(nn.Module): \n",
    "    def __init__(self, d=32, hidden_layer_size=100):\n",
    "        super(ImageVAE_enc, self).__init__()\n",
    "\n",
    "        # conv (1 input channel) to a small layer inside \n",
    "        # 28x28 (MNIST dataset)\n",
    "        self.conv1 = nn.Conv2d(1, d, 4, 2, 3, dtype=torch.float32) # 16x16 \n",
    "        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1, dtype=torch.float32) # 8x8 \n",
    "        self.conv2_bn = nn.BatchNorm2d(d*2)\n",
    "        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1, dtype=torch.float32) # 4x4 \n",
    "        self.conv3_bn = nn.BatchNorm2d(d*4)\n",
    "        self.conv4 = nn.Conv2d(d*4, hidden_layer_size, 4, 1, 0, dtype=torch.float32) # 1x1 \n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = F.leaky_relu(self.conv1(input), 0.2)\n",
    "        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "\n",
    "        return x\n",
    "        \n",
    "\n",
    "class ImageVAE_dec(nn.Module):\n",
    "    def __init__(self, d=32, hidden_layer_size=100):\n",
    "        super(ImageVAE_dec, self).__init__()\n",
    "\n",
    "        # deconv back to normal image size \n",
    "        # starts as 1x1 img with hidden_layer_size channels \n",
    "        self.deconv1 = nn.ConvTranspose2d(hidden_layer_size, d*4, 4, 1, 0, dtype=torch.float32) # 4x4 \n",
    "        self.deconv1_bn = nn.BatchNorm2d(d*4)\n",
    "        self.deconv2 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1, dtype=torch.float32) # 8x8 \n",
    "        self.deconv2_bn = nn.BatchNorm2d(d*2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(d*2, d, 4, 2, 1, dtype=torch.float32) # 16x16 \n",
    "        self.deconv3_bn = nn.BatchNorm2d(d)\n",
    "        self.deconv4 = nn.ConvTranspose2d(d, 1, 4, 2, 3, dtype=torch.float32) # 28x28 \n",
    "\n",
    "        # NOTE: in deconv, the kernel size is important, https://distill.pub/2016/deconv-checkerboard/ \n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        x = F.relu(self.deconv1_bn(self.deconv1(input)))\n",
    "        x = F.relu(self.deconv2_bn(self.deconv2(x)))\n",
    "        x = F.relu(self.deconv3_bn(self.deconv3(x)))\n",
    "        x = torch.tanh(self.deconv4(x))\n",
    "\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = ImageVAE_enc().to(device) \n",
    "dec = ImageVAE_dec().to(device) \n",
    "\n",
    "\n",
    "# training loop \n",
    "epoch = 1 \n",
    "batch_size = 25\n",
    "loss_fn = nn.MSELoss() \n",
    "optimizer = torch.optim.Adam([enc.parameters(), dec.parameters()], lr=1e-4) \n",
    "\n",
    "prev_avg_loss = 10000000 \n",
    "all_avg_losses = [] \n",
    "while True: \n",
    "    epoch_losses = [] \n",
    "    for b in range(0, len(train_ds), batch_size): \n",
    "        imgs = train_ds[b:b+batch_size] \n",
    "        imgs = train_preprocess(imgs) \n",
    "        \n",
    "        emb = enc(imgs) \n",
    "        rec = dec(emb) \n",
    "        # compare reconstruction loss \n",
    "        loss = loss_fn(imgs, rec) \n",
    "        \n",
    "        epoch_losses.append(loss.item()) \n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad() \n",
    "\n",
    "    epoch_avg_loss = sum(epoch_losses)/len(epoch_losses) \n",
    "    print(\"EPOCH {} AVG LOSS: {}\".format(epoch, epoch_avg_loss))\n",
    "\n",
    "    all_avg_losses.append(epoch_avg_loss) \n",
    "    if epoch_avg_loss > 1.05*prev_avg_loss: \n",
    "        # assume it's converged already so cut \n",
    "        break \n",
    "\n",
    "    prev_avg_loss = epoch_avg_loss \n",
    "\n",
    "    epoch += 1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(list(range(1, 1+len(all_avg_losses))), all_avg_losses) \n",
    "plt.xlabel('Epoch') \n",
    "plt.ylabel(\"MSELoss\") \n",
    "\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
