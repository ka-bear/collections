{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of this is kind of blatant copying... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also my tracking in this example is super scuffed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import pathlib \n",
    "from collections import defaultdict \n",
    "from typing import Dict, Union, List, Optional, Tuple, Generator, Any \n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.engine.results import Results, Boxes\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Generator, List, Optional, Tuple\n",
    "\n",
    "import tqdm\n",
    "import datetime \n",
    "\n",
    "\n",
    "# because of ultralytics bug it is important to unset CUBLAS_WORKSPACE_CONFIG after the module importing\n",
    "os.unsetenv(\"CUBLAS_WORKSPACE_CONFIG\")\n",
    "\n",
    "\n",
    "import logging \n",
    "\n",
    "import timm\n",
    "\n",
    "# register new models\n",
    "from .mivolo_model import *  # noqa: F403, F401\n",
    "from timm.layers import set_layer_config\n",
    "from timm.models._factory import parse_model_name\n",
    "from timm.models._helpers import load_state_dict, remap_checkpoint\n",
    "from timm.models._hub import load_model_config_from_hf\n",
    "from timm.models._pretrained import PretrainedCfg, split_model_name_tag\n",
    "from timm.models._registry import is_model, model_entrypoint\n",
    "from timm.data import resolve_data_config, IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "from imutils import VideoStream "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/WildChlamydia/MiVOLO/blob/main/mivolo/model/yolo_detector.py\n",
    "\n",
    "class Detector:\n",
    "    def __init__(\n",
    "        self,\n",
    "        weightss: str,\n",
    "        device: str = \"cuda\",\n",
    "        half: bool = True,\n",
    "        verbose: bool = False,\n",
    "        conf_thresh: float = 0.4,\n",
    "        iou_thresh: float = 0.7,\n",
    "    ):\n",
    "        self.face = YOLO(weightss[0]) \n",
    "        self.face.fuse() \n",
    "        self.person = YOLO(weightss[1]) \n",
    "        self.person.fuse() \n",
    "\n",
    "        self.device = torch.device(device)\n",
    "        self.half = half and self.device.type != \"cpu\"\n",
    "\n",
    "        if self.half:\n",
    "            self.face.model = self.face.model.half()\n",
    "            self.person.model = self.person.model.half() \n",
    "\n",
    "        self.face_detector_names: Dict[int, str] = self.face.model.names\n",
    "        self.person_detector_names: Dict[int, str] = self.person.model.names\n",
    "\n",
    "\n",
    "        # init yolo.predictor\n",
    "        self.detector_kwargs = {\"conf\": conf_thresh, \"iou\": iou_thresh, \"half\": self.half, \"verbose\": verbose}\n",
    "        # self.yolo.predict(**self.detector_kwargs)\n",
    "\n",
    "    def predict(self, image: Union[np.ndarray, str, \"PIL.Image\"]):\n",
    "        results: Results = self.yolo.predict(image, **self.detector_kwargs)[0]\n",
    "        return results \n",
    "\n",
    "    def track(self, image: Union[np.ndarray, str, \"PIL.Image\"]):\n",
    "        face_results: Results = self.face.track(image, persist=True, **self.detector_kwargs)[0]\n",
    "        person_results: Results = self.person.track(image, persist=True, **self.detector_kwargs)[0]\n",
    "        return face_results, person_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EntranceLine: \n",
    "    def __init__(self, a, b): \n",
    "        if isinstance(a, (float, int)): # m and c \n",
    "            self.m = a \n",
    "            self.c = b \n",
    "        else: # pt1 and pt2 \n",
    "            m, c = pts_to_mc(a, b) \n",
    "            self.m = m \n",
    "            self.c = c \n",
    "    \n",
    "    def above(self, x, y): # above means y < line_y \n",
    "        line_y = self.m*x + self.c \n",
    "        return (y < line_y) \n",
    "\n",
    "    def below(self, x, y): # below means y > line_y \n",
    "        line_y = self.m*x + self.c \n",
    "        return (y > line_y) \n",
    "\n",
    "    def entered(self, entrance_condition, x, y): \n",
    "        if entrance_condition == EntranceCondition.BELOW: \n",
    "            return self.below(x, y) \n",
    "        elif entrance_condition == EntranceCondition.ABOVE: \n",
    "            return self.above(x, y) \n",
    "        raise ValueError(\"entrance_condition must be either EntranceCondition.BELOW or EntranceCondition.ABOVE\") \n",
    "        \n",
    "\n",
    "''' # attempt at making EntranceCondition.ABOVE's type EntranceCondition: \n",
    "class EntranceCondition: \n",
    "    def __init__(self, above): \n",
    "        self.above = above \n",
    "\n",
    "    BELOW = exec('EntranceCondition(0)') \n",
    "    ABOVE = exec('EntranceCondition(1)') \n",
    "'''\n",
    "\n",
    "class EntranceCondition: \n",
    "    BELOW = 0 \n",
    "    ABOVE = 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/WildChlamydia/MiVOLO/blob/main/mivolo/structures.py and https://github.com/WildChlamydia/MiVOLO/blob/main/mivolo/data/misc.py \n",
    "\n",
    "def aggregate_votes_winsorized(ages, max_age_dist=6):\n",
    "    # Replace any annotation that is more than a max_age_dist away from the median\n",
    "    # with the median + max_age_dist if higher or max_age_dist - max_age_dist if below\n",
    "    median = np.median(ages)\n",
    "    ages = np.clip(ages, median - max_age_dist, median + max_age_dist)\n",
    "    return np.mean(ages)\n",
    "\n",
    "def assign_faces(\n",
    "    persons_bboxes: List[torch.tensor], faces_bboxes: List[torch.tensor], iou_thresh: float = 0.0001\n",
    ") -> Tuple[List[Optional[int]], List[int]]:\n",
    "    \"\"\"\n",
    "    Assign person to each face if it is possible.\n",
    "    Return:\n",
    "        - assigned_faces List[Optional[int]]: mapping of face_ind to person_ind\n",
    "                                            ( assigned_faces[face_ind] = person_ind ). person_ind can be None\n",
    "        - unassigned_persons_inds List[int]: persons indexes without any assigned face\n",
    "    \"\"\"\n",
    "\n",
    "    assigned_faces: List[Optional[int]] = [None for _ in range(len(faces_bboxes))]\n",
    "    unassigned_persons_inds: List[int] = [p_ind for p_ind in range(len(persons_bboxes))]\n",
    "\n",
    "    if len(persons_bboxes) == 0 or len(faces_bboxes) == 0:\n",
    "        return assigned_faces, unassigned_persons_inds\n",
    "\n",
    "    cost_matrix = box_iou(torch.stack(persons_bboxes), torch.stack(faces_bboxes), over_second=True).cpu().numpy()\n",
    "    persons_indexes, face_indexes = [], []\n",
    "\n",
    "    if len(cost_matrix) > 0:\n",
    "        persons_indexes, face_indexes = linear_sum_assignment(cost_matrix, maximize=True)\n",
    "\n",
    "    matched_persons = set()\n",
    "    for person_idx, face_idx in zip(persons_indexes, face_indexes):\n",
    "        ciou = cost_matrix[person_idx][face_idx]\n",
    "        if ciou > iou_thresh:\n",
    "            if person_idx in matched_persons:\n",
    "                # Person can not be assigned twice, in reality this should not happen\n",
    "                continue\n",
    "            assigned_faces[face_idx] = person_idx\n",
    "            matched_persons.add(person_idx)\n",
    "\n",
    "    unassigned_persons_inds = [p_ind for p_ind in range(len(persons_bboxes)) if p_ind not in matched_persons]\n",
    "\n",
    "    return assigned_faces, unassigned_persons_inds\n",
    "\n",
    "\n",
    "\n",
    "def box_iou(box1, box2, over_second=False):\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    If over_second == True, return mean(intersection-over-union, (inter / area2))\n",
    "\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    def box_area(box):\n",
    "        # box = 4xn\n",
    "        return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "    area1 = box_area(box1.T)\n",
    "    area2 = box_area(box2.T)\n",
    "\n",
    "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n",
    "\n",
    "    iou = inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)\n",
    "    if over_second:\n",
    "        return (inter / area2 + iou) / 2  # mean(inter / area2, iou)\n",
    "    else:\n",
    "        return iou\n",
    "\n",
    "\n",
    "\n",
    "AGE_GENDER_TYPE = Tuple[float, str]\n",
    "\n",
    "\n",
    "class PersonAndFaceCrops:\n",
    "    def __init__(self):\n",
    "        # int: index of person along results\n",
    "        self.crops_persons: Dict[int, np.ndarray] = {}\n",
    "\n",
    "        # int: index of face along results\n",
    "        self.crops_faces: Dict[int, np.ndarray] = {}\n",
    "\n",
    "        # int: index of face along results\n",
    "        self.crops_faces_wo_body: Dict[int, np.ndarray] = {}\n",
    "\n",
    "        # int: index of person along results\n",
    "        self.crops_persons_wo_face: Dict[int, np.ndarray] = {}\n",
    "\n",
    "    def _add_to_output(\n",
    "        self, crops: Dict[int, np.ndarray], out_crops: List[np.ndarray], out_crop_inds: List[Optional[int]]\n",
    "    ):\n",
    "        inds_to_add = list(crops.keys())\n",
    "        crops_to_add = list(crops.values())\n",
    "        out_crops.extend(crops_to_add)\n",
    "        out_crop_inds.extend(inds_to_add)\n",
    "\n",
    "    def _get_all_faces(\n",
    "        self, use_persons: bool, use_faces: bool\n",
    "    ) -> Tuple[List[Optional[int]], List[Optional[np.ndarray]]]:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "            if use_persons and use_faces\n",
    "                faces: faces_with_bodies + faces_without_bodies + [None] * len(crops_persons_wo_face)\n",
    "            if use_persons and not use_faces\n",
    "                faces: [None] * n_persons\n",
    "            if not use_persons and use_faces:\n",
    "                faces: faces_with_bodies + faces_without_bodies\n",
    "        \"\"\"\n",
    "\n",
    "        def add_none_to_output(faces_inds, faces_crops, num):\n",
    "            faces_inds.extend([None for _ in range(num)])\n",
    "            faces_crops.extend([None for _ in range(num)])\n",
    "\n",
    "        faces_inds: List[Optional[int]] = []\n",
    "        faces_crops: List[Optional[np.ndarray]] = []\n",
    "\n",
    "        if not use_faces:\n",
    "            add_none_to_output(faces_inds, faces_crops, len(self.crops_persons) + len(self.crops_persons_wo_face))\n",
    "            return faces_inds, faces_crops\n",
    "\n",
    "        self._add_to_output(self.crops_faces, faces_crops, faces_inds)\n",
    "        self._add_to_output(self.crops_faces_wo_body, faces_crops, faces_inds)\n",
    "\n",
    "        if use_persons:\n",
    "            add_none_to_output(faces_inds, faces_crops, len(self.crops_persons_wo_face))\n",
    "\n",
    "        return faces_inds, faces_crops\n",
    "\n",
    "    def _get_all_bodies(\n",
    "        self, use_persons: bool, use_faces: bool\n",
    "    ) -> Tuple[List[Optional[int]], List[Optional[np.ndarray]]]:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "            if use_persons and use_faces\n",
    "                persons: bodies_with_faces + [None] * len(faces_without_bodies) + bodies_without_faces\n",
    "            if use_persons and not use_faces\n",
    "                persons: bodies_with_faces + bodies_without_faces\n",
    "            if not use_persons and use_faces\n",
    "                persons: [None] * n_faces\n",
    "        \"\"\"\n",
    "\n",
    "        def add_none_to_output(bodies_inds, bodies_crops, num):\n",
    "            bodies_inds.extend([None for _ in range(num)])\n",
    "            bodies_crops.extend([None for _ in range(num)])\n",
    "\n",
    "        bodies_inds: List[Optional[int]] = []\n",
    "        bodies_crops: List[Optional[np.ndarray]] = []\n",
    "\n",
    "        if not use_persons:\n",
    "            add_none_to_output(bodies_inds, bodies_crops, len(self.crops_faces) + len(self.crops_faces_wo_body))\n",
    "            return bodies_inds, bodies_crops\n",
    "\n",
    "        self._add_to_output(self.crops_persons, bodies_crops, bodies_inds)\n",
    "        if use_faces:\n",
    "            add_none_to_output(bodies_inds, bodies_crops, len(self.crops_faces_wo_body))\n",
    "\n",
    "        self._add_to_output(self.crops_persons_wo_face, bodies_crops, bodies_inds)\n",
    "\n",
    "        return bodies_inds, bodies_crops\n",
    "\n",
    "    def get_faces_with_bodies(self, use_persons: bool, use_faces: bool):\n",
    "        \"\"\"\n",
    "        Return\n",
    "            faces: faces_with_bodies, faces_without_bodies, [None] * len(crops_persons_wo_face)\n",
    "            persons: bodies_with_faces, [None] * len(faces_without_bodies), bodies_without_faces\n",
    "        \"\"\"\n",
    "\n",
    "        bodies_inds, bodies_crops = self._get_all_bodies(use_persons, use_faces)\n",
    "        faces_inds, faces_crops = self._get_all_faces(use_persons, use_faces)\n",
    "\n",
    "        return (bodies_inds, bodies_crops), (faces_inds, faces_crops)\n",
    "\n",
    "    def save(self, out_dir=\"output\"):\n",
    "        ind = 0\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        for crops in [self.crops_persons, self.crops_faces, self.crops_faces_wo_body, self.crops_persons_wo_face]:\n",
    "            for crop in crops.values():\n",
    "                if crop is None:\n",
    "                    continue\n",
    "                out_name = os.path.join(out_dir, f\"{ind}_crop.jpg\")\n",
    "                cv2.imwrite(out_name, crop)\n",
    "                ind += 1\n",
    "\n",
    "\n",
    "class PersonAndFaceResult:\n",
    "    def __init__(self, results: Results):\n",
    "\n",
    "        self.yolo_results = results\n",
    "        names = set(results.names.values())\n",
    "        assert \"person\" in names and \"face\" in names\n",
    "\n",
    "        # initially no faces and persons are associated to each other\n",
    "        self.face_to_person_map: Dict[int, Optional[int]] = {ind: None for ind in self.get_bboxes_inds(\"face\")}\n",
    "        self.unassigned_persons_inds: List[int] = self.get_bboxes_inds(\"person\")\n",
    "        n_objects = len(self.yolo_results.boxes)\n",
    "        self.ages: List[Optional[float]] = [None for _ in range(n_objects)]\n",
    "        self.genders: List[Optional[str]] = [None for _ in range(n_objects)]\n",
    "        self.gender_scores: List[Optional[float]] = [None for _ in range(n_objects)]\n",
    "\n",
    "    @property\n",
    "    def n_objects(self) -> int:\n",
    "        return len(self.yolo_results.boxes)\n",
    "\n",
    "    @property\n",
    "    def n_faces(self) -> int:\n",
    "        return len(self.get_bboxes_inds(\"face\"))\n",
    "\n",
    "    @property\n",
    "    def n_persons(self) -> int:\n",
    "        return len(self.get_bboxes_inds(\"person\"))\n",
    "\n",
    "    def get_bboxes_inds(self, category: str) -> List[int]:\n",
    "        bboxes: List[int] = []\n",
    "        for ind, det in enumerate(self.yolo_results.boxes):\n",
    "            name = self.yolo_results.names[int(det.cls)]\n",
    "            if name == category:\n",
    "                bboxes.append(ind)\n",
    "\n",
    "        return bboxes\n",
    "\n",
    "    def get_distance_to_center(self, bbox_ind: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate euclidian distance between bbox center and image center.\n",
    "        \"\"\"\n",
    "        im_h, im_w = self.yolo_results[bbox_ind].orig_shape\n",
    "        x1, y1, x2, y2 = self.get_bbox_by_ind(bbox_ind).cpu().numpy()\n",
    "        center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "        dist = math.dist([center_x, center_y], [im_w / 2, im_h / 2])\n",
    "        return dist\n",
    "\n",
    "    def plot(\n",
    "        self,\n",
    "        conf=False,\n",
    "        line_width=None,\n",
    "        font_size=None,\n",
    "        font=\"Arial.ttf\",\n",
    "        pil=False,\n",
    "        img=None,\n",
    "        labels=True,\n",
    "        boxes=True,\n",
    "        probs=True,\n",
    "        ages=True,\n",
    "        genders=True,\n",
    "        gender_probs=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots the detection results on an input RGB image. Accepts a numpy array (cv2) or a PIL Image.\n",
    "        Args:\n",
    "            conf (bool): Whether to plot the detection confidence score.\n",
    "            line_width (float, optional): The line width of the bounding boxes. If None, it is scaled to the image size.\n",
    "            font_size (float, optional): The font size of the text. If None, it is scaled to the image size.\n",
    "            font (str): The font to use for the text.\n",
    "            pil (bool): Whether to return the image as a PIL Image.\n",
    "            img (numpy.ndarray): Plot to another image. if not, plot to original image.\n",
    "            labels (bool): Whether to plot the label of bounding boxes.\n",
    "            boxes (bool): Whether to plot the bounding boxes.\n",
    "            probs (bool): Whether to plot classification probability\n",
    "            ages (bool): Whether to plot the age of bounding boxes.\n",
    "            genders (bool): Whether to plot the genders of bounding boxes.\n",
    "            gender_probs (bool): Whether to plot gender classification probability\n",
    "        Returns:\n",
    "            (numpy.ndarray): A numpy array of the annotated image.\n",
    "        \"\"\"\n",
    "\n",
    "        # return self.yolo_results.plot()\n",
    "        colors_by_ind = {}\n",
    "        for face_ind, person_ind in self.face_to_person_map.items():\n",
    "            if person_ind is not None:\n",
    "                colors_by_ind[face_ind] = face_ind + 2\n",
    "                colors_by_ind[person_ind] = face_ind + 2\n",
    "            else:\n",
    "                colors_by_ind[face_ind] = 0\n",
    "        for person_ind in self.unassigned_persons_inds:\n",
    "            colors_by_ind[person_ind] = 1\n",
    "\n",
    "        names = self.yolo_results.names\n",
    "        annotator = Annotator(\n",
    "            deepcopy(self.yolo_results.orig_img if img is None else img),\n",
    "            line_width,\n",
    "            font_size,\n",
    "            font,\n",
    "            pil,\n",
    "            example=names,\n",
    "        )\n",
    "        pred_boxes, show_boxes = self.yolo_results.boxes, boxes\n",
    "        pred_probs, show_probs = self.yolo_results.probs, probs\n",
    "\n",
    "        if pred_boxes and show_boxes:\n",
    "            for bb_ind, (d, age, gender, gender_score) in enumerate(\n",
    "                zip(pred_boxes, self.ages, self.genders, self.gender_scores)\n",
    "            ):\n",
    "                c, conf, guid = int(d.cls), float(d.conf) if conf else None, None if d.id is None else int(d.id.item())\n",
    "                name = (\"\" if guid is None else f\"id:{guid} \") + names[c]\n",
    "                label = (f\"{name} {conf:.2f}\" if conf else name) if labels else None\n",
    "                if ages and age is not None:\n",
    "                    label += f\" {age:.1f}\"\n",
    "                if genders and gender is not None:\n",
    "                    label += f\" {'F' if gender == 'female' else 'M'}\"\n",
    "                if gender_probs and gender_score is not None:\n",
    "                    label += f\" ({gender_score:.1f})\"\n",
    "                annotator.box_label(d.xyxy.squeeze(), label, color=colors(colors_by_ind[bb_ind], True))\n",
    "\n",
    "        if pred_probs is not None and show_probs:\n",
    "            text = f\"{', '.join(f'{names[j] if names else j} {pred_probs.data[j]:.2f}' for j in pred_probs.top5)}, \"\n",
    "            annotator.text((32, 32), text, txt_color=(255, 255, 255))  # TODO: allow setting colors\n",
    "\n",
    "        return annotator.result()\n",
    "\n",
    "    def set_tracked_age_gender(self, tracked_objects: Dict[int, List[AGE_GENDER_TYPE]]):\n",
    "        \"\"\"\n",
    "        Update age and gender for objects based on history from tracked_objects.\n",
    "        Args:\n",
    "            tracked_objects (dict[int, list[AGE_GENDER_TYPE]]): info about tracked objects by guid\n",
    "        \"\"\"\n",
    "\n",
    "        for face_ind, person_ind in self.face_to_person_map.items():\n",
    "            pguid = self._get_id_by_ind(person_ind)\n",
    "            fguid = self._get_id_by_ind(face_ind)\n",
    "\n",
    "            if fguid == -1 and pguid == -1:\n",
    "                # YOLO might not assign ids for some objects in some cases:\n",
    "                # https://github.com/ultralytics/ultralytics/issues/3830\n",
    "                continue\n",
    "            age, gender = self._gather_tracking_result(tracked_objects, fguid, pguid)\n",
    "            if age is None or gender is None:\n",
    "                continue\n",
    "            self.set_age(face_ind, age)\n",
    "            self.set_gender(face_ind, gender, 1.0)\n",
    "            if pguid != -1:\n",
    "                self.set_gender(person_ind, gender, 1.0)\n",
    "                self.set_age(person_ind, age)\n",
    "\n",
    "        for person_ind in self.unassigned_persons_inds:\n",
    "            pid = self._get_id_by_ind(person_ind)\n",
    "            if pid == -1:\n",
    "                continue\n",
    "            age, gender = self._gather_tracking_result(tracked_objects, -1, pid)\n",
    "            if age is None or gender is None:\n",
    "                continue\n",
    "            self.set_gender(person_ind, gender, 1.0)\n",
    "            self.set_age(person_ind, age)\n",
    "\n",
    "    def _get_id_by_ind(self, ind: Optional[int] = None) -> int:\n",
    "        if ind is None:\n",
    "            return -1\n",
    "        obj_id = self.yolo_results.boxes[ind].id\n",
    "        if obj_id is None:\n",
    "            return -1\n",
    "        return obj_id.item()\n",
    "\n",
    "    def get_bbox_by_ind(self, ind: int, im_h: int = None, im_w: int = None) -> torch.tensor:\n",
    "        bb = self.yolo_results.boxes[ind].xyxy.squeeze().type(torch.int32)\n",
    "        if im_h is not None and im_w is not None:\n",
    "            bb[0] = torch.clamp(bb[0], min=0, max=im_w - 1)\n",
    "            bb[1] = torch.clamp(bb[1], min=0, max=im_h - 1)\n",
    "            bb[2] = torch.clamp(bb[2], min=0, max=im_w - 1)\n",
    "            bb[3] = torch.clamp(bb[3], min=0, max=im_h - 1)\n",
    "        return bb\n",
    "\n",
    "    def set_age(self, ind: Optional[int], age: float):\n",
    "        if ind is not None:\n",
    "            self.ages[ind] = age\n",
    "\n",
    "    def set_gender(self, ind: Optional[int], gender: str, gender_score: float):\n",
    "        if ind is not None:\n",
    "            self.genders[ind] = gender\n",
    "            self.gender_scores[ind] = gender_score\n",
    "\n",
    "    @staticmethod\n",
    "    def _gather_tracking_result(\n",
    "        tracked_objects: Dict[int, List[AGE_GENDER_TYPE]],\n",
    "        fguid: int = -1,\n",
    "        pguid: int = -1,\n",
    "        minimum_sample_size: int = 10,\n",
    "    ) -> AGE_GENDER_TYPE:\n",
    "\n",
    "        assert fguid != -1 or pguid != -1, \"Incorrect tracking behaviour\"\n",
    "\n",
    "        face_ages = [r[0] for r in tracked_objects[fguid] if r[0] is not None] if fguid in tracked_objects else []\n",
    "        face_genders = [r[1] for r in tracked_objects[fguid] if r[1] is not None] if fguid in tracked_objects else []\n",
    "        person_ages = [r[0] for r in tracked_objects[pguid] if r[0] is not None] if pguid in tracked_objects else []\n",
    "        person_genders = [r[1] for r in tracked_objects[pguid] if r[1] is not None] if pguid in tracked_objects else []\n",
    "\n",
    "        if not face_ages and not person_ages:  # both empty\n",
    "            return None, None\n",
    "\n",
    "        # You can play here with different aggregation strategies\n",
    "        # Face ages - predictions based on face or face + person, depends on history of object\n",
    "        # Person ages - predictions based on person or face + person, depends on history of object\n",
    "\n",
    "        if len(person_ages + face_ages) >= minimum_sample_size:\n",
    "            age = aggregate_votes_winsorized(person_ages + face_ages)\n",
    "        else:\n",
    "            face_age = np.mean(face_ages) if face_ages else None\n",
    "            person_age = np.mean(person_ages) if person_ages else None\n",
    "            if face_age is None:\n",
    "                face_age = person_age\n",
    "            if person_age is None:\n",
    "                person_age = face_age\n",
    "            age = (face_age + person_age) / 2.0\n",
    "\n",
    "        genders = face_genders + person_genders\n",
    "        assert len(genders) > 0\n",
    "        # take mode of genders\n",
    "        gender = max(set(genders), key=genders.count)\n",
    "\n",
    "        return age, gender\n",
    "\n",
    "    def get_results_for_tracking(self) -> Tuple[Dict[int, Tuple[float, str, List]], Dict[int, Tuple[float, str, List]]]:\n",
    "        \"\"\"\n",
    "        Get objects from current frame\n",
    "        \"\"\"\n",
    "        persons: Dict[int, AGE_GENDER_TYPE, List] = {}\n",
    "        faces: Dict[int, AGE_GENDER_TYPE, List] = {}\n",
    "\n",
    "        names = self.yolo_results.names\n",
    "        pred_boxes = self.yolo_results.boxes\n",
    "        for _, (det, age, gender, _) in enumerate(zip(pred_boxes, self.ages, self.genders, self.gender_scores)):\n",
    "            if det.id is None:\n",
    "                continue\n",
    "            cat_id, _, guid = int(det.cls), float(det.conf), int(det.id.item())\n",
    "            name = names[cat_id]\n",
    "            if name == \"person\":\n",
    "                persons[guid] = (age, gender, det.xyxy[0].tolist())\n",
    "            elif name == \"face\":\n",
    "                faces[guid] = (age, gender, det.xyxy[0].tolist())\n",
    "\n",
    "        return persons, faces\n",
    "\n",
    "    def associate_faces_with_persons(self):\n",
    "        face_bboxes_inds: List[int] = self.get_bboxes_inds(\"face\")\n",
    "        person_bboxes_inds: List[int] = self.get_bboxes_inds(\"person\")\n",
    "\n",
    "        face_bboxes: List[torch.tensor] = [self.get_bbox_by_ind(ind) for ind in face_bboxes_inds]\n",
    "        person_bboxes: List[torch.tensor] = [self.get_bbox_by_ind(ind) for ind in person_bboxes_inds]\n",
    "\n",
    "        self.face_to_person_map = {ind: None for ind in face_bboxes_inds}\n",
    "        assigned_faces, unassigned_persons_inds = assign_faces(person_bboxes, face_bboxes)\n",
    "\n",
    "        for face_ind, person_ind in enumerate(assigned_faces):\n",
    "            face_ind = face_bboxes_inds[face_ind]\n",
    "            person_ind = person_bboxes_inds[person_ind] if person_ind is not None else None\n",
    "            self.face_to_person_map[face_ind] = person_ind\n",
    "\n",
    "        self.unassigned_persons_inds = [person_bboxes_inds[person_ind] for person_ind in unassigned_persons_inds]\n",
    "\n",
    "    def crop_object(\n",
    "        self, full_image: np.ndarray, ind: int, cut_other_classes: Optional[List[str]] = None\n",
    "    ) -> Optional[np.ndarray]:\n",
    "\n",
    "        IOU_THRESH = 0.000001\n",
    "        MIN_PERSON_CROP_AFTERCUT_RATIO = 0.4\n",
    "        CROP_ROUND_RATE = 0.3\n",
    "        MIN_PERSON_SIZE = 50\n",
    "\n",
    "        obj_bbox = self.get_bbox_by_ind(ind, *full_image.shape[:2])\n",
    "        x1, y1, x2, y2 = obj_bbox\n",
    "        cur_cat = self.yolo_results.names[int(self.yolo_results.boxes[ind].cls)]\n",
    "        # get crop of face or person\n",
    "        obj_image = full_image[y1:y2, x1:x2].copy()\n",
    "        crop_h, crop_w = obj_image.shape[:2]\n",
    "\n",
    "        if cur_cat == \"person\" and (crop_h < MIN_PERSON_SIZE or crop_w < MIN_PERSON_SIZE):\n",
    "            return None\n",
    "\n",
    "        if not cut_other_classes:\n",
    "            return obj_image\n",
    "\n",
    "        # calc iou between obj_bbox and other bboxes\n",
    "        other_bboxes: List[torch.tensor] = [\n",
    "            self.get_bbox_by_ind(other_ind, *full_image.shape[:2]) for other_ind in range(len(self.yolo_results.boxes))\n",
    "        ]\n",
    "\n",
    "        iou_matrix = box_iou(torch.stack([obj_bbox]), torch.stack(other_bboxes)).cpu().numpy()[0]\n",
    "\n",
    "        # cut out other objects in case of intersection\n",
    "        for other_ind, (det, iou) in enumerate(zip(self.yolo_results.boxes, iou_matrix)):\n",
    "            other_cat = self.yolo_results.names[int(det.cls)]\n",
    "            if ind == other_ind or iou < IOU_THRESH or other_cat not in cut_other_classes:\n",
    "                continue\n",
    "            o_x1, o_y1, o_x2, o_y2 = det.xyxy.squeeze().type(torch.int32)\n",
    "\n",
    "            # remap current_person_bbox to reference_person_bbox coordinates\n",
    "            o_x1 = max(o_x1 - x1, 0)\n",
    "            o_y1 = max(o_y1 - y1, 0)\n",
    "            o_x2 = min(o_x2 - x1, crop_w)\n",
    "            o_y2 = min(o_y2 - y1, crop_h)\n",
    "\n",
    "            if other_cat != \"face\":\n",
    "                if (o_y1 / crop_h) < CROP_ROUND_RATE:\n",
    "                    o_y1 = 0\n",
    "                if ((crop_h - o_y2) / crop_h) < CROP_ROUND_RATE:\n",
    "                    o_y2 = crop_h\n",
    "                if (o_x1 / crop_w) < CROP_ROUND_RATE:\n",
    "                    o_x1 = 0\n",
    "                if ((crop_w - o_x2) / crop_w) < CROP_ROUND_RATE:\n",
    "                    o_x2 = crop_w\n",
    "\n",
    "            obj_image[o_y1:o_y2, o_x1:o_x2] = 0\n",
    "\n",
    "        remain_ratio = np.count_nonzero(obj_image) / (obj_image.shape[0] * obj_image.shape[1] * obj_image.shape[2])\n",
    "        if remain_ratio < MIN_PERSON_CROP_AFTERCUT_RATIO:\n",
    "            return None\n",
    "\n",
    "        return obj_image\n",
    "\n",
    "    def collect_crops(self, image) -> PersonAndFaceCrops:\n",
    "\n",
    "        crops_data = PersonAndFaceCrops()\n",
    "        for face_ind, person_ind in self.face_to_person_map.items():\n",
    "            face_image = self.crop_object(image, face_ind, cut_other_classes=[])\n",
    "\n",
    "            if person_ind is None:\n",
    "                crops_data.crops_faces_wo_body[face_ind] = face_image\n",
    "                continue\n",
    "\n",
    "            person_image = self.crop_object(image, person_ind, cut_other_classes=[\"face\", \"person\"])\n",
    "\n",
    "            crops_data.crops_faces[face_ind] = face_image\n",
    "            crops_data.crops_persons[person_ind] = person_image\n",
    "\n",
    "        for person_ind in self.unassigned_persons_inds:\n",
    "            person_image = self.crop_object(image, person_ind, cut_other_classes=[\"face\", \"person\"])\n",
    "            crops_data.crops_persons_wo_face[person_ind] = person_image\n",
    "\n",
    "        # uncomment to save preprocessed crops\n",
    "        # crops_data.save()\n",
    "        return crops_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/WildChlamydia/MiVOLO/blob/main/mivolo/model/create_timm_model.py \n",
    "\n",
    "\"\"\"\n",
    "Code adapted from timm https://github.com/huggingface/pytorch-image-models\n",
    "\n",
    "Modifications and additions for mivolo by / Copyright 2023, Irina Tolstykh, Maxim Kuprashevich\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    model, checkpoint_path, use_ema=True, strict=True, remap=False, filter_keys=None, state_dict_map=None\n",
    "):\n",
    "    if os.path.splitext(checkpoint_path)[-1].lower() in (\".npz\", \".npy\"):\n",
    "        # numpy checkpoint, try to load via model specific load_pretrained fn\n",
    "        if hasattr(model, \"load_pretrained\"):\n",
    "            timm.models._model_builder.load_pretrained(checkpoint_path)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Model cannot load numpy checkpoint\")\n",
    "        return\n",
    "    state_dict = load_state_dict(checkpoint_path, use_ema)\n",
    "    if remap:\n",
    "        state_dict = remap_checkpoint(model, state_dict)\n",
    "    if filter_keys:\n",
    "        for sd_key in list(state_dict.keys()):\n",
    "            for filter_key in filter_keys:\n",
    "                if filter_key in sd_key:\n",
    "                    if sd_key in state_dict:\n",
    "                        del state_dict[sd_key]\n",
    "\n",
    "    rep = []\n",
    "    if state_dict_map is not None:\n",
    "        # 'patch_embed.conv1.' : 'patch_embed.conv.'\n",
    "        for state_k in list(state_dict.keys()):\n",
    "            for target_k, target_v in state_dict_map.items():\n",
    "                if target_v in state_k:\n",
    "                    target_name = state_k.replace(target_v, target_k)\n",
    "                    state_dict[target_name] = state_dict[state_k]\n",
    "                    rep.append(state_k)\n",
    "        for r in rep:\n",
    "            if r in state_dict:\n",
    "                del state_dict[r]\n",
    "\n",
    "    incompatible_keys = model.load_state_dict(state_dict, strict=strict if filter_keys is None else False)\n",
    "    return incompatible_keys\n",
    "\n",
    "\n",
    "def create_model(\n",
    "    model_name: str,\n",
    "    pretrained: bool = False,\n",
    "    pretrained_cfg: Optional[Union[str, Dict[str, Any], PretrainedCfg]] = None,\n",
    "    pretrained_cfg_overlay: Optional[Dict[str, Any]] = None,\n",
    "    checkpoint_path: str = \"\",\n",
    "    scriptable: Optional[bool] = None,\n",
    "    exportable: Optional[bool] = None,\n",
    "    no_jit: Optional[bool] = None,\n",
    "    filter_keys=None,\n",
    "    state_dict_map=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Create a model\n",
    "    Lookup model's entrypoint function and pass relevant args to create a new model.\n",
    "    \"\"\"\n",
    "    # Parameters that aren't supported by all models or are intended to only override model defaults if set\n",
    "    # should default to None in command line args/cfg. Remove them if they are present and not set so that\n",
    "    # non-supporting models don't break and default args remain in effect.\n",
    "    kwargs = {k: v for k, v in kwargs.items() if v is not None}\n",
    "\n",
    "    model_source, model_name = parse_model_name(model_name)\n",
    "    if model_source == \"hf-hub\":\n",
    "        assert not pretrained_cfg, \"pretrained_cfg should not be set when sourcing model from Hugging Face Hub.\"\n",
    "        # For model names specified in the form `hf-hub:path/architecture_name@revision`,\n",
    "        # load model weights + pretrained_cfg from Hugging Face hub.\n",
    "        pretrained_cfg, model_name = load_model_config_from_hf(model_name)\n",
    "    else:\n",
    "        model_name, pretrained_tag = split_model_name_tag(model_name)\n",
    "        if not pretrained_cfg:\n",
    "            # a valid pretrained_cfg argument takes priority over tag in model name\n",
    "            pretrained_cfg = pretrained_tag\n",
    "\n",
    "    if not is_model(model_name):\n",
    "        raise RuntimeError(\"Unknown model (%s)\" % model_name)\n",
    "\n",
    "    create_fn = model_entrypoint(model_name)\n",
    "    with set_layer_config(scriptable=scriptable, exportable=exportable, no_jit=no_jit):\n",
    "        model = create_fn(\n",
    "            pretrained=pretrained,\n",
    "            pretrained_cfg=pretrained_cfg,\n",
    "            pretrained_cfg_overlay=pretrained_cfg_overlay,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    if checkpoint_path:\n",
    "        load_checkpoint(model, checkpoint_path, filter_keys=filter_keys, state_dict_map=state_dict_map)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def class_letterbox(im, new_shape=(640, 640), color=(0, 0, 0), scaleup=True):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    if im.shape[0] == new_shape[0] and im.shape[1] == new_shape[1]:\n",
    "        return im\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    # ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im\n",
    "\n",
    "\n",
    "\n",
    "def prepare_classification_images(\n",
    "    img_list: List[Optional[np.ndarray]],\n",
    "    target_size: int = 224,\n",
    "    mean=IMAGENET_DEFAULT_MEAN,\n",
    "    std=IMAGENET_DEFAULT_STD,\n",
    "    device=None,\n",
    ") -> torch.tensor:\n",
    "\n",
    "    prepared_images: List[torch.tensor] = []\n",
    "\n",
    "    for img in img_list:\n",
    "        if img is None:\n",
    "            img = torch.zeros((3, target_size, target_size), dtype=torch.float32)\n",
    "            img = F.normalize(img, mean=mean, std=std)\n",
    "            img = img.unsqueeze(0)\n",
    "            prepared_images.append(img)\n",
    "            continue\n",
    "        img = class_letterbox(img, new_shape=(target_size, target_size))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img = img / 255.0\n",
    "        img = (img - mean) / std\n",
    "        img = img.astype(dtype=np.float32)\n",
    "\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        img = np.ascontiguousarray(img)\n",
    "        img = torch.from_numpy(img)\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "        prepared_images.append(img)\n",
    "\n",
    "    if len(prepared_images) == 0:\n",
    "        return None\n",
    "\n",
    "    prepared_input = torch.concat(prepared_images)\n",
    "\n",
    "    if device:\n",
    "        prepared_input = prepared_input.to(device)\n",
    "\n",
    "    return prepared_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/WildChlamydia/MiVOLO/blob/main/mivolo/model/mi_volo.py \n",
    "\n",
    "\n",
    "_logger = logging.getLogger(\"MiVOLO\")\n",
    "has_compile = hasattr(torch, \"compile\")\n",
    "\n",
    "\n",
    "class Meta:\n",
    "    def __init__(self):\n",
    "        self.min_age = None\n",
    "        self.max_age = None\n",
    "        self.avg_age = None\n",
    "        self.num_classes = None\n",
    "\n",
    "        self.in_chans = 3\n",
    "        self.with_persons_model = False\n",
    "        self.disable_faces = False\n",
    "        self.use_persons = True\n",
    "        self.only_age = False\n",
    "\n",
    "        self.num_classes_gender = 2\n",
    "        self.input_size = 224\n",
    "\n",
    "    def load_from_ckpt(self, ckpt_path: str, disable_faces: bool = False, use_persons: bool = True) -> \"Meta\":\n",
    "\n",
    "        state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        self.min_age = state[\"min_age\"]\n",
    "        self.max_age = state[\"max_age\"]\n",
    "        self.avg_age = state[\"avg_age\"]\n",
    "        self.only_age = state[\"no_gender\"]\n",
    "\n",
    "        only_age = state[\"no_gender\"]\n",
    "\n",
    "        self.disable_faces = disable_faces\n",
    "        if \"with_persons_model\" in state:\n",
    "            self.with_persons_model = state[\"with_persons_model\"]\n",
    "        else:\n",
    "            self.with_persons_model = True if \"patch_embed.conv1.0.weight\" in state[\"state_dict\"] else False\n",
    "\n",
    "        self.num_classes = 1 if only_age else 3\n",
    "        self.in_chans = 3 if not self.with_persons_model else 6\n",
    "        self.use_persons = use_persons and self.with_persons_model\n",
    "\n",
    "        if not self.with_persons_model and self.disable_faces:\n",
    "            raise ValueError(\"You can not use disable-faces for faces-only model\")\n",
    "        if self.with_persons_model and self.disable_faces and not self.use_persons:\n",
    "            raise ValueError(\n",
    "                \"You can not disable faces and persons together. \"\n",
    "                \"Set --with-persons if you want to run with --disable-faces\"\n",
    "            )\n",
    "        self.input_size = state[\"state_dict\"][\"pos_embed\"].shape[1] * 16\n",
    "        return self\n",
    "\n",
    "    def __str__(self):\n",
    "        attrs = vars(self)\n",
    "        attrs.update({\"use_person_crops\": self.use_person_crops, \"use_face_crops\": self.use_face_crops})\n",
    "        return \", \".join(\"%s: %s\" % item for item in attrs.items())\n",
    "\n",
    "    @property\n",
    "    def use_person_crops(self) -> bool:\n",
    "        return self.with_persons_model and self.use_persons\n",
    "\n",
    "    @property\n",
    "    def use_face_crops(self) -> bool:\n",
    "        return not self.disable_faces or not self.with_persons_model\n",
    "\n",
    "\n",
    "class MiVOLO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        ckpt_path: str,\n",
    "        device: str = \"cuda\",\n",
    "        half: bool = True,\n",
    "        disable_faces: bool = False,\n",
    "        use_persons: bool = True,\n",
    "        verbose: bool = False,\n",
    "        torchcompile: Optional[str] = None,\n",
    "    ):\n",
    "        self.verbose = verbose\n",
    "        self.device = torch.device(device)\n",
    "        self.half = half and self.device.type != \"cpu\"\n",
    "\n",
    "        self.meta: Meta = Meta().load_from_ckpt(ckpt_path, disable_faces, use_persons)\n",
    "        if self.verbose:\n",
    "            _logger.info(f\"Model meta:\\n{str(self.meta)}\")\n",
    "\n",
    "        model_name = f\"mivolo_d1_{self.meta.input_size}\"\n",
    "        self.model = create_model(\n",
    "            model_name=model_name,\n",
    "            num_classes=self.meta.num_classes,\n",
    "            in_chans=self.meta.in_chans,\n",
    "            pretrained=False,\n",
    "            checkpoint_path=ckpt_path,\n",
    "            filter_keys=[\"fds.\"],\n",
    "        )\n",
    "        self.param_count = sum([m.numel() for m in self.model.parameters()])\n",
    "        _logger.info(f\"Model {model_name} created, param count: {self.param_count}\")\n",
    "\n",
    "        self.data_config = resolve_data_config(\n",
    "            model=self.model,\n",
    "            verbose=verbose,\n",
    "            use_test_size=True,\n",
    "        )\n",
    "\n",
    "        self.data_config[\"crop_pct\"] = 1.0\n",
    "        c, h, w = self.data_config[\"input_size\"]\n",
    "        assert h == w, \"Incorrect data_config\"\n",
    "        self.input_size = w\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        if torchcompile:\n",
    "            assert has_compile, \"A version of torch w/ torch.compile() is required for --compile, possibly a nightly.\"\n",
    "            torch._dynamo.reset()\n",
    "            self.model = torch.compile(self.model, backend=torchcompile)\n",
    "\n",
    "        self.model.eval()\n",
    "        if self.half:\n",
    "            self.model = self.model.half()\n",
    "\n",
    "    def warmup(self, batch_size: int, steps=10):\n",
    "        if self.meta.with_persons_model:\n",
    "            input_size = (6, self.input_size, self.input_size)\n",
    "        else:\n",
    "            input_size = self.data_config[\"input_size\"]\n",
    "\n",
    "        input = torch.randn((batch_size,) + tuple(input_size)).to(self.device)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            out = self.inference(input)  # noqa: F841\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    def inference(self, model_input: torch.tensor) -> torch.tensor:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if self.half:\n",
    "                model_input = model_input.half()\n",
    "            output = self.model(model_input)\n",
    "        return output\n",
    "\n",
    "    def predict(self, image: np.ndarray, detected_bboxes: PersonAndFaceResult):\n",
    "        if (\n",
    "            (detected_bboxes.n_objects == 0)\n",
    "            or (not self.meta.use_persons and detected_bboxes.n_faces == 0)\n",
    "            or (self.meta.disable_faces and detected_bboxes.n_persons == 0)\n",
    "        ):\n",
    "            # nothing to process\n",
    "            return\n",
    "\n",
    "        faces_input, person_input, faces_inds, bodies_inds = self.prepare_crops(image, detected_bboxes)\n",
    "\n",
    "        if faces_input is None and person_input is None:\n",
    "            # nothing to process\n",
    "            return\n",
    "\n",
    "        if self.meta.with_persons_model:\n",
    "            model_input = torch.cat((faces_input, person_input), dim=1)\n",
    "        else:\n",
    "            model_input = faces_input\n",
    "        output = self.inference(model_input)\n",
    "\n",
    "        # write gender and age results into detected_bboxes\n",
    "        self.fill_in_results(output, detected_bboxes, faces_inds, bodies_inds)\n",
    "\n",
    "    def fill_in_results(self, output, detected_bboxes, faces_inds, bodies_inds):\n",
    "        if self.meta.only_age:\n",
    "            age_output = output\n",
    "            gender_probs, gender_indx = None, None\n",
    "        else:\n",
    "            age_output = output[:, 2]\n",
    "            gender_output = output[:, :2].softmax(-1)\n",
    "            gender_probs, gender_indx = gender_output.topk(1)\n",
    "\n",
    "        assert output.shape[0] == len(faces_inds) == len(bodies_inds)\n",
    "\n",
    "        # per face\n",
    "        for index in range(output.shape[0]):\n",
    "            face_ind = faces_inds[index]\n",
    "            body_ind = bodies_inds[index]\n",
    "\n",
    "            # get_age\n",
    "            age = age_output[index].item()\n",
    "            age = age * (self.meta.max_age - self.meta.min_age) + self.meta.avg_age\n",
    "            age = round(age, 2)\n",
    "\n",
    "            detected_bboxes.set_age(face_ind, age)\n",
    "            detected_bboxes.set_age(body_ind, age)\n",
    "\n",
    "            _logger.info(f\"\\tage: {age}\")\n",
    "\n",
    "            if gender_probs is not None:\n",
    "                gender = \"male\" if gender_indx[index].item() == 0 else \"female\"\n",
    "                gender_score = gender_probs[index].item()\n",
    "\n",
    "                _logger.info(f\"\\tgender: {gender} [{int(gender_score * 100)}%]\")\n",
    "\n",
    "                detected_bboxes.set_gender(face_ind, gender, gender_score)\n",
    "                detected_bboxes.set_gender(body_ind, gender, gender_score)\n",
    "\n",
    "    def prepare_crops(self, image: np.ndarray, detected_bboxes: PersonAndFaceResult):\n",
    "\n",
    "        if self.meta.use_person_crops and self.meta.use_face_crops:\n",
    "            detected_bboxes.associate_faces_with_persons()\n",
    "\n",
    "        crops: PersonAndFaceCrops = detected_bboxes.collect_crops(image)\n",
    "        (bodies_inds, bodies_crops), (faces_inds, faces_crops) = crops.get_faces_with_bodies(\n",
    "            self.meta.use_person_crops, self.meta.use_face_crops\n",
    "        )\n",
    "\n",
    "        if not self.meta.use_face_crops:\n",
    "            assert all(f is None for f in faces_crops)\n",
    "\n",
    "        faces_input = prepare_classification_images(\n",
    "            faces_crops, self.input_size, self.data_config[\"mean\"], self.data_config[\"std\"], device=self.device\n",
    "        )\n",
    "\n",
    "        if not self.meta.use_person_crops:\n",
    "            assert all(p is None for p in bodies_crops)\n",
    "\n",
    "        person_input = prepare_classification_images(\n",
    "            bodies_crops, self.input_size, self.data_config[\"mean\"], self.data_config[\"std\"], device=self.device\n",
    "        )\n",
    "\n",
    "        _logger.info(\n",
    "            f\"faces_input: {faces_input.shape if faces_input is not None else None}, \"\n",
    "            f\"person_input: {person_input.shape if person_input is not None else None}\"\n",
    "        )\n",
    "\n",
    "        return faces_input, person_input, faces_inds, bodies_inds\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = MiVOLO(\"../pretrained/checkpoint-377.pth.tar\", half=True, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/WildChlamydia/MiVOLO/blob/main/mivolo/predictor.py \n",
    "\n",
    "\n",
    "def filter_results_by_face(results): # redundant now since it's handled in ress_to_objs \n",
    "    cls = results.boxes.cls \n",
    "    is1 = [i for i in range(len(cls)) if cls[i] == 1.0] \n",
    "    is0 = [i for i in range(len(cls)) if cls[i] == 0.0] \n",
    "    face_boxes = results.boxes[is1].xyxy \n",
    "    person_boxes = results.boxes[is0].xyxy \n",
    "\n",
    "    # find persons that overlap with boxes \n",
    "    matching_person_boxes_idxs = [] \n",
    "    match_not_found_for_face_idx = [] \n",
    "    for fbidx in range(len(face_boxes)): \n",
    "        face_box = face_boxes[fbidx]\n",
    "        found=False \n",
    "        for pbidx in range(len(person_boxes)): \n",
    "            person_box = person_boxes[pbidx] \n",
    "            if boxes_overlap(face_box, person_box): \n",
    "                matching_person_boxes_idxs.append(pbidx)  \n",
    "                found=True \n",
    "                break \n",
    "        if (not found): \n",
    "            match_not_found_for_face_idx.append(fbidx)\n",
    "\n",
    "    for notfoundidx in sorted(match_not_found_for_face_idx, reverse=True): \n",
    "        is1.pop(notfoundidx) \n",
    "\n",
    "    return results[is1 + matching_person_boxes_idxs] \n",
    "    \n",
    "def boxes_overlap(face, person): \n",
    "    return (person[0] <= face[0]) and (person[1] <= face[1]) and (person[2] >= face[2]) and (person[3] >= face[3]) \n",
    "\n",
    "\n",
    "def ress_to_objs(face_res, person_res): \n",
    "    raw_person_data = person_res.boxes.data \n",
    "    pd = [] \n",
    "\n",
    "    for rpdidx in range(raw_person_data.shape[0]): \n",
    "        if person_res.boxes.cls[rpdidx] < 0.1: # definitely 0 \n",
    "            pd.append(raw_person_data[rpdidx].tolist()) \n",
    "    person_data = torch.tensor(pd) \n",
    "\n",
    "    face_data = face_res.boxes.data \n",
    "    \n",
    "    #print(\"NAMES:\", person_res.names)\n",
    "    #print(\"RAW PERSON DATA:\", raw_person_data, person_res.names[0])\n",
    "    #print(\"PERSON DATA:\", person_data, person_data.shape[0])\n",
    "    #print(\"FACE DATA:\", face_data, face_data.shape[0])\n",
    "    #1/0\n",
    "    # match and assign IDs \n",
    "\n",
    "    # find persons that overlap with boxes \n",
    "    matching_person_boxes_idxss = [] \n",
    "    for fbidx in range(face_data.shape[0]): \n",
    "        face = face_data[fbidx]\n",
    "        for pbidx in range(person_data.shape[0]): \n",
    "            person = person_data[pbidx] \n",
    "            #print(\"FACE BBOX:\", face[:4]) \n",
    "            #print(\"PERSON BBOX:\", person[:4])\n",
    "            if boxes_overlap(face[:4], person[:4]): \n",
    "                matching_person_boxes_idxss.append([fbidx, pbidx])  \n",
    "\n",
    "    #print(\"MATCHES:\", matching_person_boxes_idxss)\n",
    "\n",
    "    box_data = [] \n",
    "    has6 = False \n",
    "    for fbidx, pbidx in matching_person_boxes_idxss: \n",
    "        person_data_entry = person_data[pbidx].tolist() \n",
    "        #person_data_entry[4] += 1 # so that there's no 0 and 0 issue later \n",
    "        #personid = person_data_entry[4] \n",
    "        face_data_entry = face_data[fbidx].tolist() \n",
    "        #if len(face_data_entry)==6: \n",
    "            #face_data_entry.insert(4, -personid) \n",
    "        #else: \n",
    "        #face_data_entry[4] = -personid \n",
    "\n",
    "        # set classes again \n",
    "        # i blame the weirdness of this code on the completely nonstandard format of stuff here \n",
    "        # just set class (last one) to correct one, and ID (4) is smtg else \n",
    "        if len(person_data_entry) == 6: \n",
    "            person_data_entry.append(0) \n",
    "            person_data_entry[4] = 0 # ID uncertain \n",
    "            has6 = True \n",
    "        else: \n",
    "            person_data_entry[6] = 0 \n",
    "        \n",
    "        if len(face_data_entry) == 6: \n",
    "            face_data_entry.append(0) \n",
    "            face_data_entry[4] = 0 # ID uncertain \n",
    "            has6 = True \n",
    "        else: \n",
    "            face_data_entry[6] = 1 \n",
    "            face_data_entry[4] = -face_data_entry[4] # negate the ID in case, to avoid stuff \n",
    "\n",
    "        # add to box_data \n",
    "        box_data.append(person_data_entry) \n",
    "        box_data.append(face_data_entry)\n",
    "\n",
    "    box_data = torch.tensor(box_data).reshape((-1,7)) \n",
    "    \n",
    "    if has6: # need to show that id is uncertain so \n",
    "        box_data = box_data[:,[0,1,2,3,6,5]] \n",
    "        print(\"HAS6\") \n",
    "        print(box_data)\n",
    "\n",
    "    person_res.boxes = Boxes(box_data, person_res.boxes.orig_shape)\n",
    "    #print(\"FINAL BOX DATA:\",person_res.boxes.data)\n",
    "    person_res.names = {0:'person', 1:'face'} \n",
    "\n",
    "    return person_res \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "curdir = pathlib.Path(__file__).parent.resolve() \n",
    "\n",
    "yolonames = ['yolov8n_face.pt', 'yolov8n.pt'] \n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, detector_weightss=[os.path.join(curdir, yoloname) for yoloname in yolonames], device='cpu', \n",
    "                 checkpoint=os.path.join(curdir, 'model_imdb_cross_person_4.22_99.46.pth.tar'), \n",
    "                 with_persons=True, disable_faces=False, verbose: bool = False, entrance_line=EntranceLine( (0,180) , (640, 180) ), \n",
    "                 entrance_condition=EntranceCondition.BELOW):\n",
    "        self.detector = Detector(detector_weightss, device, verbose=verbose)\n",
    "        self.age_gender_model = MiVOLO(\n",
    "            checkpoint,\n",
    "            device,\n",
    "            half=True,\n",
    "            use_persons=with_persons,\n",
    "            disable_faces=disable_faces,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.entrance_line:EntranceLine = entrance_line \n",
    "        self.entrance_condition:EntranceCondition = entrance_condition \n",
    "\n",
    "    def detect(self, image: np.ndarray) -> Tuple[PersonAndFaceResult, Optional[np.ndarray]]:\n",
    "        #detected_objects: PersonAndFaceResult = self.detector.predict(image)\n",
    "        detected_objects = self.detector.track(image, return_raw_results=True)\n",
    "        detected_objects = filter_results_by_face(detected_objects) # ---------------------------------- APPLICAION OF FILTER \n",
    "        detected_objects:PersonAndFaceResult = PersonAndFaceResult(detected_objects) \n",
    "        return detected_objects \n",
    "    \n",
    "    def recognize(self, image:np.ndarray, detected_objects:PersonAndFaceResult): \n",
    "        self.age_gender_model.predict(image, detected_objects)\n",
    "\n",
    "        out_im = detected_objects.plot()\n",
    "\n",
    "        return detected_objects, out_im\n",
    "\n",
    "    def recognize_track(self, frame, detected_objects_history: Dict[int, List[List]], entereds:list, return_plotted=False, return_det_cnt=False):\n",
    "\n",
    "        face_res, person_res = self.detector.track(frame, return_raw_results=True)\n",
    "        detected_objects = ress_to_objs(face_res, person_res) \n",
    "        \n",
    "        #detected_objects = self.detector.track(frame, return_raw_results=True) \n",
    "        #detected_objects = filter_results_by_face(detected_objects) # ---------------------------------- APPLICAION OF FILTER \n",
    "        \n",
    "        #print('\\n\\n')\n",
    "        #print(\"DETECTED OBJECTS: \")\n",
    "        #print(detected_objects) \n",
    "        #print('\\n') \n",
    "        detected_objects:PersonAndFaceResult = PersonAndFaceResult(detected_objects) \n",
    "        #starttime = time.time() \n",
    "        self.age_gender_model.predict(frame, detected_objects)\n",
    "        #endtime = time.time() \n",
    "        #print(\"TIME TAKEN FOR MIVOLO:\", endtime-starttime) \n",
    "\n",
    "        current_frame_objs = detected_objects.get_results_for_tracking()\n",
    "        cur_persons: Dict[int, AGE_GENDER_TYPE, List] = current_frame_objs[0]\n",
    "        cur_faces: Dict[int, AGE_GENDER_TYPE, List] = current_frame_objs[1]\n",
    "\n",
    "        # add tr_persons and tr_faces to history\n",
    "        for guid, datapos in cur_faces.items():\n",
    "            d1, d2, pos = datapos \n",
    "            data = (d1, d2) \n",
    "            #print(datapos)\n",
    "            # not useful for tracking :)\n",
    "            if None not in data:\n",
    "                #print(\"GOT TO POS\", pos)\n",
    "                if self.entrance_line.entered(self.entrance_condition, (pos[0]+pos[2])//2, (pos[1]+pos[3])//2): \n",
    "                    #print(\"ENTERED ALREADY\")\n",
    "                    if guid not in detected_objects_history.keys(): continue # this means it was detected inside already from the start \n",
    "                    # done already yay \n",
    "                    # average out ages \n",
    "                    male_cnt = 0 \n",
    "                    for age, gen in detected_objects_history[guid]: \n",
    "                        male_cnt += int(gen=='male')\n",
    "                    \n",
    "                    final_gen = 'male' if (male_cnt*2 >= len(detected_objects_history[guid])) else 'female' # mode \n",
    "\n",
    "                    ages = [] # probably will be using median \n",
    "                    for age, gen in detected_objects_history[guid]: \n",
    "                        if (gen == final_gen): ages.append(age) \n",
    "                    final_age = ages[len(ages)//2] # median \n",
    "\n",
    "                    entereds.append([final_age, final_gen, datetime.datetime.now()]) # age, gender, time \n",
    "                    detected_objects_history.pop(guid, None)\n",
    "                else: \n",
    "                    detected_objects_history[guid].append(data)\n",
    "\n",
    "        for guid, datapos in cur_persons.items():\n",
    "            d1, d2, pos = datapos \n",
    "            data = (d1, d2) \n",
    "            if guid not in detected_objects_history.keys(): continue # has been removed \n",
    "            if None not in data:\n",
    "                detected_objects_history[guid].append(data)\n",
    "\n",
    "        detected_objects.set_tracked_age_gender(detected_objects_history)\n",
    "\n",
    "        #print(\"CUR PERSONS ITEMS\") \n",
    "        #print(cur_persons)\n",
    "        #print(\"DETECTED OBJECTS HISTORY\") \n",
    "        #print(detected_objects_history) \n",
    "        #print(entereds) \n",
    "\n",
    "        if return_det_cnt: \n",
    "            if return_plotted: return detected_objects_history, entereds, detected_objects.plot(), len(cur_faces.keys())\n",
    "            else: return detected_objects_history, entereds, len(cur_faces.keys())\n",
    "        else: \n",
    "            if return_plotted: return detected_objects_history, entereds, detected_objects.plot()\n",
    "            else: return detected_objects_history, entereds \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "predictor = Predictor(verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_video = False \n",
    "debug_savedir = \".\"\n",
    "filename_prefix = 'yolodetdebug_'\n",
    "camera_id = 0 \n",
    "\n",
    "\n",
    "# define entrance line \n",
    "entrance_y_coordinate = 180 \n",
    "entrance_line_xys = (0, entrance_y_coordinate) , (640, entrance_y_coordinate)  \n",
    "entrance_line = EntranceLine( *entrance_line_xys ) \n",
    "# y = mx+c; this is m and c \n",
    "entrance_condition = EntranceCondition.BELOW # higher y than line \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_dt_format(dt): \n",
    "    return dt.strftime(\"%Y%m%d%H%M%S\") \n",
    "\n",
    "if save_video: \n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "    out = cv2.VideoWriter(os.path.join(debug_savedir, \n",
    "                                       filename_prefix+'_video_'+to_dt_format(datetime.datetime.now())+'.avi'), \n",
    "                                       fourcc, 1, (640, 480) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get ready model \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "predictor = Predictor(verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# run \n",
    "detected_objects_history: Dict[int, List[List]] = defaultdict(list)\n",
    "\n",
    "# load entereds if needed \n",
    "if os.path.isfile(os.path.join(filename_prefix+\"_entereds_save.txt\")): \n",
    "    with open(os.path.join(filename_prefix+\"_entereds_save.txt\"), 'r') as f: \n",
    "        saved_prev_ckpt = eval(f.readline()) \n",
    "        saved_error_time = eval(f.readline()) \n",
    "        entereds = eval(f.readline()) \n",
    "    if (datetime.datetime.now() - saved_prev_ckpt).total_seconds() < 900: \n",
    "        # still the same period yay, keep entereds \n",
    "        pass \n",
    "    else: \n",
    "        # add save function to save results here !! ---------------------------------------------------------------------------\n",
    "        # reset \n",
    "        entereds = [] \n",
    "\n",
    "    os.remove(os.path.join(filename_prefix+\"_entereds_save.txt\"))\n",
    "\n",
    "else: \n",
    "    entereds = [] \n",
    "\n",
    "\n",
    "\n",
    "# find next save checkpoint \n",
    "next_ckpt = datetime.datetime.now() \n",
    "while (next_ckpt.minute%15) != 0: \n",
    "    next_ckpt = next_ckpt + datetime.timedelta(minutes=1) \n",
    "next_ckpt = next_ckpt.replace(second=0, microsecond=0) \n",
    "# get prev checkpoint \n",
    "prev_ckpt = next_ckpt - datetime.timedelta(minutes=15) \n",
    "\n",
    "def get_next_ckpt(dt): \n",
    "    return dt + datetime.timedelta(minutes=15) \n",
    "\n",
    "# start running \n",
    "try: \n",
    "    vs = VideoStream(src=camera_id).start() #0\n",
    "    # vs = cv2.VideoCapture(camera_id, cv2.CAP_DSHOW) # for pci yuy2\n",
    "\n",
    "    while True: \n",
    "        \n",
    "        frame = vs.read() \n",
    "        #ret, frame = vs.read()\n",
    "        #if not ret: \n",
    "        #    raise ValueError(\"Failed to read from webcam!\")\n",
    "\n",
    "        if not save_video: \n",
    "            detected_objects_history, entereds = predictor.recognize_track(frame, detected_objects_history, entereds) \n",
    "        else: \n",
    "            detected_objects_history, entereds, frame, det_cnt = predictor.recognize_track(frame, detected_objects_history, entereds, return_plotted=True, return_det_cnt=True) \n",
    "            frame = cv2.line(frame, *entrance_line_xys, (255,0,0)) # draw entrance line on frame \n",
    "            frame = cv2.putText(frame, str(datetime.datetime.now()), (10,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2) # put time \n",
    "            frame = cv2.putText(frame, str(datetime.datetime.now()), (10,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2) # put time \n",
    "            if det_cnt != 0: \n",
    "                out.write(frame)\n",
    "        \n",
    "        if (datetime.datetime.now() - next_ckpt).total_seconds() > 0: # past the checkpoint \n",
    "            # sadd saving code HERE !!! ----------------------------------------------------------------------------\n",
    "\n",
    "            # clear some storage \n",
    "            entereds = [] \n",
    "            # no need to clear detected_objects_history as the key is already removed when putting into entereds \n",
    "            prev_ckpt = next_ckpt \n",
    "            next_ckpt = get_next_ckpt(next_ckpt) \n",
    "        \n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "        #print(detected_objects_history) \n",
    "        #print(entereds)\n",
    "\n",
    "    raise ValueError(\"ENDED VIDEO STREAM\")\n",
    "\n",
    "\n",
    "except ValueError as e: \n",
    "    error_time = datetime.datetime.now() \n",
    "\n",
    "    print() \n",
    "    print(\"ERROR:\") \n",
    "    print(e) \n",
    "    print(\"\\nENDING SESSION!\") \n",
    "    if save_video: out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    #save_output(entereds, prev_ckpt, er'ror_time)\n",
    "    with open(os.path.join(filename_prefix+\"_entereds_save.txt\"), 'w') as f: \n",
    "        f.write(repr(prev_ckpt) + '\\n' + repr(error_time) + '\\n' + repr(entereds) + '\\n') \n",
    "\n",
    "    print(\"ENTEREDS:\", entereds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
