{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torchvision"
      ],
      "metadata": {
        "id": "NH9tVvxhnJaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6QrE6g2mkMB",
        "outputId": "a702e919-96d3-4a91-a528-fc033bc9412a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                      download=True, transform=transform)\n",
        "subset_indices = torch.randperm(len(trainset))[:20000]  # i have negative patience\n",
        "trainset = Subset(trainset, subset_indices)\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=1024, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                     download=True, transform=transform)\n",
        "subset_indices = torch.randperm(len(testset))[:5000]\n",
        "testset = Subset(testset, subset_indices)\n",
        "test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=96):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2  # (32 // 4) ** 2 = 64 patches\n",
        "        #we use convolution as an epic hack to replace patching\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # (B, embed_dim, 8, 8)  with patch_size=4\n",
        "        x = x.flatten(2)   # (B, embed_dim, 64)\n",
        "        x = x.transpose(1, 2)  # (B, 64, embed_dim)  <- Correct order for MultiheadAttention\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features, drop):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, n_heads, mlp_ratio, drop, attn_drop):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = nn.MultiheadAttention(dim, n_heads, dropout=attn_drop, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        hidden_features = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=hidden_features, out_features=dim, drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x, _ = self.attn(x, x, x)\n",
        "        x = x + residual\n",
        "\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = x + residual\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, n_classes=10, embed_dim=96,  # Adjusted embed_dim\n",
        "                 depth=6, n_heads=8, mlp_ratio=4., drop_rate=0.4, attn_drop_rate=0.2):  # Adjusted depth, n_heads\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_channels=in_channels, embed_dim=embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.n_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            Block(dim=embed_dim, n_heads=n_heads, mlp_ratio=mlp_ratio, drop=drop_rate, attn_drop=attn_drop_rate)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x) # (B, num_patches, embed_dim)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1) # (B, num_patches + 1, embed_dim)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = x[:, 0]  # CLS token\n",
        "        x = self.head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "m-jR3MWWpzGO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViT()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.03)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Training Accuracy: {100 * correct_train / total_train:.2f}%')\n",
        "    running_loss = 0.0\n",
        "\n",
        "\n",
        "model.eval()\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_test += labels.size(0)\n",
        "        correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct_test / total_test\n",
        "print(f'Test Accuracy: {test_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja1EqCqIp0rh",
        "outputId": "7b3cdd09-a173-4916-c562-b212b4cc979e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 2.0679, Training Accuracy: 22.14%\n",
            "Epoch [2/20], Loss: 1.9384, Training Accuracy: 27.36%\n",
            "Epoch [3/20], Loss: 1.8322, Training Accuracy: 31.09%\n",
            "Epoch [4/20], Loss: 1.7521, Training Accuracy: 33.84%\n",
            "Epoch [5/20], Loss: 1.6940, Training Accuracy: 36.17%\n",
            "Epoch [6/20], Loss: 1.6300, Training Accuracy: 38.91%\n",
            "Epoch [7/20], Loss: 1.6062, Training Accuracy: 39.95%\n",
            "Epoch [8/20], Loss: 1.5485, Training Accuracy: 42.96%\n",
            "Epoch [9/20], Loss: 1.5009, Training Accuracy: 44.38%\n",
            "Epoch [10/20], Loss: 1.4515, Training Accuracy: 46.02%\n",
            "Epoch [11/20], Loss: 1.4142, Training Accuracy: 47.98%\n",
            "Epoch [12/20], Loss: 1.3829, Training Accuracy: 49.69%\n",
            "Epoch [13/20], Loss: 1.3432, Training Accuracy: 50.73%\n",
            "Epoch [14/20], Loss: 1.3096, Training Accuracy: 51.95%\n",
            "Epoch [15/20], Loss: 1.2992, Training Accuracy: 52.28%\n",
            "Epoch [16/20], Loss: 1.2796, Training Accuracy: 53.34%\n",
            "Epoch [17/20], Loss: 1.2579, Training Accuracy: 54.03%\n",
            "Epoch [18/20], Loss: 1.2351, Training Accuracy: 54.58%\n",
            "Epoch [19/20], Loss: 1.2262, Training Accuracy: 55.21%\n",
            "Epoch [20/20], Loss: 1.2059, Training Accuracy: 55.83%\n",
            "Test Accuracy: 54.78%\n"
          ]
        }
      ]
    }
  ]
}