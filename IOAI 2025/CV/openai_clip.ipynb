{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example i'll finetune openai CLIP on the PASCAL VOC 2012 Dataset (https://www.kaggle.com/datasets/gopalbhattrai/pascal-voc-2012-dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#INCOMPLETE !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# code adapted from: https://github.com/openai/CLIP/issues/83 \n",
    "\n",
    "import clip \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torchvision \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from PIL import Image \n",
    "\n",
    "import albumentations as A \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCH = 4 \n",
    "BATCH_SIZE = 16 \n",
    "\n",
    "\n",
    "transform = A.Compose([ # yup i'm using albumentations not torchvision for these transforms haha though ofc torchvision works too \n",
    "    A.HorizontalFlip(p=0.5), \n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1)), \n",
    "    #A.RandomRain(), \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-L/14\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "\n",
    "\n",
    "def train_preprocess(x:Image.Image): # might not be the optimal way but haha \n",
    "    x = transform(np.array(x)) \n",
    "    return preprocess(Image.fromarray(x['image'])) \n",
    "\n",
    "\n",
    "# this is done first, so that we can get the preprocess() function used by CLIP and stuff before data loading \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocd_train = torchvision.datasets.VOCDetection('./data/', image_set='train', download=True, transform = train_preprocess) \n",
    "vocd_test = torchvision.datasets.VOCDetection('./data/', image_set='test', download=True, transform=preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation \n",
    "\n",
    "class image_title_dataset(Dataset):\n",
    "    surrounding_pixels = 8 \n",
    "\n",
    "    def __init__(self):\n",
    "        self.vlmdata = pd.read_json('.train_data/vlm.jsonl', lines=True) \n",
    "        self.length = 0 \n",
    "        self.poss = [] \n",
    "        for i in range(len(self.vlmdata)): \n",
    "           self.poss.append( self.length ) \n",
    "           self.length += len(self.vlmdata.loc[i].annotations) \n",
    "        self.poss.append(self.length) \n",
    "        \n",
    "        self.next_i = 0 \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length \n",
    "\n",
    "    def __getitem__(self, in_idx):\n",
    "        idxset = False \n",
    "        if (self.poss[self.next_i] <= in_idx): \n",
    "            if (in_idx < self.poss[self.next_i+1]): \n",
    "                idx = self.next_i \n",
    "                idxset = True \n",
    "            elif (in_idx < self.poss[self.next_i+2]): \n",
    "               self.next_i += 1 \n",
    "               idx = self.next_i \n",
    "               idxset = True \n",
    "        \n",
    "        if (not idxset): \n",
    "            # binary search out idx \n",
    "            left=0 \n",
    "            right=len(self.vlmdata)-1 \n",
    "            \n",
    "            while (right-left > 1): \n",
    "                mid = (left+right)//2 \n",
    "                if (self.poss[mid] > in_idx): \n",
    "                    right = mid \n",
    "                else: \n",
    "                   left = mid \n",
    "\n",
    "            if (in_idx > self.poss[right]): \n",
    "               self.next_i = right \n",
    "               idx = right \n",
    "            else: \n",
    "               self.next_i = left \n",
    "               idx = left \n",
    "\n",
    "        \n",
    "\n",
    "        image = Image.open(\"train_data/images/\"+self.vlmdata.loc[idx].image)\n",
    "\n",
    "        crop_bbox = self.get_crop_bbox(*self.vlmdata.loc[idx].annotations[in_idx-self.poss[self.next_i]]['bbox'], *image.size)\n",
    "\n",
    "        image = image.crop(crop_bbox) \n",
    "        image = transform(image=np.array(image))['image']\n",
    "\n",
    "        '''plt.figure(figsize=(20,10)) \n",
    "        plt.axis('off') \n",
    "        plt.imshow(image)\n",
    "        plt.show() \n",
    "        print(image) ''' \n",
    "\n",
    "        image = preprocess(Image.fromarray(image)) # Image from PIL module\n",
    "\n",
    "        caption = self.vlmdata.loc[idx].annotations[in_idx-self.poss[self.next_i]]['caption'] \n",
    "        return image, clip.tokenize(caption)[0] \n",
    "    \n",
    "    def get_crop_bbox(self, x,y,w,h, maxx, maxy): \n",
    "        a = max(0, x-image_title_dataset.surrounding_pixels) \n",
    "        b = max(0, y-image_title_dataset.surrounding_pixels)\n",
    "        c = min(maxx, x+w+image_title_dataset.surrounding_pixels) \n",
    "        d = min(maxy, y+h+image_title_dataset.surrounding_pixels)\n",
    "        return (a, b, c, d) \n",
    "\n",
    "# use your own data\n",
    "dataset = image_title_dataset()\n",
    "train_dataloader = DataLoader(dataset,batch_size = BATCH_SIZE) #Define your own dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "\n",
    "if device == \"cpu\":\n",
    "  model.float()\n",
    "else :\n",
    "  clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-7,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "print(\"DEVICE:\", device)\n",
    "\n",
    "\n",
    "\n",
    "# training process \n",
    "for epoch in range(EPOCH):\n",
    "    print(\"EPOCH\", epoch)\n",
    "    for batch in train_dataloader :\n",
    "        #print(\"batch\")\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, texts = batch \n",
    "\n",
    "        #print(\"IMAGES:\", images) \n",
    "        #print(\"TEXTS:\", texts)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "\n",
    "\n",
    "\n",
    "    # save model \n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, \"model_checkpoint/model_\"+str(epoch)+\".pt\") #just change to your preferred folder/filename\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
