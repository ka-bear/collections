{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example i'll finetune openai CLIP on the PASCAL VOC 2012 Dataset (https://www.kaggle.com/datasets/gopalbhattrai/pascal-voc-2012-dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from: https://github.com/openai/CLIP/issues/83 \n",
    "\n",
    "import clip \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torchvision \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from PIL import Image \n",
    "\n",
    "import albumentations as A \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import random \n",
    "\n",
    "seed = 10 \n",
    "random.seed(seed)\n",
    "np.random.seed(seed) \n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCH = 4 \n",
    "BATCH_SIZE = 16 \n",
    "\n",
    "\n",
    "transform = A.Compose([ # yup i'm using albumentations not torchvision for these transforms haha though ofc torchvision works too \n",
    "    A.HorizontalFlip(p=0.5), \n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1)), \n",
    "    #A.RandomRain(), \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "print(\"DEVICE:\", device)\n",
    "model, preprocess = clip.load(\"ViT-L/14\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "\n",
    "\n",
    "def train_preprocess(x:Image.Image): # might not be the optimal way but haha \n",
    "    x = transform(np.array(x)) \n",
    "    return preprocess(Image.fromarray(x['image'])) \n",
    "\n",
    "\n",
    "# this is done first, so that we can get the preprocess() function used by CLIP and stuff before data loading \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data/\n",
      "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data/\n"
     ]
    }
   ],
   "source": [
    "# data preparation \n",
    "\n",
    "class VOCDDataset(Dataset):\n",
    "    surrounding_pixels = 0\n",
    "\n",
    "    vocd_train = torchvision.datasets.VOCDetection('./data/', image_set='train', download=True) \n",
    "    vocd_test = torchvision.datasets.VOCDetection('./data/', image_set='val', download=True)\n",
    "\n",
    "\n",
    "    def assign_indices(self): \n",
    "\n",
    "        if self.mode == 'train': \n",
    "            data_len = len(VOCDDataset.vocd_train)\n",
    "        elif self.mode == 'test': \n",
    "            data_len = len(VOCDDataset.vocd_test)\n",
    "\n",
    "        idxs = list(range(data_len)) \n",
    "        if self.shuffle: \n",
    "            random.shuffle(idxs) \n",
    "\n",
    "        self.idxs = idxs \n",
    "\n",
    "        self.length = 0 \n",
    "        self.poss = [] \n",
    "        for i in idxs: \n",
    "            self.poss.append( self.length ) \n",
    "            if self.mode == 'train': \n",
    "                num_objs = len(VOCDDataset.vocd_train[i][1]['annotation']['object']) # number of objects \n",
    "            elif self.mode == 'test': \n",
    "                num_objs = len(VOCDDataset.vocd_test[i][1]['annotation']['object'])\n",
    "            self.length += num_objs \n",
    "        self.poss.append(self.length) \n",
    "\n",
    "        #print(\"POSS:\", self.poss)\n",
    "        \n",
    "        self.next_i = 0 \n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, mode='train', shuffle=True):\n",
    "        assert mode in ['train', 'test',], 'Only train and test sets are available'\n",
    "        self.mode = mode \n",
    "\n",
    "\n",
    "        self.shuffle = shuffle \n",
    "\n",
    "        self.assign_indices()\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length \n",
    "\n",
    "    def __getitem__(self, in_idx):\n",
    "        idxset = False \n",
    "        if (self.poss[self.next_i] <= in_idx): \n",
    "            if (in_idx < self.poss[self.next_i+1]): \n",
    "                idx = self.next_i \n",
    "                idxset = True \n",
    "            elif (in_idx < self.poss[self.next_i+2]): \n",
    "               self.next_i += 1 \n",
    "               idx = self.next_i \n",
    "               idxset = True \n",
    "        \n",
    "        if (not idxset): \n",
    "            # binary search out idx \n",
    "            left=0 \n",
    "\n",
    "            right=len(self.poss)-1\n",
    "            \n",
    "            while (right-left > 1): \n",
    "                mid = (left+right)//2 \n",
    "                if (self.poss[mid] > in_idx): \n",
    "                    right = mid \n",
    "                else: \n",
    "                   left = mid \n",
    "\n",
    "            if (in_idx > self.poss[right]): \n",
    "               self.next_i = right \n",
    "               idx = right \n",
    "            else: \n",
    "               self.next_i = left \n",
    "               idx = left \n",
    "\n",
    "        #print(\"IN_IDX {} > POSS[{}] ({})\".format(in_idx, idx, self.poss[idx])) \n",
    "        \n",
    "        if self.mode == 'train': \n",
    "            image, y = VOCDDataset.vocd_train[self.idxs[idx]]\n",
    "        elif self.mode == 'test': \n",
    "            image, y = VOCDDataset.vocd_test[self.idxs[idx]]\n",
    "\n",
    "        objects = y['annotation']['object'] \n",
    "        # objects[i]['name], objects[i]['bndbox'] will be dict of {'xmin', 'ymin', 'x}\n",
    "        obj = objects[in_idx-self.poss[idx]]\n",
    "\n",
    "        crop_bbox = self.get_crop_bbox(obj['bndbox'], *image.size)\n",
    "\n",
    "        image = image.crop(crop_bbox) \n",
    "        image = transform(image=np.array(image))['image']\n",
    "\n",
    "        '''plt.figure(figsize=(20,10)) \n",
    "        plt.axis('off') \n",
    "        plt.imshow(image)\n",
    "        plt.show() \n",
    "        print(image) ''' \n",
    "\n",
    "        image = preprocess(Image.fromarray(image)) # Image from PIL module\n",
    "\n",
    "        caption = obj['name']\n",
    "        return image, clip.tokenize(caption)[0] \n",
    "    \n",
    "    def get_crop_bbox(self, bndbox, maxx, maxy): \n",
    "        \n",
    "        a = max(0, int(bndbox['xmin'])-VOCDDataset.surrounding_pixels) \n",
    "        b = max(0, int(bndbox['ymin'])-VOCDDataset.surrounding_pixels)\n",
    "        c = min(maxx, int(bndbox['xmax'])+VOCDDataset.surrounding_pixels) \n",
    "        d = min(maxy, int(bndbox['ymax'])+VOCDDataset.surrounding_pixels)\n",
    "        return (a, b, c, d) \n",
    "\n",
    "dataset = VOCDDataset(mode='train')\n",
    "train_dataloader = DataLoader(dataset,batch_size = BATCH_SIZE) #Define your own dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "\n",
    "if device == \"cpu\":\n",
    "  model.float()\n",
    "else :\n",
    "  clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda:0\n",
      "EPOCH 0\n",
      "EPOCH 1\n",
      "EPOCH 2\n",
      "EPOCH 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-7,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "print(\"DEVICE:\", device)\n",
    "\n",
    "\n",
    "\n",
    "# training process \n",
    "for epoch in range(EPOCH):\n",
    "    print(\"EPOCH\", epoch)\n",
    "    for batch in train_dataloader :\n",
    "        #print(\"batch\")\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, texts = batch \n",
    "\n",
    "        #print(\"IMAGES:\", images) \n",
    "        #print(\"TEXTS:\", texts)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "\n",
    "\n",
    "\n",
    "    # save model \n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, \"clip_models/model_\"+str(epoch)+\".pt\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
