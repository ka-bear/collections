{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "- numerical representation of words\n",
    "\n",
    "### GloVe: Global Vectors for Word Representation|\n",
    "- An unsupervised learning algorithm for obtaining vector representations for words\n",
    "- \"The training objective of GloVe is to learn word vectors such that their dot product equals the logarithm of the wordsâ€™ probability of co-occurrence.\" - taken from the GloVe project page.\n",
    "- First we need a cooccurrence matrix X from the corpus/text -> matrix X such that X_{ij} shows us number of times i appears in context of j => n-gram sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Cooccurence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from sklearn.preprocessing import normalize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Genesis dataset and tokenize it\n",
    "corpus = nltk.corpus.genesis.words()[0:300]\n",
    "window_size = 5  # The size of the context window\n",
    "\n",
    "# Generate co-occurrence matrix\n",
    "def create_cooccurrence_matrix(corpus, window_size, vocab_size):\n",
    "    corpus = [word.lower() for word in corpus if word not in string.punctuation]\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(set(corpus))}\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    vocab_size = len(word_to_idx)\n",
    "    cooccurrence_matrix = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n",
    "\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        target_word = corpus[i]\n",
    "        target_idx = word_to_idx[target_word]\n",
    "        \n",
    "        # Define the window context range\n",
    "        start = max(i - window_size, 0)\n",
    "        end = min(i + window_size, len(corpus))\n",
    "        \n",
    "        # Count co-occurrences in the context window\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                context_word = corpus[j]\n",
    "                context_idx = word_to_idx[context_word]\n",
    "                cooccurrence_matrix[target_idx, context_idx] += 1\n",
    "    \n",
    "    return cooccurrence_matrix, word_to_idx, idx_to_word\n",
    "\n",
    "cock_mat, word_to_idx, idx_to_word = create_cooccurrence_matrix(corpus, window_size, vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = cock_mat.shape[0]\n",
    "embedding_dim = 100  # Dimension of word embeddings\n",
    "learning_rate = 0.1\n",
    "epochs = 20\n",
    "batch_size = 1024\n",
    "\n",
    "class GloVeModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim,  alpha=0.75, x_max=100):\n",
    "        super(GloVeModel, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.bias_wi = nn.Embedding(vocab_size, 1)\n",
    "        self.bias_wj = nn.Embedding(vocab_size, 1)\n",
    "        self.alpha = alpha\n",
    "        self.x_max = x_max\n",
    "\n",
    "    def forward(self, wi, wj):\n",
    "        wi_embed = self.word_embeddings(wi)\n",
    "        wj_embed = self.context_embeddings(wj)\n",
    "        wi_bias = self.bias_wi(wi)  # [batch_size, 1]\n",
    "        wj_bias = self.bias_wj(wj)  # [batch_size, 1]\n",
    "        return torch.sum(wi_embed * wj_embed, dim=1) + wi_bias.squeeze() + wj_bias.squeeze() #Dot product \n",
    "        \n",
    "    def loss(self, cock_mat, predictions):\n",
    "        logXij = torch.where(cock_mat > 0, torch.log(cock_mat), torch.zeros_like(cock_mat))\n",
    "        #weight f(xij) as defined in paper\n",
    "        weight_term = torch.where(cock_mat < self.x_max, torch.pow(cock_mat / self.x_max, self.alpha), torch.ones_like(cock_mat))\n",
    "\n",
    "        loss = weight_term * torch.pow(predictions - logXij, 2)\n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 5045.591790393148\n",
      "Epoch 2/20, Loss: 1128.8292956238424\n",
      "Epoch 3/20, Loss: 445.07131927004826\n",
      "Epoch 4/20, Loss: 231.5430280246482\n",
      "Epoch 5/20, Loss: 132.1775304393899\n",
      "Epoch 6/20, Loss: 79.60237106857802\n",
      "Epoch 7/20, Loss: 49.69493308339192\n",
      "Epoch 8/20, Loss: 31.855333866887353\n",
      "Epoch 9/20, Loss: 20.84425056023504\n",
      "Epoch 10/20, Loss: 13.868710841579349\n",
      "Epoch 11/20, Loss: 9.35738230683056\n",
      "Epoch 12/20, Loss: 6.389841038519024\n",
      "Epoch 13/20, Loss: 4.409705105416914\n",
      "Epoch 14/20, Loss: 3.0720452898829893\n",
      "Epoch 15/20, Loss: 2.1585620649652704\n",
      "Epoch 16/20, Loss: 1.5286779960898715\n",
      "Epoch 17/20, Loss: 1.090509531907611\n",
      "Epoch 18/20, Loss: 0.7832397443558189\n",
      "Epoch 19/20, Loss: 0.5661462359258103\n",
      "Epoch 20/20, Loss: 0.41169094105608744\n"
     ]
    }
   ],
   "source": [
    "# Create the GloVe model\n",
    "model = GloVeModel(vocab_size, embedding_dim)\n",
    "\n",
    "# Adagrad optimizer\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for target_idx in range(vocab_size):\n",
    "        for context_idx in range(vocab_size):\n",
    "            cooc = cock_mat[target_idx, context_idx]\n",
    "            if cooc > 0:  # Only process pairs with non-zero co-occurrence\n",
    "                wi = torch.tensor([target_idx], dtype=torch.long)\n",
    "                wj = torch.tensor([context_idx], dtype=torch.long)\n",
    "                cooc = torch.tensor([cooc], dtype=torch.float)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions = model(wi, wj)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = model.loss(cooc, predictions)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the trained embeddings to get the top similar words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: god, Similarity: 1.0000, Bias: 0.4564\n",
      "Word: night, Similarity: 0.1613, Bias: 0.0191\n",
      "Word: darkness, Similarity: 0.1409, Bias: 0.0385\n",
      "Word: moved, Similarity: 0.1212, Bias: -0.7573\n",
      "Word: earth, Similarity: 0.1166, Bias: -0.1654\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_similar_words(word, top_n=5):\n",
    "    # Get the word index\n",
    "    word_idx = word_to_idx.get(word.lower())\n",
    "    if word_idx is None:\n",
    "        print(f\"'{word}' not found in vocabulary.\")\n",
    "        return []\n",
    "\n",
    "    # Get the embedding of the word\n",
    "    word_embedding = model.word_embeddings(torch.tensor([word_idx], dtype=torch.long))\n",
    "    word_bias = model.bias_wi(torch.tensor([word_idx], dtype=torch.long))\n",
    "\n",
    "    # Compute cosine similarity for all words\n",
    "    similarities = []\n",
    "    for idx in range(vocab_size):\n",
    "        # Check if the index exists in idx_to_word\n",
    "        if idx not in idx_to_word:\n",
    "            continue  # Skip invalid indices\n",
    "        \n",
    "        # Get the embedding of each word\n",
    "        context_embedding = model.word_embeddings(torch.tensor([idx], dtype=torch.long))\n",
    "        context_bias = model.bias_wi(torch.tensor([idx], dtype=torch.long))\n",
    "\n",
    "        # Calculate cosine similarity between the word and the context word\n",
    "        similarity = torch.cosine_similarity(word_embedding, context_embedding, dim=1)\n",
    "        similarity = similarity.item()  # Convert to scalar\n",
    "\n",
    "        similarities.append((idx_to_word[idx], similarity, context_bias.item()))\n",
    "    \n",
    "    # Sort the similar words based on similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top_n most similar words\n",
    "    similar_words = [(word, sim, bias) for word, sim, bias in similarities[:top_n]]\n",
    "    return similar_words\n",
    "\n",
    "similar_words = get_similar_words(\"god\", top_n=5)\n",
    "for word, sim, bias in similar_words:\n",
    "    print(f\"Word: {word}, Similarity: {sim:.4f}, Bias: {bias:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors for word representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). https://doi.org/10.3115/v1/d14-1162 - https://nlp.stanford.edu/pubs/glove.pdf\n",
    "2. https://www.foldl.me/2014/glove-python\n",
    "3. https://github.com/hans/glove.py/blob/master/glove.py\n",
    "4. https://nlp.stanford.edu/projects/glove/\n",
    "5 .https://github.com/noaRricky/pytorch-glove\n",
    "6. ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skibidi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
