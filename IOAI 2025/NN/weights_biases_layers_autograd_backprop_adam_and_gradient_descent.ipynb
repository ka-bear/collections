{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH5Z0HaAaWi4"
      },
      "source": [
        "# Weights, Biases and Layers implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVyynxFiaaxj"
      },
      "source": [
        "With automatic backprop (differentiation with approximation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "07pHNF7VbR_Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np # can replace with cupy\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcL_mQcAiG09",
        "outputId": "49d85273-f17b-429f-d63e-678d0f8b7ef8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-1.4426949038686683, 0.0]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# PREP FOR TESTING\n",
        "\n",
        "# binary crossentropy\n",
        "def CEloss(p_s0, p_s1, y_0, y_1, eps=1e-7):\n",
        "  #print(p_s0, p_s1, y_0, y_1, eps)\n",
        "  return (-1/np.log(2)) * (y_0*np.log(p_s0+eps) + y_1*np.log(p_s1+eps))\n",
        "\n",
        "\n",
        "# to help turn gradients after function to gradients before function\n",
        "def calc_grads_intinputs(fn, vals:list, ress:list, dx=1e-8, fn_out=None): # dx < eps # actually it's float inputs\n",
        "  #This only works if the function takes in int inputs like fn(*args):\n",
        "  # note that fn should return a numpy array so that it can be, like, subtracted\n",
        "  if fn_out is None:\n",
        "    fn_out = fn(*vals, *ress)\n",
        "  grads = []\n",
        "  for vidx in range(len(vals)):\n",
        "    plus = fn(*vals[:vidx], vals[vidx]+dx, *vals[vidx+1:], *ress)\n",
        "    minus = fn(*vals[:vidx], vals[vidx]-dx, *vals[vidx+1:], *ress)\n",
        "    # (plus - minus)/(2*dx) would give d(fn)/d(x)\n",
        "    grads.append((plus - minus)/(2*dx))\n",
        "  return grads\n",
        "\n",
        "\n",
        "# to help turn gradients after function to gradients before function\n",
        "def calc_grads_nparr_input(fn, all_vals:np.array, vary, dx=1e-8, fn_out=None): # dx < eps\n",
        "  #This only works if the function takes in int inputs like fn(*args):\n",
        "  # note that fn should return a numpy array so that it can be, like, subtracted\n",
        "\n",
        "  all_vals = all_vals.astype(np.float64) # increase precision for grad calculation\n",
        "\n",
        "  if fn_out is None:\n",
        "    fn_out = fn(all_vals)\n",
        "  grads = []\n",
        "\n",
        "  # turn vary into a list of the indices if it's a bool array\n",
        "  if (len(vary)) > 0 and (isinstance(vary[0], bool)):\n",
        "    inds = []\n",
        "    for v in range(len(vary)):\n",
        "      if vary[v]:\n",
        "        inds.append(v)\n",
        "    vary = inds\n",
        "\n",
        "  for vidx in vary:\n",
        "    add = np.array([int(i==vidx) for i in range(len(all_vals))])\n",
        "    plus = fn(all_vals + add*dx)\n",
        "    minus = fn(all_vals - add*dx)\n",
        "    # (plus - minus)/(2*dx) would give d(fn)/d(x)\n",
        "    grads.append((plus - minus)/(2*dx))\n",
        "  return grads\n",
        "\n",
        "\n",
        "# test it out\n",
        "calc_grads_intinputs(CEloss, [1,0], [1,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KxfIc1_RaSLf"
      },
      "outputs": [],
      "source": [
        "class Weight:\n",
        "  def __init__(self, _w, _dtype=np.float32):\n",
        "    self.dtype = _dtype\n",
        "    self.w = np.array(_w, dtype=_dtype)\n",
        "    self.last_x = np.array(0, dtype=_dtype)\n",
        "    self.m = np.array(0, dtype=_dtype)\n",
        "    self.v = np.array(0, dtype=_dtype)\n",
        "    self.t = 0\n",
        "    self.clear_grad()\n",
        "\n",
        "  def mul(self, x):\n",
        "    self.last_x = x\n",
        "    return x * self.w\n",
        "\n",
        "  def grad_from(self, next):\n",
        "    self.grad += next.grad\n",
        "\n",
        "  def finalize_grad(self):\n",
        "    self.grad = (self.grad*self.last_x).reshape(())\n",
        "\n",
        "  def update_w(self, lr):\n",
        "    self.w -= self.grad * lr\n",
        "\n",
        "  def adam_update_w(self, lr, beta1, beta2, t=None, m_prev=None, v_prev=None, eps=1e-7):\n",
        "    # from https://medium.com/@weidagang/demystifying-the-adam-optimizer-in-machine-learning-4401d162cb9e\n",
        "\n",
        "    if m_prev is None: m_prev = self.m\n",
        "    if v_prev is None: v_prev = self.v\n",
        "    if t is None: t = self.t\n",
        "\n",
        "    # Update biased first moment estimate.\n",
        "    # m is the exponentially moving average of the gradients.\n",
        "    # beta1 is the decay rate for the first moment.\n",
        "    m = beta1 * m_prev + (1 - beta1) * self.grad\n",
        "\n",
        "    # Update biased second raw moment estimate.\n",
        "    # v is the exponentially moving average of the squared gradients.\n",
        "    # beta2 is the decay rate for the second moment.\n",
        "    v = beta2 * v_prev + (1 - beta2) * self.grad**2\n",
        "\n",
        "    # Compute bias-corrected first moment estimate.\n",
        "    # This corrects the bias in the first moment caused by initialization at origin.\n",
        "    m_hat = m / (1 - beta1**(t + 1))\n",
        "\n",
        "    # Compute bias-corrected second raw moment estimate.\n",
        "    # This corrects the bias in the second moment caused by initialization at origin.\n",
        "    v_hat = v / (1 - beta2**(t + 1))\n",
        "\n",
        "    # Update parameters.\n",
        "    # Parameters are adjusted based on the learning rate, corrected first moment,\n",
        "    # and the square root of the corrected second moment.\n",
        "    # epsilon is a small number to avoid division by zero.\n",
        "    self.w -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
        "\n",
        "    # Save the first and second moment estimates.\n",
        "    self.m = m\n",
        "    self.v = v\n",
        "    self.t += 1\n",
        "\n",
        "  def clear_grad(self):\n",
        "    self.grad = np.array(0, dtype=self.dtype) # grad is dL/d[]\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"Weight of \"+str(self.w)+\", dtype: \"+str(self.dtype)\n",
        "\n",
        "\n",
        "\n",
        "class Bias:\n",
        "  def __init__(self, _b, _dtype=np.float32):\n",
        "    self.dtype = _dtype\n",
        "    self.b = np.array(_b, dtype=_dtype)\n",
        "    self.grad = np.array(0, dtype=_dtype)\n",
        "    self.m = np.array(0, dtype=_dtype)\n",
        "    self.v = np.array(0, dtype=_dtype)\n",
        "    self.t = 0\n",
        "    self.clear_grad()\n",
        "\n",
        "  def add(self, x):\n",
        "    return self.b + x\n",
        "\n",
        "  def grad_from(self, next):\n",
        "    self.grad += next.grad\n",
        "\n",
        "  def update_b(self, lr):\n",
        "    self.b -= self.grad * lr # NOTE i didnt times lr last time i ran this so...\n",
        "\n",
        "  def adam_update_b(self, lr, beta1, beta2, t=None, m_prev=None, v_prev=None, eps=1e-7):\n",
        "    # from https://medium.com/@weidagang/demystifying-the-adam-optimizer-in-machine-learning-4401d162cb9e\n",
        "\n",
        "    if m_prev is None: m_prev = self.m\n",
        "    if v_prev is None: v_prev = self.v\n",
        "    if t is None: t = self.t\n",
        "\n",
        "    # Update biased first moment estimate.\n",
        "    # m is the exponentially moving average of the gradients.\n",
        "    # beta1 is the decay rate for the first moment.\n",
        "    m = beta1 * m_prev + (1 - beta1) * self.grad\n",
        "\n",
        "    # Update biased second raw moment estimate.\n",
        "    # v is the exponentially moving average of the squared gradients.\n",
        "    # beta2 is the decay rate for the second moment.\n",
        "    v = beta2 * v_prev + (1 - beta2) * self.grad**2\n",
        "\n",
        "    # Compute bias-corrected first moment estimate.\n",
        "    # This corrects the bias in the first moment caused by initialization at origin.\n",
        "    m_hat = m / (1 - beta1**(t + 1))\n",
        "\n",
        "    # Compute bias-corrected second raw moment estimate.\n",
        "    # This corrects the bias in the second moment caused by initialization at origin.\n",
        "    v_hat = v / (1 - beta2**(t + 1))\n",
        "\n",
        "    # Update parameters.\n",
        "    # Parameters are adjusted based on the learning rate, corrected first moment,\n",
        "    # and the square root of the corrected second moment.\n",
        "    # epsilon is a small number to avoid division by zero.\n",
        "    self.b -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
        "\n",
        "    # Save the first and second moment estimates.\n",
        "    self.m = m\n",
        "    self.v = v\n",
        "    self.t += 1\n",
        "\n",
        "  def clear_grad(self):\n",
        "    self.grad = np.array(0, dtype=self.dtype) # grad is dL/d[]\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"Bias of \"+str(self.b)+\", dtype: \"+str(self.dtype)\n",
        "\n",
        "\n",
        "class LayerFunc: # for functions like activation functions or softmax/pooling/similar\n",
        "  # this function's format needs to be like sigmoid(x1, x2, ..., x10), so like use *args please\n",
        "  EPS = 1e-7\n",
        "\n",
        "  def __init__ (self, n_in, n_out, fun, _dtype=np.float32):\n",
        "    self.n_in = n_in\n",
        "    self.n_out = n_out\n",
        "    self.fun = fun\n",
        "    self.dtype = _dtype\n",
        "\n",
        "  # for this to work, if n_out>1, self.fun must return an np.array\n",
        "  def get_in_grads(self, in_vals, out_grads:np.array, vary:list=None): # turn gradients ater func to gradients before func\n",
        "    # if vary is None, then vary all\n",
        "    if vary is None:\n",
        "      vary = list(range(len(in_vals)))\n",
        "\n",
        "    rel_grads = calc_grads_nparr_input(self.fun, in_vals, vary)\n",
        "    # this grads is d(out)/dx\n",
        "    # so we should take grads @ final_grads for each x\n",
        "\n",
        "    # just in case\n",
        "    out_grads = np.array(out_grads)\n",
        "\n",
        "    #print(\"REL:\", rel_grads, \"OUT:\", out_grads)\n",
        "\n",
        "    if self.n_out == 1:\n",
        "      return rel_grads @ out_grads.T\n",
        "\n",
        "    #print(rel_grads[0].shape, out_grads.shape)\n",
        "\n",
        "    grads = [rel_grads[i] @ out_grads.T for i in range(len(rel_grads))]\n",
        "    return grads\n",
        "\n",
        "  def __call__(self, in_vals):\n",
        "    #print(\"CALLING LAYERFUNC ON\", in_vals)\n",
        "    return self.fun(in_vals)\n",
        "\n",
        "\n",
        "\n",
        "class EmptyLayerFunc(LayerFunc):\n",
        "  def __init__(self, num, _dtype=np.float32):\n",
        "    self.n_in = num\n",
        "    self.n_out = num\n",
        "    self.fun = lambda x:x\n",
        "    self.dtype = _dtype\n",
        "\n",
        "  def get_in_grads(self, in_vals, in_consts, out_grads:np.array):\n",
        "    return out_grads # sincei t's empty so the rest can be ignored\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class Layer:\n",
        "  def __init__(self, ins, outs, weight_gen, bias_gen, activation:LayerFunc, _dtype=np.float32):\n",
        "    self.ins = ins\n",
        "    self.outs = outs\n",
        "    self.ws = [ [ Weight(next(weight_gen)) for j in range(outs) ] for i in range(ins) ]\n",
        "    self.bs = [ Bias(next(bias_gen)) for j in range(outs) ]\n",
        "    self.activation = activation\n",
        "    self.dtype = _dtype\n",
        "\n",
        "  def forward(self, xs):\n",
        "    hs = [0 for _ in range(len(self.bs))]\n",
        "\n",
        "    for i in range(len(self.ws)):\n",
        "      for j in range(len(self.ws[i])):\n",
        "        #print((len(self.ws), len(self.ws[0])))\n",
        "        hs[j] += self.ws[i][j].mul(xs[i])\n",
        "\n",
        "    for j in range(len(self.bs)):\n",
        "      hs[j] = self.bs[j].add(hs[j])\n",
        "\n",
        "    #for j in range(len(self.activations)):\n",
        "    #  hs[j] = self.activations[j](hs[j])\n",
        "\n",
        "    hs = np.array(hs)\n",
        "\n",
        "    self.bef_activation_hs = hs.copy()\n",
        "\n",
        "    hs = self.activation(hs)\n",
        "\n",
        "    return hs\n",
        "\n",
        "  def backward(self, grads, lr, clear=True): # provides grad for each h\n",
        "    # but first consider the layerfunc\n",
        "    grads = self.activation.get_in_grads(self.bef_activation_hs.ravel(), grads) # in_grad to LayerFunc is an intermediate grad in Layer\n",
        "\n",
        "   # print(\"CALCULATED GRADS:\", grads)\n",
        "\n",
        "    in_grads = [0 for _ in range(self.ins)]\n",
        "\n",
        "    for i in range(len(self.ws)):\n",
        "      for j in range(len(self.ws[i])):\n",
        "        self.ws[i][j].grad = grads[j]\n",
        "        self.ws[i][j].finalize_grad()\n",
        "        in_grads[i] += self.ws[i][j].w * self.ws[i][j].grad\n",
        "        #print(\"GRADS J\", type(grads[j]))\n",
        "        self.ws[i][j].update_w(lr)\n",
        "        if clear: self.ws[i][j].clear_grad()\n",
        "\n",
        "    for j in range(len(self.bs)):\n",
        "      self.bs[j].grad = grads[j]\n",
        "      self.bs[j].update_b(lr)\n",
        "      if clear: self.bs[j].clear_grad()\n",
        "\n",
        "    return in_grads\n",
        "\n",
        "  def backward_adam(self, grads, lr, beta1, beta2, clear=True):\n",
        "    # but first consider the layerfunc\n",
        "    grads = self.activation.get_in_grads(self.bef_activation_hs, grads) # in_grad to LayerFunc is an intermediate grad in Layer\n",
        "\n",
        "    in_grads = [0 for _ in range(self.ins)]\n",
        "\n",
        "    for i in range(len(self.ws)):\n",
        "      for j in range(len(self.ws[i])):\n",
        "        self.ws[i][j].grad = grads[j]\n",
        "        self.ws[i][j].finalize_grad()\n",
        "        in_grads[i] += self.ws[i][j].w * self.ws[i][j].grad\n",
        "        self.ws[i][j].adam_update_w(lr, beta1, beta2)\n",
        "        if clear: self.ws[i][j].clear_grad()\n",
        "\n",
        "    for j in range(len(self.bs)):\n",
        "      self.bs[j].grad = grads[j]\n",
        "      self.bs[j].adam_update_b(lr, beta1, beta2)\n",
        "      if clear: self.bs[j].clear_grad()\n",
        "\n",
        "    return in_grads\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9QOhbkNvSBZf"
      },
      "outputs": [],
      "source": [
        "def normal_generator(seed=10, mu=0, sigma=1):\n",
        "  rng = np.random.default_rng(seed)\n",
        "  while True:\n",
        "    yield rng.normal(mu, sigma)\n",
        "\n",
        "\n",
        "def sigmoid_activation(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def get_sigmoid_layerfunc(num, _dtype=np.float32):\n",
        "  return LayerFunc(num, num, sigmoid_activation, _dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy-vIKtyRCWA",
        "outputId": "1d920385-dc62-4f98-9f95-0c34d8acae57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DIST: 0.8623164985025764\n"
          ]
        }
      ],
      "source": [
        "# DATA GENERATION\n",
        "\n",
        "# 3 inputs, 4 hiddens, 2 outputs\n",
        "dim_in = 3\n",
        "dim_hidden = 4\n",
        "dim_out = 2\n",
        "# x,y,z point classification task\n",
        "\n",
        "line0init = np.array([0, 1, 0])\n",
        "line0vec = np.array([1, 3, -5])\n",
        "\n",
        "line1init = np.array([-1, 0, 0])\n",
        "line1vec = np.array([3, -1, 2])\n",
        "\n",
        "# closest distance between line 0 and line 1: https://www.geeksforgeeks.org/shortest-distance-between-two-lines-in-3d-space-class-12-maths/\n",
        "cross01 = np.cross(line0vec, line1vec)\n",
        "mulc = cross01.copy() * (line1init - line0init)\n",
        "dist = np.sqrt(mulc.dot(mulc)) / np.sqrt(cross01.dot(cross01))\n",
        "print(\"DIST:\", dist)\n",
        "\n",
        "def get_line(num:int):\n",
        "  if num == 0:\n",
        "    return line0init + (np.random.random()-0.5)*5*line0vec + np.random.normal(scale=0.2, size=(3, )).clip(-0.45, 0.45)*dist\n",
        "  else:\n",
        "    return line1init + (np.random.random()-0.5)*5*line1vec + np.random.normal(scale=0.2, size=(3, )).clip(-0.45, 0.45)*dist\n",
        "\n",
        "'''\n",
        "# we won't need these\n",
        "def get_batch(batch_size=1000):\n",
        "  labels = np.random.randint(0, 2, size=(batch_size,1))\n",
        "  xss = []\n",
        "  for label in labels:\n",
        "    xss.append(get_line(label).reshape((-1,1)))\n",
        "  xs = np.stack(xss)\n",
        "  ys = np.stack([labels==0, labels==1], axis=1)\n",
        "  #print(labels[:5])\n",
        "  #print(ys[:5])\n",
        "  return xs, ys.astype(np.int32)\n",
        "\n",
        "\n",
        "# test generation\n",
        "xs, ys = get_batch(1000)\n",
        "print(xs.shape)\n",
        "print(ys.shape)\n",
        "'''\n",
        "\n",
        "def xy_gen(seed=10):\n",
        "  while True:\n",
        "    res = random.getrandbits(1)\n",
        "    x = get_line(res)#.tolist()\n",
        "    y = [int(res==0), int(res==1)]\n",
        "    yield x, y\n",
        "\n",
        "xyg = xy_gen()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z7YHk2CVxZ-",
        "outputId": "3ffcbf7d-41cf-495d-d084-1782596bc36d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  2.14200636,   7.92984055, -11.77581845])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x, y = next(xyg)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r40zsxLPTrgD"
      },
      "outputs": [],
      "source": [
        "# initialize params\n",
        "\n",
        "l1 = Layer(dim_in, dim_hidden, normal_generator(), normal_generator(), get_sigmoid_layerfunc(dim_hidden))\n",
        "\n",
        "def softmax(p):\n",
        "  t = np.exp(p[0]) + np.exp(p[1])\n",
        "  return np.stack([np.exp(p[0])/t, np.exp(p[1])/t])\n",
        "\n",
        "l2 = Layer(dim_hidden, dim_out, normal_generator(), normal_generator(), LayerFunc(dim_out, dim_out, softmax))\n",
        "\n",
        "\n",
        "def get_lr(epochNum:int=0):\n",
        "  return (5e-4)*(np.exp(-0.00004*epochNum))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQCXqsBmVY3e",
        "outputId": "6efa025e-6c93-4f47-ef7f-ddd78f9839ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0 AVG EVAL LOSS: 0.9324091275991319\n",
            "ACCURACY: 0.523\n",
            "0      AVG TRAIN LOSS: 0.9699833474755578\n",
            "EPOCH 1 AVG TRAIN LOSS: 0.8999815883259198\n",
            "EPOCH 2 AVG TRAIN LOSS: 0.843183332795122\n",
            "EPOCH 3 AVG TRAIN LOSS: 0.7783507526883707\n",
            "EPOCH 4 AVG TRAIN LOSS: 0.7050119367175717\n",
            "EPOCH 5 AVG EVAL LOSS: 0.625263763531739\n",
            "ACCURACY: 0.773\n",
            "5      AVG TRAIN LOSS: 0.6360996234843231\n",
            "EPOCH 10 AVG EVAL LOSS: 0.3570850863128287\n",
            "ACCURACY: 0.989\n",
            "10      AVG TRAIN LOSS: 0.36397199841555555\n",
            "EPOCH 15 AVG EVAL LOSS: 0.22653924547680881\n",
            "ACCURACY: 0.992\n",
            "15      AVG TRAIN LOSS: 0.23150470956521116\n",
            "EPOCH 20 AVG EVAL LOSS: 0.1581691664303159\n",
            "ACCURACY: 0.995\n",
            "20      AVG TRAIN LOSS: 0.15911437987640062\n",
            "EPOCH 25 AVG EVAL LOSS: 0.11869035663330425\n",
            "ACCURACY: 0.996\n",
            "25      AVG TRAIN LOSS: 0.11684426508755164\n",
            "EPOCH 30 AVG EVAL LOSS: 0.09429298814877808\n",
            "ACCURACY: 0.999\n",
            "30      AVG TRAIN LOSS: 0.0907169060156591\n",
            "EPOCH 35 AVG EVAL LOSS: 0.07696269535389677\n",
            "ACCURACY: 0.999\n",
            "35      AVG TRAIN LOSS: 0.07618056947605732\n",
            "EPOCH 40 AVG EVAL LOSS: 0.06455912959861117\n",
            "ACCURACY: 1.0\n",
            "40      AVG TRAIN LOSS: 0.06254252194369188\n",
            "EPOCH 45 AVG EVAL LOSS: 0.05549762372173436\n",
            "ACCURACY: 1.0\n",
            "45      AVG TRAIN LOSS: 0.05462698477056638\n",
            "EPOCH 50 AVG EVAL LOSS: 0.048575428181670044\n",
            "ACCURACY: 1.0\n",
            "50      AVG TRAIN LOSS: 0.047345107494715925\n",
            "EPOCH 55 AVG EVAL LOSS: 0.04329538878889751\n",
            "ACCURACY: 1.0\n",
            "55      AVG TRAIN LOSS: 0.04272543499949059\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 55\u001b[0m\n\u001b[0;32m     51\u001b[0m epochtotalloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(runs_per_epoch):\n\u001b[0;32m     54\u001b[0m   \u001b[38;5;66;03m# get data\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m   x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(xyg)\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;66;03m#print(x)\u001b[39;00m\n\u001b[0;32m     57\u001b[0m   ypred \u001b[38;5;241m=\u001b[39m l2\u001b[38;5;241m.\u001b[39mforward(l1\u001b[38;5;241m.\u001b[39mforward(x))\n",
            "Cell \u001b[1;32mIn[5], line 50\u001b[0m, in \u001b[0;36mxy_gen\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m   res \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mgetrandbits(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[43mget_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#.tolist()\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   y \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(res\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mint\u001b[39m(res\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m     52\u001b[0m   \u001b[38;5;28;01myield\u001b[39;00m x, y\n",
            "Cell \u001b[1;32mIn[5], line 23\u001b[0m, in \u001b[0;36mget_line\u001b[1;34m(num)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_line\u001b[39m(num:\u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m     22\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m line0init \u001b[38;5;241m+\u001b[39m (np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom()\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39mline0vec \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, ))\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.45\u001b[39m, \u001b[38;5;241m0.45\u001b[39m)\u001b[38;5;241m*\u001b[39mdist\n\u001b[0;32m     24\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m line1init \u001b[38;5;241m+\u001b[39m (np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom()\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39mline1vec \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, ))\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.45\u001b[39m, \u001b[38;5;241m0.45\u001b[39m)\u001b[38;5;241m*\u001b[39mdist\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# RUN\n",
        "\n",
        "# prep eval set\n",
        "eval_num = 1000\n",
        "evals = [next(xyg) for _ in range(eval_num)]\n",
        "\n",
        "def evaluate(epochNum:int = None):\n",
        "\n",
        "  losses = []\n",
        "  correct = 0\n",
        "  for x, y in evals:\n",
        "    ypred = l2.forward(l1.forward(x))\n",
        "\n",
        "    # get loss\n",
        "    losses.append(CEloss(ypred[0], ypred[1], y[0], y[1]))\n",
        "\n",
        "    # accuracy\n",
        "    if ypred[0] > ypred[1]:\n",
        "      # predicts 0\n",
        "      correct += y[0]\n",
        "    else:\n",
        "      # predicts 1\n",
        "      correct += y[1]\n",
        "\n",
        "  #print(\"YPRED\", ypred, \"Y\", y, \"LOSS\", CEloss(ypred[0], ypred[1], y[0], y[1])) # show one sample\n",
        "\n",
        "  loss = sum(losses)/len(losses)\n",
        "\n",
        "\n",
        "  # display\n",
        "  if epochNum is None:\n",
        "    print('EVAL LOSS:', loss)\n",
        "  else:\n",
        "    print('EPOCH', epochNum, 'AVG EVAL LOSS:', loss)\n",
        "\n",
        "  print(\"ACCURACY:\", correct/len(losses))\n",
        "\n",
        "  return loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_epoch_losses = []\n",
        "eval_epoch_losses = []\n",
        "\n",
        "epoch = 0\n",
        "runs_per_epoch = 10000\n",
        "avgepochloss = 10\n",
        "while avgepochloss > 0.02:\n",
        "\n",
        "  epochtotalloss = 0\n",
        "\n",
        "  for i in range(runs_per_epoch):\n",
        "    # get data\n",
        "    x, y = next(xyg)\n",
        "    #print(x)\n",
        "    ypred = l2.forward(l1.forward(x))\n",
        "\n",
        "    # get loss\n",
        "    loss = CEloss(ypred[0], ypred[1], y[0], y[1])\n",
        "    #print(\"RAW LOSS:\", CEloss(p_s[:,0,0], p_s[:,1,0], y[:,0,0], y[:,1,0]))\n",
        "    #print(\"LOSS:\", loss)\n",
        "\n",
        "    #print(ypred[0], ypred[1], y[0], y[1], loss)\n",
        "\n",
        "    #1/0\n",
        "\n",
        "    # update params\n",
        "    lr = get_lr(epoch)\n",
        "    grads = l2.backward(calc_grads_intinputs(CEloss, ypred.ravel().tolist(), y, fn_out=loss), lr)\n",
        "    l1.backward(grads, lr)\n",
        "\n",
        "\n",
        "    epochtotalloss += loss\n",
        "\n",
        "  avgepochloss = epochtotalloss/runs_per_epoch\n",
        "\n",
        "  if epoch%5 == 0:\n",
        "    eval_loss = evaluate(epoch)\n",
        "    print(epoch, \"     AVG TRAIN LOSS:\", avgepochloss)\n",
        "    eval_epoch_losses.append(eval_loss) # includes epoch 0\n",
        "  elif epoch < 5:\n",
        "    print(\"EPOCH\", epoch, \"AVG TRAIN LOSS:\", avgepochloss)\n",
        "\n",
        "  epoch += 1\n",
        "  train_epoch_losses.append(avgepochloss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I think it's pretty clear that it's learning and functional, so i just terminated it \n",
        "also 100% accuracy yay \n",
        "\n",
        "do note that since i keep generating more train data, this is theoretically able to converge since the train data will cover the entire distribution. \n",
        "and it should be nearly linearly separable - so perceptron model should be enough for this task \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
