{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3MPAqq4GiIr3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3GNHxdTodcL0"
   },
   "outputs": [],
   "source": [
    "# Adaptive Learning Rate\n",
    "class AdaptiveLR():\n",
    "    \"\"\" Implements adaptive learning rate adjustment. \"\"\"\n",
    "    def __init__(self, optimizer, factor=0.5, patience=3, min_lr=1e-6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optimizer (torch.optim.Optimizer): How long to wait after last time validation loss improved.\n",
    "                            Default: 5\n",
    "            factor (float): Factor by which the learning rate will be reduced.\n",
    "                        Default: 0.9\n",
    "            patience (int): Number of epochs with no improvement before reducing LR.\n",
    "                        Default: 5\n",
    "            min_lr (float): Lower bound on the learning rate.\n",
    "                        Default: 1e-6\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.best_loss = float('inf')\n",
    "        self.wait = 0\n",
    "\n",
    "    def step(self, loss):\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    new_lr = max(param_group['lr'] * self.factor, self.min_lr)\n",
    "                    param_group['lr'] = new_lr\n",
    "                self.wait = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "p0N1m7KMgLb7"
   },
   "outputs": [],
   "source": [
    "# L2 Regularization\n",
    "def l2_regularization(model, lambda_reg):\n",
    "    \"\"\" Computes L2 regularization loss. \"\"\"\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        lambda_reg (float): Regularization coefficient.\n",
    "    \"\"\"\n",
    "    l2_loss = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "    return lambda_reg * l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TIHzCWcXgNVz"
   },
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "class EarlyStopping():\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(self, patience=5, delta=0, path=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 5\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str or None): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.path is not None:\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YMfBsR7hgNTg"
   },
   "outputs": [],
   "source": [
    "# Dropout (Manual Implementation)\n",
    "class CustomDropout(nn.Module):\n",
    "    \"\"\" Implements dropout. \"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            p (float): Dropout probability.\n",
    "                      Default: 0.5\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mask = (torch.rand_like(x) > self.p).float()\n",
    "            return x * mask / (1 - self.p)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CJ5-cROegNRQ"
   },
   "outputs": [],
   "source": [
    "# Weight Initialization (still implementing?)\n",
    "def WeightInit(m):\n",
    "    \"\"\" Applies Xavier initialization to linear layers. \"\"\"\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        m (nn.Module): Module to be initialized.\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LgT62CaOgNPQ"
   },
   "outputs": [],
   "source": [
    "# Batch Normalization\n",
    "class CustomBatchNorm(nn.Module):\n",
    "    \"\"\" Implements batch normalization without using PyTorch's built-in BN layers. \"\"\"\n",
    "    def __init__(self, num_features, momentum=0.1, eps=1e-5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          num_features (int): Number of features in input.\n",
    "          momentum (float): Momentum factor for moving average.\n",
    "                            Default: 0.1\n",
    "          eps (float): Small value to avoid division by zero.\n",
    "                      Default: 1e-5\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self.running_mean = torch.zeros(num_features)\n",
    "        self.running_var = torch.ones(num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mean = x.mean(dim=0)\n",
    "            var = x.var(dim=0, unbiased=False)\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x_norm + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZResAeQgNI5",
    "outputId": "6b7f78be-9c54-466c-97df-289d4cdc0e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3332\n",
      "Epoch 2, Loss: 0.2694\n",
      "Epoch 3, Loss: 0.2670\n",
      "Epoch 4, Loss: 0.2681\n",
      "Epoch 5, Loss: 0.2670\n",
      "Epoch 6, Loss: 0.2711\n",
      "Epoch 7, Loss: 0.2680\n",
      "Epoch 8, Loss: 0.2712\n",
      "Epoch 9, Loss: 0.2260\n",
      "Epoch 10, Loss: 0.2116\n",
      "Epoch 11, Loss: 0.2057\n",
      "Epoch 12, Loss: 0.2034\n",
      "Epoch 13, Loss: 0.2029\n",
      "Epoch 14, Loss: 0.2007\n",
      "Epoch 15, Loss: 0.1995\n",
      "Epoch 16, Loss: 0.2016\n",
      "Epoch 17, Loss: 0.1980\n",
      "Epoch 18, Loss: 0.1976\n",
      "Epoch 19, Loss: 0.2000\n",
      "Epoch 20, Loss: 0.1977\n",
      "Epoch 21, Loss: 0.1971\n",
      "Epoch 22, Loss: 0.1990\n",
      "Epoch 23, Loss: 0.1986\n",
      "Epoch 24, Loss: 0.1986\n",
      "Epoch 25, Loss: 0.1751\n",
      "Epoch 26, Loss: 0.1645\n",
      "Epoch 27, Loss: 0.1596\n",
      "Epoch 28, Loss: 0.1618\n",
      "Epoch 29, Loss: 0.1590\n",
      "Epoch 30, Loss: 0.1555\n",
      "Epoch 31, Loss: 0.1539\n",
      "Epoch 32, Loss: 0.1548\n",
      "Epoch 33, Loss: 0.1558\n",
      "Epoch 34, Loss: 0.1558\n",
      "Epoch 35, Loss: 0.1394\n",
      "Epoch 36, Loss: 0.1348\n",
      "Epoch 37, Loss: 0.1318\n",
      "Epoch 38, Loss: 0.1305\n",
      "Epoch 39, Loss: 0.1274\n",
      "Epoch 40, Loss: 0.1283\n",
      "Epoch 41, Loss: 0.1256\n",
      "Epoch 42, Loss: 0.1257\n",
      "Epoch 43, Loss: 0.1258\n",
      "Epoch 44, Loss: 0.1259\n",
      "Epoch 45, Loss: 0.1178\n",
      "Epoch 46, Loss: 0.1137\n",
      "Epoch 47, Loss: 0.1127\n",
      "Epoch 48, Loss: 0.1110\n",
      "Epoch 49, Loss: 0.1091\n",
      "Epoch 50, Loss: 0.1092\n"
     ]
    }
   ],
   "source": [
    "#Full Example\n",
    "\n",
    "class ExampleModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.apply(WeightInit)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = CustomBatchNorm(hidden_size)\n",
    "        self.dropout1 = CustomDropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training loop with all implemented techniques\n",
    "def train(model, train_loader, optimizer, criterion, num_epochs=50):\n",
    "    adaptive_lr = AdaptiveLR(optimizer)\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.view(inputs.size(0), -1), targets\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets) + l2_regularization(model, lambda_reg=1e-4)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        adaptive_lr.step(avg_loss)\n",
    "        early_stopping.__call__(avg_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "model = ExampleModel(28*28, 128, 10)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train(model, train_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "te8PmbN41WKi",
    "outputId": "3d3eb2bc-87b5-4a07-a075-09bba491187a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3334\n",
      "Epoch 2, Loss: 0.2730\n",
      "Epoch 3, Loss: 0.2698\n",
      "Epoch 4, Loss: 0.2683\n",
      "Epoch 5, Loss: 0.2703\n",
      "Epoch 6, Loss: 0.2716\n",
      "Epoch 7, Loss: 0.2675\n",
      "Epoch 8, Loss: 0.2710\n",
      "Epoch 9, Loss: 0.2687\n",
      "Epoch 10, Loss: 0.2682\n",
      "Epoch 11, Loss: 0.2231\n",
      "Epoch 12, Loss: 0.2077\n",
      "Epoch 13, Loss: 0.2073\n",
      "Epoch 14, Loss: 0.2044\n",
      "Epoch 15, Loss: 0.2005\n",
      "Epoch 16, Loss: 0.1998\n",
      "Epoch 17, Loss: 0.2014\n",
      "Epoch 18, Loss: 0.1994\n",
      "Epoch 19, Loss: 0.1975\n",
      "Epoch 20, Loss: 0.1993\n"
     ]
    }
   ],
   "source": [
    "#Adaptive LR Example\n",
    "\n",
    "class ExampleModel1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.apply(WeightInit)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = CustomBatchNorm(hidden_size)\n",
    "        self.dropout1 = CustomDropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training loop with all implemented techniques\n",
    "def train(model, train_loader, optimizer, criterion, num_epochs=20):\n",
    "    adaptive_lr = AdaptiveLR(optimizer)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.view(inputs.size(0), -1), targets\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets) + l2_regularization(model, lambda_reg=1e-4)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        adaptive_lr.step(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "model = ExampleModel1(28*28, 128, 10)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train(model, train_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yd1jagjQ8OgB",
    "outputId": "aa2feae3-1d56-4cd2-af04-6842bfe3f483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3880\n",
      "Epoch 2, Loss: 0.3620\n",
      "Epoch 3, Loss: 0.3661\n",
      "Epoch 4, Loss: 0.3670\n",
      "Epoch 5, Loss: 0.3709\n",
      "Epoch 6, Loss: 0.3677\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "#Early Stopping Example\n",
    "class ExampleModel2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.apply(WeightInit)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = CustomBatchNorm(hidden_size)\n",
    "        self.dropout1 = CustomDropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training loop with all implemented techniques\n",
    "def train(model, train_loader, optimizer, criterion, num_epochs=50):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.view(inputs.size(0), -1), targets\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets) + l2_regularization(model, lambda_reg=1e-4)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        early_stopping.__call__(avg_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "model = ExampleModel2(28*28, 128, 10)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train(model, train_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRuAIZs18YM4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
