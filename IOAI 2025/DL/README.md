ALL DONE!!
# Syllabus

- Multi-Layer Perceptrons (MLP) 
- Stochastic Gradient Descent (SGD), MiniBatch Gradient Descent 
- Momentum Methods (Adam, AdamW) 
- Adaptive Learning Rates 
- Convergence and Learning Rates 
- Weight Regularization 
- Early Stopping 
- Dropout, Gaussian Noise 
- Weight Initialization 
- Batch Normalization 
- Autoencoders and Sparse Encoders

# People

- Jaden
- Luke
- Shine
